@article{zhou2012nonparametric,
	Abstract = {Nonparametric Bayesian methods are considered for recovery of imagery based upon compressive, incomplete, and/or noisy measurements. A truncated beta-Bernoulli process is employed to infer an appropriate dictionary for the data under test and also for image recovery. In the context of compressive sensing, significant improvements in image recovery are manifested using learned dictionaries, relative to using standard orthonormal image expansions. The compressive-measurement projections are also optimized for the learned dictionary. Additionally, we consider simpler (incomplete) measurements, defined by measuring a subset of image pixels, uniformly selected at random. Spatial interrelationships within imagery are exploited through use of the Dirichlet and probit stick-breaking processes. Several example results are presented, with comparisons to other methods in the literature.},
	Author = {Zhou, Mingyuan and Chen, Haojun and Paisley, John and Ren, Lu and Li, Lingbo and Xing, Zhengming and Dunson, David and Sapiro, Guillermo and Carin, Lawrence},
	Journal = {IEEE Transactions on Image Processing},
	Number = {1},
	Pages = {130--144},
	Publisher = {IEEE},
	Title = {Nonparametric {B}ayesian dictionary learning for analysis of noisy and incomplete images},
	Url_Code = {Results/BPFAImage/},
	Url_Link = {http://ieeexplore.ieee.org/document/5898409/},
	Url_PDF = {Papers/BPFAImage_Journal_11-2.pdf},
	Volume = {21},
	Year = {2012}
	}

@inproceedings{zhou2011dependent,
	Abstract = {A dependent hierarchical beta process (dHBP) is developed as a prior for data that may be represented in terms of a sparse set of latent features, with covariate-dependent feature usage. The dHBP is applicable to general covariates and data models, imposing that signals with similar covariates are likely to be manifested in terms of similar features. Coupling the dHBP with the Bernoulli process, and upon marginalizing out the dHBP, the model may be interpreted as a covariate-dependent hierarchical Indian buffet process. As applications, we consider interpolation and denoising of an image, with covariates defined by the location of image patches within an image. Two types of noise models are considered: (i) typical white Gaussian noise; and (ii) spiky noise of arbitrary amplitude, distributed uniformly at random. In these examples, the features correspond to the atoms of a dictionary, learned based upon the data under test (without a priori training data). State-of-the-art performance is demonstrated, with efficient inference using hybrid Gibbs, Metropolis-Hastings and slice sampling.},
	Address = {Fort Lauderdale, FL, USA},
	Author = {Mingyuan Zhou and Hongxia Yang and Guillermo Sapiro and David Dunson and Lawrence Carin},
	Booktitle = {International Conference on Artificial Intelligence and Statistics},
	Editor = {Geoffrey Gordon and David Dunson and Miroslav Dud{\'\i}k},
	Month = {11--13 Apr},
	Note = {(Oral presentation)},
	Pages = {883--891},
	Publisher = {PMLR},
	Series = {Proceedings of Machine Learning Research},
	Title = {Dependent Hierarchical Beta Process for Image Interpolation and Denoising},
	Url_Link = {http://proceedings.mlr.press/v15/zhou11a.html},
	Url_PDF = {http://proceedings.mlr.press/v15/zhou11a/zhou11a.pdf},
	Url_Slide = {Papers/dHBP_AISTATS2011_MingyuanZhou_04122011.pdf},
	Url_Video = {http://videolectures.net/aistats2011_zhou_dependent/},
	Volume = {15},
	Year = {2011}
	}

@incollection{zhou2009non,
	Abstract = {Non-parametric Bayesian techniques are considered for learning dictionaries for sparse image representations, with applications in denoising, inpainting and compressive sensing (CS). The beta process is employed as a prior for learning the dictionary, and this non-parametric method naturally infers an appropriate dictionary size. The Dirichlet process and a probit stick-breaking process are also considered to exploit structure within an image. The proposed method can learn a sparse dictionary in situ; training images may be exploited if available, but they are not required. Further, the noise variance need not be known, and can be non-stationary. Another virtue of the proposed method is that sequential inference can be readily employed, thereby allowing scaling to large images. Several example results are presented, using both Gibbs and variational Bayesian inference, with comparisons to other state-of-the-art approaches.},
	Author = {Zhou, Mingyuan and Haojun Chen and Lu Ren and Sapiro, Guillermo and Carin, Lawrence and John W. Paisley},
	Booktitle = {Advances in Neural Information Processing Systems},
	Editor = {Y. Bengio and D. Schuurmans and J. D. Lafferty and C. K. I. Williams and A. Culotta},
	Note = {(Oral presentation)},
	Pages = {2295--2303},
	Publisher = {Curran Associates, Inc.},
	Title = {Non-Parametric {B}ayesian Dictionary Learning for Sparse Image Representations},
	Url_Code = {Softwares/BPFA_Denoising_Inpainting_codes_Inference_10292009.zip},
	Url_Link = {http://papers.nips.cc/paper/3851-non-parametric-bayesian-dictionary-learning-for-sparse-image-representations},
	Url_PDF = {http://papers.nips.cc/paper/3851-non-parametric-bayesian-dictionary-learning-for-sparse-image-representations.pdf},
	Url_Slide = {Papers/MingyuanZhou_NIPS_12082009.pdf},
	Url_Video = {http://videolectures.net/mingyuan_zhou/},
	Year = {2009}
	}
