



@inproceedings{zhou2014beta,
  title={Beta-Negative Binomial Process and Exchangeable Random Partitions for Mixed-Membership Modeling},
author = {Zhou, Mingyuan},
booktitle = {Advances in Neural Information Processing Systems},
editor = {Z. Ghahramani and M. Welling and C. Cortes and N. D. Lawrence and K. Q. Weinberger},
pages = {3455--3463},
year = {2014},
publisher = {Curran Associates, Inc.},
url_link = {http://papers.nips.cc/paper/5298-beta-negative-binomial-process-and-exchangeable-random-partitions-for-mixed-membership-modeling.html},
url_pdf={Papers/BNBP_Collapsed_v7_arXiv.pdf},
url_poster={Papers/BNBP_NIPS2014_Poster.pdf},
url_code = {https://GitHub.com/mingyuanzhou/BNBP_collapsed},
url_arxiv={http://arxiv.org/abs/1410.7812},
Abstract={The beta-negative binomial process (BNBP), an integer-valued stochastic process, is employed to partition a count vector into a latent random count matrix. As the marginal probability distribution of the BNBP that governs the exchangeable random partitions of grouped data has not yet been developed, current inference for the BNBP has to truncate the number of atoms of the beta process. This paper introduces an exchangeable partition probability function to explicitly describe how the BNBP clusters the data points of each group into a random number of exchangeable partitions, which are shared across all the groups. A fully collapsed Gibbs sampler is developed for the BNBP, leading to a novel nonparametric Bayesian topic model that is distinct from existing ones, with simple implementation, fast convergence, good mixing, and state-of-the-art predictive performance.},
}

	
		
@article{polatkan2015bayesian,
  title={A {B}ayesian nonparametric approach to image super-resolution},
  author={Polatkan, Gungor and Zhou, Mingyuan and Carin, Lawrence and Blei, David and Daubechies, Ingrid},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume={37},
  number={2},
  pages={346--358},
  year={2015},
  publisher={IEEE},
  url_link={http://ieeexplore.ieee.org/document/6809161/},
  url_pdf={http://arxiv.org/abs/1209.5019},
  url_arxiv={http://arxiv.org/abs/1209.5019},
  abstract={Super-resolution methods form high-resolution images from low-resolution images. In this paper, we develop a new Bayesian nonparametric model for super-resolution. Our method uses a beta-Bernoulli process to learn a set of recurring visual patterns, called dictionary elements, from the data. Because it is nonparametric, the number of elements found is also determined from the data. We test the results on both benchmark and natural images, comparing with several other models from the research literature. We perform large-scale human evaluation experiments to assess the visual quality of the results. In a first implementation, we use Gibbs sampling to approximate the posterior. However, this algorithm is not feasible for large-scale data. To circumvent this, we then develop an online variational Bayes (VB) algorithm. This algorithm finds high quality dictionaries in a fraction of the time needed by the Gibbs sampler.},
}
		

@article{zhou2015negative,
  title={Negative binomial process count and mixture modeling},
  author={Zhou, Mingyuan and Carin, Lawrence},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume={37},
  number={2},
  pages={307--320},
  year={2015},
  publisher={IEEE},
  url_link = {https://ieeexplore.ieee.org/document/6636308},
  url_pdf= {Papers/Mingyuan_PAMI_9.pdf},
  url_arxiv = {http://arxiv.org/abs/1209.3442},
  url_code={Softwares/NBP_PFA_v1.zip},
  abstract={The seemingly disjoint problems of count and mixture modeling are united under the negative binomial (NB) process. A gamma process is employed to model the rate measure of a Poisson process, whose normalization provides a random probability measure for mixture modeling and whose marginalization leads to an NB process for count modeling. A draw from the NB process consists of a Poisson distributed finite number of distinct atoms, each of which is associated with a logarithmic distributed number of data samples. We reveal relationships between various count- and mixture-modeling distributions and construct a Poisson-logarithmic bivariate distribution that connects the NB and Chinese restaurant table distributions. Fundamental properties of the models are developed, and we derive efficient Bayesian inference. It is shown that with augmentation and normalization, the NB process and gamma-NB process can be reduced to the Dirichlet process and hierarchical Dirichlet process, respectively. These relationships highlight theoretical, structural, and computational advantages of the NB process. A variety of NB processes, including the beta-geometric, beta-NB, marked-beta-NB, marked-gamma-NB and zero-inflated-NB processes, with distinct sharing mechanisms, are also constructed. These models are applied to topic modeling, with connections made to existing algorithms under Poisson factor analysis. Example results show the importance of inferring both the NB dispersion and probability parameters.},
}




@inproceedings{zhou2012augment,
title = {Augment-and-Conquer Negative Binomial Processes},
author = {Zhou, Mingyuan and Carin, Lawrence},
booktitle = {Advances in Neural Information Processing Systems},
editor = {F. Pereira and C. J. C. Burges and L. Bottou and K. Q. Weinberger},
pages = {2546--2554},
year = {2012},
publisher = {Curran Associates, Inc.},
Url_Link = {https://papers.nips.cc/paper/4677-augment-and-conquer-negative-binomial-processes},
Url_PDF = {Papers/Mingyuan_NBP_NIPS2012.pdf},
Url_slide = {Papers/NBP_NIPS2012_Slides_Mingyuan_12052012.pdf},
Url_Poster = {Papers/NBP_NIPS2012_Poster_MingyuanZhou_12052012.pdf},
Url_Code = {Softwares/NBP_PFA_v1.zip},
Note= {(Spotlight oral presentation)},
Abstract={By developing data augmentation methods unique to the negative binomial (NB) distribution, we unite seemingly disjoint count and mixture models under the NB process framework. We develop fundamental properties of the models and derive efficient Gibbs sampling inference. We show that the gamma-NB process can be reduced to the hierarchical Dirichlet process with normalization, highlighting its unique theoretical, structural and computational advantages. A variety of NB processes with distinct sharing mechanisms are constructed and applied to topic modeling, with connections to existing algorithms, showing the importance of inferring both the NB dispersion and probability parameters.},
}



@inproceedings{zhou2012lognormal,
  author =    {Mingyuan Zhou and Lingbo Li and David Dunson and Lawrence Carin},
  title =     {Lognormal and Gamma Mixed Negative Binomial Regression},
  booktitle = {International Conference on Machine Learning},
  series =    {ICML '12},
  year =      {2012},
  editor =    {John Langford and Joelle Pineau},
  location =  {Edinburgh, Scotland, GB},
  isbn =      {978-1-4503-1285-1},
  month =     {July},
  publisher = {Omnipress},
  address =   {New York, NY, USA},
  pages=      {1343--1350},
  Url_Paper= {Papers/Mingyuan_ICML_2012.pdf},
  Url_Appendix={Papers/Mingyuan_LGNB_Appendix.pdf},
  Url_Code = {Softwares/LGNB_Regression_v0.zip},
  Url_Slide = {Papers/LGNB_MingyuanZhou_06272012.pdf}
  Url_Video = {http://techtalks.tv/talks/lognormal-and-gamma-mixed-negative-binomial-regression/57332/},
  Url_Link= {https://icml.cc/Conferences/2012/papers/665.pdf},
  Url_Poster= {Papers/LGNB_ICML2012_Poster_MingyuanZhou_06272012.pdf},
  Abstract = {In regression analysis of counts, a lack of simple and efficient algorithms for posterior computation has made Bayesian approaches appear unattractive and thus underdeveloped. We propose a lognormal and gamma mixed negative binomial (NB) regression model for counts, and present efficient closed-form Bayesian inference; unlike conventional Poisson models, the proposed approach has two free parameters to include two different kinds of dispersion, and allows the incorporation of prior information, such as sparsity in the regression coefficients. By placing a gamma distribution prior on the NB dispersion parameter r, and connecting a lognormal distribution prior with the logit of the NB probability parameter p, efficient Gibbs sampling and variational Bayes inference are both developed. The closed-form updates are obtained by exploiting conditional conjugacy via both a compound Poisson representation and a Polya-Gamma distribution based data augmentation approach. The proposed Bayesian inference can be implemented routinely, while being easily generalizable to more complex settings involving multivariate dependence structures. The algorithms are illustrated using real examples.},
}



@InProceedings{zhou2011beta,
  title = 	 {Beta-Negative Binomial Process and Poisson Factor Analysis},
  author = 	 {Mingyuan Zhou and Lauren Hannah and David Dunson and Lawrence Carin},
  booktitle = 	 {International Conference on Artificial Intelligence and Statistics},
  pages = 	 {1462--1471},
  year = 	 {2012},
  editor = 	 {Neil D. Lawrence and Mark Girolami},
  volume = 	 {22},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {La Palma, Canary Islands},
  month = 	 {21--23 Apr},
  publisher = 	 {PMLR},
  Url_Code = {Softwares/NBP_PFA_v1.zip},
  Url_PDF = 	 {Papers/AISTATS2012_NegBinoBeta_PFA_v19.pdf},
  Url_Poster = {Papers/BNBP_AISTATS2012_Poster_MingyuanZhou_04122012_Tall.pdf},
  Url_Link = 	 {http://proceedings.mlr.press/v22/zhou12c.html},
  abstract = 	 {A beta-negative binomial (BNB) process is proposed, leading to a beta-gamma-Poisson process, which may be viewed as a “multi-scoop” generalization of the beta-Bernoulli process. The BNB process is augmented into a beta-gamma-gamma-Poisson hierarchical structure, and applied as a nonparametric Bayesian prior for an infinite Poisson factor analysis model. A finite approximation for the beta process Levy random measure is constructed for convenient implementation. Efficient MCMC computations are performed with data augmentation and marginalization techniques. Encouraging results are shown on document count matrix factorization.}
}


@inproceedings{li2011integration,
  author =    {Lingbo Li and Mingyuan Zhou and Guillermo Sapiro and Lawrence Carin},
  title =     {On the Integration of Topic Modeling and Dictionary Learning },
  booktitle = {International Conference on Machine Learning},
  series =    {ICML '11},
  year =      {2011},
  editor =    {Lise Getoor and Tobias Scheffer},
  location =  {Bellevue, Washington, USA},
  isbn =      {978-1-4503-0619-5},
  month =     {June},
  publisher = {ACM},
  pages=      {625--632},
  Url_Link= {http://www.icml-2011.org/papers/375_icmlpaper.pdf},
  Url_PDF= {http://www.icml-2011.org/papers/375_icmlpaper.pdf},
  Abstract={A new nonparametric Bayesian model is developed to integrate dictionary learning and topic model into a unified framework. The model is employed to analyze partially annotated images, with the dictionary learning performed directly on image patches. Efficient inference is performed with a Gibbs slice sampler, and encouraging results are reported on widely used datasets.},
}

@article{xing2012dictionary,
	author = {Xing, Zhengming and Zhou, Mingyuan and Castrodad, Alexey and Sapiro, Guillermo and Carin, Lawrence},
	title = {Dictionary Learning for Noisy and Incomplete Hyperspectral Images},
	journal = {SIAM Journal on Imaging Sciences},
	volume = {5},
	number = {1},
	pages = {33-56},
	year = {2012},
	Url_Link= {http://epubs.siam.org/doi/abs/10.1137/110837486},
	Url_PDF= {Papers/BPFA_HSI_6.pdf},
	Abstract = {We consider analysis of noisy and incomplete hyperspectral imagery, with the objective of removing the noise and inferring the missing data. The noise statistics may be wavelength dependent, and the fraction of data missing (at random) may be substantial, including potentially entire bands, offering the potential to significantly reduce the quantity of data that need be measured. To achieve this objective, the imagery is divided into contiguous three-dimensional (3D) spatio-spectral blocks of spatial dimension much less than the image dimension. It is assumed that each such 3D block may be represented as a linear combination of dictionary elements of the same dimension, plus noise, and the dictionary elements are learned in situ based on the observed data (no a priori training). The number of dictionary elements needed for representation of any particular block is typically small relative to the block dimensions, and all the image blocks are processed jointly (“collaboratively") to infer the underlying dictionary. We address dictionary learning from a Bayesian perspective, considering two distinct means of imposing sparse dictionary usage. These models allow inference of the number of dictionary elements needed as well as the underlying wavelength-dependent noise statistics. It is demonstrated that drawing the dictionary elements from a Gaussian process prior, imposing structure on the wavelength dependence of the dictionary elements, yields significant advantages, relative to the more conventional approach of using an independent and identically distributed Gaussian prior for the dictionary elements; this advantage is particularly evident in the presence of noise. The framework is demonstrated by processing hyperspectral imagery with a significant number of voxels missing uniformly at random, with imagery at specific wavelengths missing entirely, and in the presence of substantial additive noise.},
}

@article{zhou2012nonparametric,
	Abstract = {Nonparametric Bayesian methods are considered for recovery of imagery based upon compressive, incomplete, and/or noisy measurements. A truncated beta-Bernoulli process is employed to infer an appropriate dictionary for the data under test and also for image recovery. In the context of compressive sensing, significant improvements in image recovery are manifested using learned dictionaries, relative to using standard orthonormal image expansions. The compressive-measurement projections are also optimized for the learned dictionary. Additionally, we consider simpler (incomplete) measurements, defined by measuring a subset of image pixels, uniformly selected at random. Spatial interrelationships within imagery are exploited through use of the Dirichlet and probit stick-breaking processes. Several example results are presented, with comparisons to other methods in the literature.},
	Author = {Zhou, Mingyuan and Chen, Haojun and Paisley, John and Ren, Lu and Li, Lingbo and Xing, Zhengming and Dunson, David and Sapiro, Guillermo and Carin, Lawrence},
	Journal = {IEEE Transactions on Image Processing},
	Number = {1},
	Pages = {130--144},
	Publisher = {IEEE},
	Title = {Nonparametric {B}ayesian dictionary learning for analysis of noisy and incomplete images},
	Url_Code = {Results/BPFAImage/},
	Url_Link = {http://ieeexplore.ieee.org/document/5898409/},
	Url_PDF = {Papers/BPFAImage_Journal_11-2.pdf},
	Volume = {21},
	Year = {2012}}

@inproceedings{zhou2011dependent,
	Abstract = {A dependent hierarchical beta process (dHBP) is developed as a prior for data that may be represented in terms of a sparse set of latent features, with covariate-dependent feature usage. The dHBP is applicable to general covariates and data models, imposing that signals with similar covariates are likely to be manifested in terms of similar features. Coupling the dHBP with the Bernoulli process, and upon marginalizing out the dHBP, the model may be interpreted as a covariate-dependent hierarchical Indian buffet process. As applications, we consider interpolation and denoising of an image, with covariates defined by the location of image patches within an image. Two types of noise models are considered: (i) typical white Gaussian noise; and (ii) spiky noise of arbitrary amplitude, distributed uniformly at random. In these examples, the features correspond to the atoms of a dictionary, learned based upon the data under test (without a priori training data). State-of-the-art performance is demonstrated, with efficient inference using hybrid Gibbs, Metropolis-Hastings and slice sampling.},
	Address = {Fort Lauderdale, FL, USA},
	Author = {Mingyuan Zhou and Hongxia Yang and Guillermo Sapiro and David Dunson and Lawrence Carin},
	Booktitle = {International Conference on Artificial Intelligence and Statistics},
	Editor = {Geoffrey Gordon and David Dunson and Miroslav Dud{\'\i}k},
	Month = {11--13 Apr},
	Note = {(Oral presentation)},
	Pages = {883--891},
	Publisher = {PMLR},
	Series = {Proceedings of Machine Learning Research},
	Title = {Dependent Hierarchical Beta Process for Image Interpolation and Denoising},
	Url_Link = {http://proceedings.mlr.press/v15/zhou11a.html},
	Url_PDF = {http://proceedings.mlr.press/v15/zhou11a/zhou11a.pdf},
	Url_Slide = {Papers/dHBP_AISTATS2011_MingyuanZhou_04122011.pdf},
	Url_Video = {http://videolectures.net/aistats2011_zhou_dependent/},
	Volume = {15},
	Year = {2011}}

@incollection{zhou2009non,
	Abstract = {Non-parametric Bayesian techniques are considered for learning dictionaries for sparse image representations, with applications in denoising, inpainting and compressive sensing (CS). The beta process is employed as a prior for learning the dictionary, and this non-parametric method naturally infers an appropriate dictionary size. The Dirichlet process and a probit stick-breaking process are also considered to exploit structure within an image. The proposed method can learn a sparse dictionary in situ; training images may be exploited if available, but they are not required. Further, the noise variance need not be known, and can be non-stationary. Another virtue of the proposed method is that sequential inference can be readily employed, thereby allowing scaling to large images. Several example results are presented, using both Gibbs and variational Bayesian inference, with comparisons to other state-of-the-art approaches.},
	Author = {Zhou, Mingyuan and Haojun Chen and Lu Ren and Sapiro, Guillermo and Carin, Lawrence and John W. Paisley},
	Booktitle = {Advances in Neural Information Processing Systems},
	Editor = {Y. Bengio and D. Schuurmans and J. D. Lafferty and C. K. I. Williams and A. Culotta},
	Note = {(Oral presentation)},
	Pages = {2295--2303},
	Publisher = {Curran Associates, Inc.},
	Title = {Non-Parametric {B}ayesian Dictionary Learning for Sparse Image Representations},
	Url_Code = {Softwares/BPFA_Denoising_Inpainting_codes_Inference_10292009.zip},
	Url_Link = {http://papers.nips.cc/paper/3851-non-parametric-bayesian-dictionary-learning-for-sparse-image-representations},
	Url_PDF = {http://papers.nips.cc/paper/3851-non-parametric-bayesian-dictionary-learning-for-sparse-image-representations.pdf},
	Url_Slide = {Papers/MingyuanZhou_NIPS_12082009.pdf},
	Url_Video = {http://videolectures.net/mingyuan_zhou/},
	Year = {2009}}
