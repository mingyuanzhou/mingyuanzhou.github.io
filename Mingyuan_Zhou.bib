

@article{han2022card,
      title={{CARD}: Classification and Regression Diffusion Models}, 
      author={Xizewen Han and Huangjie Zheng and Mingyuan Zhou},
      year={2022},
journal={arXiv preprint arXiv:2206.07275},
  url={https://arxiv.org/abs/2206.07275},
  pdf={https://arxiv.org/pdf/2206.07275},
  url_arxiv={https://arxiv.org/abs/2206.07275},
  Note = {(the first two authors contributed equally)},
      abstract={Learning the distribution of a continuous or categorical response variable y given its covariates x is a fundamental problem in statistics and machine learning. Deep neural network-based supervised learning algorithms have made great progress in predicting the mean of y given x, but they are often criticized for their ability to accurately capture the uncertainty of their predictions. In this paper, we introduce classification and regression diffusion (CARD) models, which combine a denoising diffusion-based conditional generative model and a pre-trained conditional mean estimator, to accurately predict the distribution of y given x.  We demonstrate the outstanding ability of CARD in conditional distribution prediction with both toy examples and real-world datasets, the experimental results on which show that CARD in general outperforms state-of-the-art methods, including Bayesian neural network-based ones that are designed for uncertainty estimation, especially when the conditional distribution of y given x is multi-modal. 
}
}


@article{wang2022probabilistic,
      title = {Probabilistic Conformal Prediction Using Conditional Random Samples},
      author={Wang, Zhendong and Gao, Ruijiang and Yin, Mingzhang and Zhou, Mingyuan and Blei, David M.},
      year={2022},
journal={arXiv preprint arXiv:2206.06584},
  url = {https://arxiv.org/abs/2206.06584},
  pdf={https://arxiv.org/pdf/2206.06584},
  url_arxiv={https://arxiv.org/abs/2206.06584},
  Note = {(the first three authors contributed equally)},
abstract={This paper proposes probabilistic conformal prediction (PCP), a predictive inference algorithm that estimates a target variable by a discontinuous predictive set. Given inputs, PCP construct the predictive set based on random samples from an estimated generative model. It is efficient and compatible with either explicit or implicit conditional generative models. Theoretically, we show that PCP guarantees correct marginal coverage with finite samples. Empirically, we study PCP on a variety of simulated and real datasets. Compared to existing methods for conformal inference, PCP provides sharper predictive sets.}
}



@article{wang2022diffusion,
      title={Diffusion-{GAN}: Training {GANs} with Diffusion}, 
      author={Zhendong Wang and Huangjie Zheng and Pengcheng He and Weizhu Chen and Mingyuan Zhou},
      year={2022},
journal={arXiv preprint arXiv:2206.02262},
  url={https://arxiv.org/abs/2206.02262},
  pdf={https://arxiv.org/pdf/2206.02262.pdf},
  url_arxiv={https://arxiv.org/abs/2206.02262},
    url_code={https://github.com/Zhendong-Wang/Diffusion-GAN},
      abstract={For stable training of generative adversarial networks (GANs), injecting instance noise into the input of the discriminator is considered as a theoretically sound solution, which, however, has not yet delivered on its promise in practice. This paper introduces Diffusion-GAN that employs a Gaussian mixture distribution, defined over all the diffusion steps of a forward diffusion chain, to inject instance noise. A random sample from the mixture, which is diffused from an observed or generated data, is fed as the input to the discriminator. The generator is updated by backpropagating its gradient through the forward diffusion chain, whose length is adaptively adjusted to control the maximum noise-to-data ratio allowed at each training step. Theoretical analysis verifies the soundness of the proposed Diffusion-GAN, which provides model- and domain-agnostic differentiable augmentation. A rich set of experiments on diverse datasets show that Diffusion-GAN can provide stable and data-efficient GAN training, bringing consistent performance improvement over strong GAN baselines for synthesizing photo-realistic images.
}
}


@inproceedings{yang2022regularizing,
title={Regularizing a Model-based Policy Stationary Distribution to Stabilize Offline Reinforcement Learning},
author={Shentao Yang and Yihao Feng and Shujian Zhang and Mingyuan Zhou},
booktitle={ICML 2022: International Conference on Machine Learning},
month={July},
year={2022},
url={https://arxiv.org/abs/2206.07166},
pdf={https://arxiv.org/pdf/2206.07166},
url_arxiv={https://arxiv.org/abs/2206.07166},
abstract={Offline reinforcement learning (RL) extends the paradigm of classical RL algorithms to purely learning from static datasets, without interacting with the underlying environment during the learning process. A key challenge of offline RL is the instability of policy training, caused by the mismatch between the distribution of the offline data and the undiscounted stationary state-action distribution of the learned policy. To avoid the detrimental impact of distribution mismatch, we regularize the undiscounted stationary distribution of the current policy towards the offline data during the policy optimization process. Further, we train a dynamics model to both implement this regularization and better estimate the stationary distribution of the current policy, reducing the error induced by distribution mismatch. On a wide range of continuous-control offline RL datasets, our method indicates competitive performance, which validates our algorithm. The code is publicly available.}
}

@inproceedings{chen2022deep,
title={Deep Variational Graph Convolutional Recurrent Network for Multivariate Time Series Anomaly Detection},
author={Wenchao Chen and Long Tian and Bo Chen and Liang Dai and Zhibin Duan and Mingyuan Zhou},
booktitle={ICML 2022: International Conference on Machine Learning},
month={July},
year={2022},
Note = {(the first two authors contributed equally)},
url={},
pdf={},
url_arxiv={},
abstract={}
}

@inproceedings{duan2022bayesian,
title={Bayesian Deep Embedding Topic Meta-Learner},
author={Zhibin Duan and Yishi Xu and Jianqiao Sun and Bo Chen and Wenchao Chen and Chaojie Wang and Mingyuan Zhou},
booktitle={ICML 2022: International Conference on Machine Learning},
month={July},
year={2022},
Note = {(the first two authors contributed equally)},
url={},
pdf={},
url_arxiv={},
abstract={}
}


@inproceedings{zhang2022allsh,
title={{ALLSH}: Active Learning Guided by Local Sensitivity and Hardness},
author={Zhang, Shujian and Gong, Chengyue and Liu, Xingchao and He, Pengcheng and Chen, Weizhu and Zhou, Mingyuan},
booktitle={Findings of NAACL 2022: Annual Conference of the North American Chapter of the Association for Computational Linguistics},
year={2022},
url={https://arxiv.org/abs/2205.04980},
pdf={https://arxiv.org/pdf/2205.04980.pdf},
url_arxiv={https://arxiv.org/abs/2205.04980},
      abstract={Active learning, which effectively collects informative unlabeled data for annotation, reduces the demand for labeled data. In this work, we propose to retrieve unlabeled samples with a local sensitivity and hardness-aware acquisition function. The proposed method generates data copies through local perturbations and selects data points whose predictive likelihoods diverge the most from their copies. We further empower our acquisition function by injecting the select-worst case perturbation. Our method achieves consistent gains over the commonly used active learning strategies in various classification tasks. Furthermore, we observe consistent improvements over the baselines on the study of prompt selection in prompt-based few-shot learning. These experiments demonstrate that our acquisition guided by local sensitivity and hardness can be effective and beneficial for many NLP tasks.}
}


@article{yang2022regularized,
      title={A Regularized Implicit Policy for Offline Reinforcement Learning}, 
      author={Shentao Yang and Zhendong Wang and Huangjie Zheng and Yihao Feng and Mingyuan Zhou},
      year={2022},
journal={arXiv preprint arXiv:2202.09673},
  url={https://arxiv.org/abs/2202.09673},
  pdf={https://arxiv.org/pdf/2202.09673.pdf},
  url_arxiv={https://arxiv.org/abs/2202.09673},
      abstract={Offline reinforcement learning enables learning from a fixed dataset, without further interactions with the environment. The lack of environmental interactions makes the policy training vulnerable to state-action pairs far from the training dataset and prone to missing rewarding actions. For training more effective agents, we propose a framework that supports learning a flexible yet well-regularized fully-implicit policy. We further propose a simple modification to the classical policy-matching methods for regularizing with respect to the dual form of the Jensen--Shannon divergence and the integral probability metrics. We theoretically show the correctness of the policy-matching approach, and the correctness and a good finite-sample property of our modification. An effective instantiation of our framework through the GAN structure is provided, together with techniques to explicitly smooth the state-action mapping for robust generalization beyond the static dataset. Extensive experiments and ablation study on the D4RL dataset validate our framework and the effectiveness of our algorithmic designs.
}
}


@article{zheng2022truncated,
      title={Truncated Diffusion Probabilistic Models}, 
      author={Huangjie Zheng and Pengcheng He and Weizhu Chen and Mingyuan Zhou},
      year={2022},
journal={arXiv preprint arXiv:2202.09671},
  url={https://arxiv.org/abs/2202.09671},
  pdf={https://arxiv.org/pdf/2202.09671.pdf},
  url_arxiv={https://arxiv.org/abs/2202.09671},
  url_code={https://github.com/JegZheng/truncated-diffusion-probabilistic-models
},
      abstract={Employing a forward Markov diffusion chain to gradually map the data to a noise distribution, diffusion probabilistic models learn how to generate the data by inferring a reverse Markov diffusion chain to invert the forward diffusion process. To achieve competitive data generation performance, they demand a long diffusion chain that makes them computationally intensive in not only training but also generation. To significantly improve the computation efficiency, we propose to truncate the forward diffusion chain by abolishing the requirement of diffusing the data to random noise. Consequently, we start the inverse diffusion chain from an implicit generative distribution, rather than random noise, and learn its parameters by matching it to the distribution of the data corrupted by the truncated forward diffusion chain. Experimental results show our truncated diffusion probabilistic models provide consistent improvements over the non-truncated ones in terms of the generation performance and the number of required inverse diffusion steps.
}
}



@article{zheng2022mixing,
      title={Mixing and Shifting: Exploiting Global and Local Dependencies in Vision {MLP}s}, 
      author={Huangjie Zheng and Pengcheng He and Weizhu Chen and Mingyuan Zhou},
      year={2022},
journal={arXiv preprint arXiv:2202.06510},
  url={https://arxiv.org/abs/2202.06510},
  pdf={https://arxiv.org/pdf/2202.06510.pdf},
  url_arxiv={https://arxiv.org/abs/2202.06510},
      abstract={Token-mixing multi-layer perceptron (MLP) models have shown competitive performance in computer vision tasks with a simple architecture and relatively small computational cost. Their success in maintaining computation efficiency is mainly attributed to avoiding the use of self-attention that is often computationally heavy, yet this is at the expense of not being able to mix tokens both globally and locally. In this paper, to exploit both global and local dependencies without self-attention, we present Mix-Shift-MLP (MS-MLP) which makes the size of the local receptive field used for mixing increase with respect to the amount of spatial shifting. In addition to conventional mixing and shifting techniques, MS-MLP mixes both neighboring and distant tokens from fine- to coarse-grained levels and then gathers them via a shifting operation. This directly contributes to the interactions between global and local tokens. Being simple to implement, MS-MLP achieves competitive performance in multiple vision benchmarks. For example, an MS-MLP with 85 million parameters achieves 83.8% top-1 classification accuracy on ImageNet-1K. Moreover, by combining MS-MLP with state-of-the-art Vision Transformers such as the Swin Transformer, we show MS-MLP achieves further improvements on three different model scales, e.g., by 0.5% on ImageNet-1K classification with Swin-B. The code is available at https://github.com/JegZheng/MS-MLP.
}
}



@article{he2022variational,
      title={A Variational Edge Partition Model for Supervised Graph Representation Learning}, 
      author={Yilin He and Chaojie Wang and Hao Zhang and Bo Chen and Mingyuan Zhou},
      year={2022},
journal={arXiv preprint arXiv:2202.03233},
  url={https://arxiv.org/abs/2202.03233},
  pdf={https://arxiv.org/pdf/2202.03233.pdf},
  url_arxiv={https://arxiv.org/abs/2202.03233},
      abstract={Graph neural networks (GNNs), which propagate the node features through the edges and learn how to transform the aggregated features under label supervision, have achieved great success in supervised feature extraction for both node-level and graph-level classification tasks. However, GNNs typically treat the graph structure as given and ignore how the edges are formed. This paper introduces a graph generative process to model how the observed edges are generated by aggregating the node interactions over a set of overlapping node communities, each of which contributes to the edges via a logical OR mechanism. Based on this generative model, we partition each edge into the summation of multiple community-specific weighted edges and use them to define community-specific GNNs. A variational inference framework is proposed to jointly learn a GNN based inference network  that partitions the edges into different communities, these community-specific GNNs, and a GNN based predictor that combines community-specific GNNs for the end classification task. Extensive evaluations on real-world graph datasets have verified the effectiveness of the proposed method in learning discriminative representations for both node-level and graph-level classification tasks. 
}
}


@article{chen2022infinite,
  title={Infinite Switching Dynamic Probabilistic Network With {B}ayesian Nonparametric Learning},
  author={Chen, Wenchao and Chen, Bo and Liu, Yicheng and Wang, Chaojie and Peng, Xiaojun and Liu, Hongwei and Zhou, Mingyuan},
  journal={IEEE Transactions on Signal Processing},
  volume={70},
  pages={2224--2238},
  year={2022},
  url={https://ieeexplore.ieee.org/abstract/document/9738490},
  publisher={IEEE},
  abstract={To model sequentially observed multivariate nonstationary count data, we propose a switching Poisson-gamma dynamical systems (SPGDS), a dynamic probabilistic network with switching mechanism. Different from previous models, SPGDS assigns its latent variables into mixture of gamma distributed parameters to model complex sequences and describe the nonlinear dynamics, meanwhile, capture various temporal dependencies. Moreover, SPGDS can model all discrete and nonnegative real data by linking them to latent counts. To take advantage of Bayesian nonparametrics in handling the unknown number of mixture components, we integrate Dirichlet process (DP) mixture into SPGDS and develop an infinite switching Poisson-gamma dynamical systems (iSPGDS). For efficient and nonparametric inference, we develop a infinite switching recurrent variational inference network, combined with a scalable hybrid stochastic gradient-MCMC and variational inference method, which is scalable to large scale sequences and fast in out-of-sample prediction. Besides, to handle the time-series categorization task, we further propose an supervised attention iSPGDS (attn-iSPGDS), which combines the representation power of iSPGDS, discriminative power of deep neural networks, and selection power of the attention mechanism under a principled probabilistic framework. Experiments on both unsupervised and supervised tasks demonstrate that the proposed model not only has excellent fitting and prediction performance on complex sequences, but also separates different dynamical patterns within them.}
}



@inproceedings{
wang2022representing,
title={Representing Mixtures of Word Embeddings with Mixtures of Topic Embeddings},
author={Dongsheng Wang and Dandan Guo and He Zhao and Huangjie Zheng and Korawat Tanwisuth and Bo Chen and Mingyuan Zhou},
booktitle={ICLR 2022: International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=IYMuTbGzjFU},
pdf={https://openreview.net/pdf?id=IYMuTbGzjFU},
url_code={https://github.com/wds2014/WeTe}
}

@inproceedings{
guo2022learning,
title={Learning Prototype-oriented Set Representations for Meta-Learning},
author={Dandan Guo and Long Tian and Minghe Zhang and Mingyuan Zhou and Hongyuan Zha},
booktitle={ICLR 2022: International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=WH6u2SvlLp4},
pdf={https://openreview.net/pdf?id=WH6u2SvlLp4},
url_code={https://openreview.net/attachment?id=WH6u2SvlLp4&name=supplementary_material},
abstract={Learning from set-structured data is a fundamental problem that has recently attracted increasing attention, where a series of summary networks are introduced to deal with the set input. In fact, many meta-learning problems can be treated as set-input tasks. Most existing summary networks aim to design different architectures for the input set in order to enforce permutation invariance. However, scant attention has been paid to the common cases where different sets in a meta distribution are closely related and share certain statistical properties. Viewing each set as a distribution over a set of global prototypes, this paper provides a novel prototype-oriented optimal transport (POT) framework to improve existing summary networks. To learn the distribution over the global prototypes, we minimize its regularized optimal transport distance to the set empirical distribution over data points, providing a natural unsupervised way to improve the summary network. Since our plug-and-play framework can be applied to many meta learning problems, we further instantiate it to the cases of few-shot classification and implicit meta generative modeling. Extensive experiments demonstrate that our framework significantly improves the existing summary networks on learning more powerful summary statistics from sets and can be successfully integrated into metric-based few-shot classification and generative modeling applications, providing a promising tool for addressing set-input and meta-learning problems.}
}

@inproceedings{
chi2022meta,
title={Meta Discovery: Learning to Discover Novel Classes given Very Limited Data},
author={Haoang Chi and Feng Liu and Wenjing Yang and Long Lan and Tongliang Liu and Bo Han and Gang Niu and Mingyuan Zhou and Masashi Sugiyama},
booktitle={ICLR 2022: International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=MEpKGLsY8f},
pdf={https://openreview.net/pdf?id=MEpKGLsY8f},
url_code={https://github.com/Haoang97/MEDI},
abstract={In novel class discovery (NCD), we are given labeled data from seen classes and unlabeled data from unseen classes, and we train clustering models for the unseen classes. However, the implicit assumptions behind NCD are still unclear. In this paper, we demystify assumptions behind NCD and find that high-level semantic features should be shared among the seen and unseen classes. Based on this finding, NCD is theoretically solvable under certain assumptions and can be naturally linked to meta-learning that has exactly the same assumption as NCD. Thus, we can empirically solve the NCD problem by meta-learning algorithms after slight modifications. This meta-learning-based methodology significantly reduces the amount of unlabeled data needed for training and makes it more practical, as demonstrated in experiments. The use of very limited data is also justified by the application scenario of NCD: since it is unnatural to label only seen-class data, NCD is sampling instead of labeling in causality. Therefore, unseen-class data should be collected on the way of collecting seen-class data, which is why they are novel and first need to be clustered.}
}


@inproceedings{zheng2021exploiting,
title={Exploiting Chain Rule and {B}ayes' Theorem to Compare Probability Distributions},
author={Huangjie Zheng and Mingyuan Zhou},
booktitle = {NeurIPS 2021: Neural Information Processing Systems},
month={Dec.},
year = {2021},
url={https://openreview.net/forum?id=f-ggKIDTu5D},
pdf={Papers/CT_NeurIPS2021.pdf},
url_arxiv={https://arxiv.org/abs/2012.14100},
url_code={https://github.com/JegZheng/CT-pytorch},
abstract={To measure the difference between two probability distributions, referred to as the source and target, respectively, we exploit both the chain rule and Bayes' theorem to construct conditional transport (CT), which is constituted by both a forward component and a backward one. The forward CT is the expected cost of moving a source data point to a target one, with their joint distribution defined by the product of the source probability density function (PDF) and a source-dependent conditional distribution, which is related to the target PDF via Bayes' theorem. The backward CT is defined by reversing the direction. The CT cost can be approximated by replacing the source and target PDFs with their discrete empirical distributions supported on mini-batches, making it amenable to implicit distributions and stochastic gradient descent-based optimization. When applied to train a generative model, CT is shown to strike a good balance between mode-covering and mode-seeking behaviors and strongly resist mode collapse. On a wide variety of benchmark datasets for generative modeling, substituting the default statistical distance of an existing generative adversarial network with CT is shown to consistently improve the performance. PyTorch-style code is provided.}
}


@inproceedings{tanwisuth2021prototype,
  title={A Prototype-Oriented Framework for Unsupervised Domain Adaptation},
  author={Korawat Tanwisuth and Xinjie Fan and Huangjie Zheng and Shujian Zhang and Hao Zhang and Bo Chen and Mingyuan Zhou},
booktitle = {NeurIPS 2021: Neural Information Processing Systems},
month={Dec.},
url={https://openreview.net/forum?id=yH2VrkpiCK6},
pdf={https://arxiv.org/pdf/2110.12024.pdf},
url_arxiv={https://arxiv.org/abs/2110.12024},
url_code={https://github.com/korawat-tanwisuth/Proto_DA},
Note = {(the first three authors contributed equally)},
year = {2021},
abstract={Existing methods for unsupervised domain adaptation often rely on minimizing some statistical distance between the source and target samples in the latent space. To avoid the sampling variability, class imbalance, and data-privacy concerns that often plague these methods, we instead provide a memory and computation-efficient probabilistic framework to extract class prototypes and align the target features with them. We demonstrate the general applicability of our method on a wide range of scenarios, including single-source, multi-source, class-imbalance, and source-private domain adaptation. Requiring no additional model parameters and having a moderate increase in computation over the source model alone, the proposed method achieves competitive performance with state-of-the-art methods.}
}


@inproceedings{dimitriev2021carms,
  title={{CARMS}: Categorical-Antithetic-{REINFORCE} Multi-Sample Gradient Estimator},
  author={Dimitriev, Aleksandar and Zhou, Mingyuan},
booktitle = {NeurIPS 2021: Neural Information Processing Systems},
month={Dec.},
year = {2021},
url={https://openreview.net/forum?id=kwOjbvNyM-K},
pdf={https://arxiv.org/pdf/2110.14002.pdf},
url_arxiv={https://arxiv.org/abs/2110.14002?context=cs},
url_code={https://github.com/alekdimi/carms},
abstract={Accurately backpropagating the gradient through categorical variables is a challenging task that arises in various domains, such as training discrete latent variable models. To this end, we propose CARMS, an unbiased estimator for categorical random variables based on multiple mutually negatively correlated (jointly antithetic) samples. CARMS combines REINFORCE with copula based sampling to avoid duplicate samples and reduce its variance, while keeping the estimator unbiased using importance sampling. It generalizes both the ARMS antithetic estimator for binary variables, which is CARMS for two categories, as well as LOORF/VarGrad, the leave-one-out REINFORCE estimator, which is CARMS with independent samples.  We evaluate CARMS on several benchmark datasets on a generative modeling task, as well as a structured output prediction task, and find it to outperform competing methods including a strong self-control baseline. The code is publicly available.}
}




	
@inproceedings{armandpour2021convex,
  title={Convex Polytope Trees 
},
  author={Mohammadreza Armandpour and Ali Sadeghian and Mingyuan Zhou
},
url={https://openreview.net/forum?id=MvGKpmPsN7c},
pdf={https://arxiv.org/pdf/2010.11266.pdf},
url_arxiv={https://arxiv.org/abs/2010.11266},
url_code={https://github.com/rezaarmand/Convex_Polytope_Trees},
booktitle = {NeurIPS 2021: Neural Information Processing Systems},
month={Dec.},
year = {2021},
abstract={A decision tree is commonly restricted to use a single hyperplane to split the covariate space at each of its internal nodes. It often requires a large number of nodes to achieve high accuracy. In this paper, we propose convex polytope trees (CPT) to expand the family of decision trees by an interpretable generalization of their decision boundary. The splitting function at each node of CPT is based on the logical disjunction of a community of differently weighted probabilistic linear decision-makers, which also geometrically corresponds to a convex polytope in the covariate space. We use a nonparametric Bayesian prior at each node to infer the community's size, encouraging simpler decision boundaries by shrinking the number of polytope facets. We develop a greedy method to efficiently construct CPT and scalable end-to-end training algorithms for the tree parameters when the tree structure is given. We empirically demonstrate the efficiency of CPT over existing state-of-the-art decision trees in several real-world classification and regression tasks from diverse domains.}
}


@inproceedings{zhang2021alignment,
  title={Alignment Attention by Matching Key and Query Distributions},
  author={Shujian Zhang and Xinjie Fan and Huangjie Zheng and Korawat Tanwisuth and Mingyuan Zhou
},
url={https://openreview.net/forum?id=th788unrdTj},
pdf={https://arxiv.org/pdf/2110.12567.pdf},
url_arxiv={https://arxiv.org/abs/2110.12567},
url_code={https://github.com/szhang42/alignment_attention},
booktitle = {NeurIPS 2021: Neural Information Processing Systems},
month={Dec.},
year = {2021},
abstract={The neural attention mechanism has been incorporated into deep neural networks to achieve state-of-the-art performance in various domains. Most such models use multi-head self-attention which is appealing for the ability to attend to information from different perspectives. This paper introduces alignment attention that explicitly encourages self-attention to match the distributions of the key and query within each head. The resulting alignment attention networks can be optimized as an unsupervised regularization in the existing attention framework. It is simple to convert any models with self-attention, including pre-trained ones, to the proposed alignment attention. On a variety of language understanding tasks, we show the effectiveness of our method in accuracy, uncertainty estimation, generalization across domains, and robustness to adversarial attacks. We further demonstrate the general applicability of our approach on graph attention and visual question answering, showing the great potential of incorporating our alignment method into various attention-related tasks.}
}


@inproceedings{duan2021topicnet,
  title={TopicNet: Semantic Graph-Guided Topic Discovery},
  author={Zhibin Duan and Yishi Xu  and Bo Chen and Dongsheng Wang and Chaojie Wang and Mingyuan Zhou},
booktitle = {NeurIPS 2021: Neural Information Processing Systems},
month={Dec.},
year = {2021},
url={https://openreview.net/forum?id=ZB8Du-E1KUz},
pdf={https://openreview.net/pdf?id=ZB8Du-E1KUz},
url_arxiv={https://arxiv.org/abs/2110.14286},
url_code={https://github.com/BoChenGroup/TopicNet},
abstract={Existing deep hierarchical topic models are able to extract semantically meaningful topics from a text corpus  in an unsupervised manner and automatically organize them into a topic hierarchy.  However, it is unclear how to incorporate prior belief such as knowledge graph to guide the learning of the topic hierarchy. To address this issue, we introduce TopicNet as a deep hierarchical topic model that can inject prior structural knowledge as inductive bias to influence the learning. TopicNet represents each topic as a Gaussian-distributed embedding vector, projects the topics of all layers into a shared embedding space, and explores both the symmetric and asymmetric similarities between Gaussian embedding vectors to incorporate prior semantic hierarchies. With a variational auto-encoding inference network,  the model parameters are optimized by minimizing the evidence lower bound and supervised loss via stochastic gradient descent. Experiments on widely used benchmark show that TopicNet outperforms related deep topic models on discovering deeper interpretable topics and mining better document representations.}
}

@inproceedings{wang2021probabilistic,
  title={Probabilistic Margins for Instance Reweighting in Adversarial Training
},
  author={Qizhou Wang and  Feng Liu and  Bo Han and  Tongliang Liu and  Chen Gong and  Gang Niu and  Mingyuan Zhou and  Masashi Sugiyama
},
booktitle = {NeurIPS 2021: Neural Information Processing Systems},
month={Dec.},
year = {2021},
url={https://openreview.net/forum?id=rg8gNkvs3u},
pdf={https://openreview.net/pdf?id=rg8gNkvs3u},
url_arxiv={https://arxiv.org/abs/2106.07904},
url_code={https://github.com/QizhouWang/MAIL},
abstract={Reweighting adversarial data during training has been recently shown to improve adversarial robustness, where data closer to the current decision boundaries are regarded as more critical and given larger weights. However, existing methods measuring the closeness are not very reliable: they are discrete and can take only a few values, and they are path-dependent, i.e., they may change given the same start and end points with different attack paths. In this paper, we propose three types of probabilistic margin (PM), which are continuous and path-independent, for measuring the aforementioned closeness and reweighing adversarial data. Specifically, a PM is defined as the difference between two estimated class-posterior probabilities, e.g., such a probability of the true label minus the probability of the most confusing label given some natural data. Though different PMs capture different geometric properties, all three PMs share a negative correlation with the vulnerability of data: data with larger/smaller PMs are safer/riskier and should have smaller/larger weights. Experiments demonstrated that PMs are reliable and PM-based reweighting methods outperformed state-of-the-art counterparts.}
}


@ARTICLE{zhang2021learning,
  author={Zhang, Hao and Wang, Chaojie and Wang, Zhengjue and Duan, Zhibin and Chen, Bo and Zhou, Mingyuan and Henao, Ricardo and Carin, Lawrence},
  journal={IEEE Transactions on Neural Networks and Learning Systems}, 
  title={Learning Hierarchical Document Graphs From Multilevel Sentence Relations}, 
  year={2021},
  volume={},
  number={},
  pages={1-13},
  doi={10.1109/TNNLS.2021.3113297},
  url={https://ieeexplore.ieee.org/document/9555254},
  pdf={https://ieeexplore.ieee.org/document/9555254},
  abstract={Organizing the implicit topology of a document as a graph, and further performing feature extraction via the graph convolutional network (GCN), has proven effective in document analysis. However, existing document graphs are often restricted to expressing single-level relations, which are predefined and independent of downstream learning. A set of learnable hierarchical graphs are built to explore multilevel sentence relations, assisted by a hierarchical probabilistic topic model. Based on these graphs, multiple parallel GCNs are used to extract multilevel semantic features, which are aggregated by an attention mechanism for different document-comprehension tasks. Equipped with variational inference, the graph construction and GCN are learned jointly, allowing the graphs to evolve dynamically to better match the downstream task. The effectiveness and efficiency of the proposed multilevel sentence relation graph convolutional network (MuserGCN) is demonstrated via experiments on document classification, abstractive summarization, and matching.}
  }


@article{UnniYaoHanZhouZheng+2021+4057+4065,
author = {Rohit Unni and Kan Yao and Xizewen Han and Mingyuan Zhou and Yuebing Zheng},
doi = {doi:10.1515/nanoph-2021-0392},
url = {https://doi.org/10.1515/nanoph-2021-0392},
pdf={https://www.degruyter.com/document/doi/10.1515/nanoph-2021-0392/pdf},
title = {A mixture-density-based tandem optimization network for on-demand inverse design of thin-film high reflectors},
journal = {Nanophotonics},
number = {16},
volume = {10},
year = {2021},
pages = {4057--4065}
}




@inproceedings{Zhang2021bayesian,
  title={Bayesian Attention Belief Networks},
  author={Shujian Zhang and Xinjie Fan and Bo Chen and Mingyuan Zhou},
booktitle = {ICML 2021: International Conference on Machine Learning},
month={July},
Note = {(the first two authors contributed equally)},
year={2021},
url={http://proceedings.mlr.press/v139/zhang21f.html},
pdf={https://arxiv.org/pdf/2106.05251.pdf},
url_arxiv={https://arxiv.org/abs/2106.05251},
abstract={Attention-based neural networks have achieved state-of-the-art results on a wide range of tasks. Most such models use deterministic attention while stochastic attention is less explored due to the optimization difficulties or complicated model design. This paper introduces Bayesian attention belief networks, which construct a decoder network by modeling unnormalized attention weights with a hierarchy of gamma distributions, and an encoder network by stacking Weibull distributions with a deterministic-upward-stochastic-downward structure to approximate the posterior. The resulting auto-encoding networks can be optimized in a differentiable way with a variational lower bound. It is simple to convert any models with deterministic attention, including pretrained ones, to the proposed Bayesian attention belief networks. On a variety of language understanding tasks, we show that our method outperforms deterministic attention and state-of-the-art stochastic attention in accuracy, uncertainty estimation, generalization across domains, and robustness to adversarial attacks. We further demonstrate the general applicability of our method on neural machine translation and visual question answering, showing great potential of incorporating our method into various attention-related tasks.}
}


@inproceedings{dimitriev2021arms,
title={{ARMS}: {A}ntithetic-{REINFORCE}-Multi-Sample Gradient for Binary Variables},
author={Aleksandar Dimitriev and Mingyuan Zhou},
booktitle={ICML 2021: International Conference on Machine Learning},
month={July},
year={2021},
url={https://proceedings.mlr.press/v139/dimitriev21a.html},
pdf={http://proceedings.mlr.press/v139/dimitriev21a/dimitriev21a.pdf},
url_arxiv={https://arxiv.org/abs/2105.14141},
url_code={https://github.com/alekdimi/arms},
abstract={Estimating the gradients for binary variables is a task that arises frequently in various domains, such as training discrete latent variable models. What has been commonly used is a REINFORCE based Monte Carlo estimation method that uses either independent samples or pairs of negatively correlated samples. To better utilize more than two samples, we propose ARMS, an Antithetic REINFORCE-based Multi-Sample gradient estimator. ARMS uses a copula to generate any number of mutually antithetic samples. It is unbiased, has low variance, and generalizes both DisARM, which we show to be ARMS with two samples, and the leave-one-out REINFORCE (LOORF) estimator, which is ARMS with uncorrelated samples. We evaluate ARMS on several datasets for training generative models, and our experimental results show that it outperforms competing methods. We also develop a version of ARMS for optimizing the multi-sample variational bound, and show that it outperforms both VIMCO and DisARM. The code is publicly available.}
}



@inproceedings{duan2021sawtooth,
title={Sawtooth Factorial Topic Embeddings Guided Gamma Belief Network},
author={Zhibin Duan and Dongsheng Wang and Bo Chen and Chaojie Wang and Wenchao Chen and Yewen Li and Jie Ren and Mingyuan Zhou},
booktitle={ICML 2021: International Conference on Machine Learning},
month={July},
year={2021},
url={https://arxiv.org/abs/2107.02757},
pdf={https://arxiv.org/pdf/2107.02757.pdf},
url_arxiv={https://arxiv.org/abs/2107.02757},
url_code={https://github.com/BoChenGroup/SawETM},
Note = {(the first two authors contributed equally)},
abstract={Hierarchical topic models such as the gamma belief network (GBN) have delivered promising results in mining multi-layer document representations and discovering interpretable topic taxonomies. However, they often assume in the prior that the topics at each layer are independently drawn from the Dirichlet distribution, ignoring the dependencies between the topics both at the same layer and across different layers. To relax this assumption, we propose sawtooth factorial topic embedding guided GBN, a deep generative model of documents that captures the dependencies and semantic similarities between the topics in the embedding space. Specifically, both the words and topics are represented as embedding vectors of the same dimension. The topic matrix at a layer is factorized into the product of a factor loading matrix and a topic embedding matrix, the transpose of which is set as the factor loading matrix of the layer above. Repeating this particular type of factorization, which shares components between adjacent layers, leads to a structure referred to as sawtooth factorization. An auto-encoding variational inference network is constructed to optimize the model parameter via stochastic gradient descent. Experiments on big corpora show that our models outperform other neural topic models on extracting deeper interpretable topics and deriving better document representations.}
}




@article{zheng2021contrastive,
      title={Contrastive Attraction and Contrastive Repulsion for Representation Learning}, 
      author={Huangjie Zheng and Xu Chen and Jiangchao Yao and Hongxia Yang and Chunyuan Li and Ya Zhang and Hao Zhang and Ivor W. Tsang and Jingren Zhou and Mingyuan Zhou},
      month={May},
      year={2021},
      journal={arXiv preprint arXiv:2105.03746},
  url={https://arxiv.org/abs/2105.03746},
  pdf={https://arxiv.org/pdf/2105.03746.pdf},
  url_arxiv={https://arxiv.org/abs/2105.03746},
      abstract={Contrastive learning (CL) is effective in learning data representations without label supervision, where the encoder needs to contrast each positive sample over multiple negative samples via a one-vs-many softmax cross-entropy loss. However, conventional CL is sensitive to how many negative samples are included and how they are selected. Proposed in this paper is a doubly CL strategy that contrasts positive samples and negative ones within themselves separately. We realize this strategy with contrastive attraction and contrastive repulsion (CACR) makes the query not only exert a greater force to attract more distant positive samples but also do so to repel closer negative samples. Theoretical analysis reveals the connection between CACR and CL from the perspectives of both positive attraction and negative repulsion and shows the benefits in both efficiency and robustness brought by separately contrasting within the sampled positive and negative pairs. Extensive large-scale experiments on standard vision tasks show that CACR not only consistently outperforms existing CL methods on benchmark datasets in representation learning, but also provides interpretable contrastive weights, demonstrating the efficacy of the proposed doubly contrastive strategy.}
}




@article{guo2022matching,
  title={Matching Visual Features to Hierarchical Semantic Topics for Image Paragraph Captioning},
  author={Guo, Dandan and Lu, Ruiying and Chen, Bo and Zeng, Zequn and Zhou, Mingyuan},
  journal={International Journal of Computer Vision},
  pages={1--18},
  year={2022},
    url={https://link.springer.com/article/10.1007/s11263-022-01624-6},
  pdf={https://arxiv.org/pdf/2105.04143.pdf},
  url_arxiv={https://arxiv.org/abs/2105.04143},
  Note = {(the first three authors contributed equally)},
      abstract={Observing a set of images and their corresponding paragraph-captions, a challenging task is to learn how to produce a semantically coherent paragraph to describe the visual content of an image. Inspired by recent successes in integrating semantic topics into this task, this paper develops a plug-and-play hierarchical-topic-guided image paragraph generation framework, which couples a visual extractor with a deep topic model to guide the learning of a language model. To capture the correlations between the image and text at multiple levels of abstraction and learn the semantic topics from images, we design a variational inference network to build the mapping from image features to textual captions. To guide the paragraph generation, the learned hierarchical topics and visual features are integrated into the language model, including Long Short-Term Memory and Transformer, and jointly optimized. Experiments on public datasets demonstrate that the proposed models, which are competitive with many state-of-the-art approaches in terms of standard evaluation metrics, can be used to both distill interpretable multi-layer semantic topics and generate diverse and coherent captions.},
  publisher={Springer}
}



@inproceedings{duan2021enslm,
title={{EnsLM}: Ensemble Language Model for Data Diversity by Semantic Clustering},
author={Zhibin Duan and Hao Zhang and Chaojie Wang and Zhengjue Wang and Bo Chen and Mingyuan Zhou
},
booktitle={ACL-IJCNLP 2021: The Joint Conference of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing},
url={https://aclanthology.org/2021.acl-long.230/},
pdf={https://aclanthology.org/2021.acl-long.230.pdf},
url_code={https://github.com/bochengroup/enslm},
month={Aug.},
year={2021}
}



@ARTICLE{wang2021multimodal,
  author={Wang, Chaojie and Chen, Bo and Xiao, Sucheng and Wang, Zhengjue and Zhang, Hao and Wang, Penghui and Han, Ning and Zhou, Mingyuan},
  journal={IEEE Transactions on Cybernetics}, 
  title={Multimodal {W}eibull Variational Autoencoder for Jointly Modeling Image-Text Data}, 
  year={2021},
  pages={1-16},
  url={https://ieeexplore.ieee.org/document/9417864?denied=},
  abstract={For multimodal representation learning, traditional black-box approaches often fall short of extracting interpretable multilayer hidden structures, which contribute to visualize the connections between different modalities at multiple semantic levels. To extract interpretable multimodal latent representations and visualize the hierarchial semantic relationships between different modalities, based on deep topic models, we develop a novel multimodal Poisson gamma belief network (mPGBN) that tightly couples the observations of different modalities via imposing sparse connections between their modality-specific hidden layers. To alleviate the time-consuming Gibbs sampler adopted by traditional topic models in the testing stage, we construct a Weibull-based variational inference network (encoder) to directly map the observations to their latent representations, and further combine it with the mPGBN (decoder), resulting in a novel multimodal Weibull variational autoencoder (MWVAE), which is fast in out-of-sample prediction and can handle large-scale multimodal datasets. Qualitative evaluations on bimodal data consisting of image-text pairs show that the developed MWVAE can successfully extract expressive multimodal latent representations for downstream tasks like missing modality imputation and multimodal retrieval. Further extensive quantitative results demonstrate that both MWVAE and its supervised extension sMWVAE achieve state-of-the-art performance on various multimodal benchmarks.}
}


@article{wen2021gradient,
  title={Gradient Estimation of Information Measures in Deep Learning},
  author={Wen, Liangjian and Bai, Haoli and He, Lirong and Zhou, Yiji and Zhou, Mingyuan and Xu, Zenglin},
  journal={Knowledge-Based Systems},
  pages={107046},
  year={2021},
  url={https://doi.org/10.1016/j.knosys.2021.107046},
  abstract={Information measures including entropy and mutual information (MI) have been widely applied in deep learning. Despite the successes, exiting estimation methods suffer from either high variance or high bias. This may lead to unstable training or poor performance in deep learning. Since estimating information measures in themselves is very difficult, we explore an alternative appealing strategy, by directly estimating the gradients of information measures with respect to model parameters. We propose a general gradient estimation method for information measures based on the score estimation. In detail, we establish the Entropy Gradient Estimator (EGE) and the Mutual Information Gradient Estimator (MIGE) to estimate the gradient of entropy and mutual information with respect to model parameters, respectively. For dealing with the optimization of entropy and mutual information, we can directly plug in their gradient approximation with relevant parameters to enable stochastic backpropagation for stability and efficiency. Our proposed method exhibits higher accuracy and lower variance for gradient estimation of information measures. Extensive experiments on various deep learning tasks have demonstrated the superiority of our method.}
}

@inproceedings{fan2021adversarially,
title={Adversarially Adaptive Normalization for Single Domain Generalization},
author={Xinjie Fan and Qifei Wang and Junjie Ke and Feng Yang and Boqing Gong and Mingyuan Zhou},
booktitle={CVPR 2021: Conference on Computer Vision and Pattern Recognition},
month={June},
year={2021},
url={http://arxiv.org/abs/2106.01899},
pdf={https://arxiv.org/pdf/2106.01899.pdf},
url_arxiv={https://arxiv.org/abs/2106.01899},
abstract={Single domain generalization aims to learn a model that performs well on many unseen domains with only one domain data for training. Existing works focus on studying the adversarial domain augmentation (ADA) to improve the model's generalization capability. The impact on domain generalization of the statistics of normalization layers is still underinvestigated. In this paper, we propose a generic normalization approach, adaptive standardization and rescaling normalization (ASR-Norm), to complement the missing part in previous works. ASR-Norm learns both the standardization and rescaling statistics via neural networks. This new form of normalization can be viewed as a generic form of the traditional normalizations. When trained with ADA, the statistics in ASR-Norm are learned to be adaptive to the data coming from different domains, and hence improves the model generalization performance across domains, especially on the target domain with large discrepancy from the source domain. The experimental results show that ASR-Norm can bring consistent improvement to the state-of-the-art ADA approaches by 1.6%, 2.7%, and 6.3% averagely on the Digits, CIFAR-10-C, and PACS benchmarks, respectively. As a generic tool, the improvement introduced by ASR-Norm is agnostic to the choice of ADA methods.}
}



@inproceedings{armandpour2021partition,
title={Partition-Guided GANs},
author={Mohammadreza Armandpour and Ali Sadeghian and  Chunyuan Li and  Mingyuan Zhou},
booktitle={CVPR 2021: Conference on Computer Vision and Pattern Recognition},
month={June},
year={2021},
url={https://www.computer.org/csdl/proceedings-article/cvpr/2021/450900f095/1yeI3P6bZQc},
pdf={https://arxiv.org/pdf/2104.00816.pdf},
url_arxiv={https://arxiv.org/abs/2104.00816},
url_code={https://github.com/alisadeghian/PGMGAN},
abstract={Despite the success of Generative Adversarial Networks (GANs), their training suffers from several well-known problems, including mode collapse and difficulties learning a disconnected set of manifolds. In this paper, we break down the challenging task of learning complex high dimensional distributions, supporting diverse data samples, to simpler sub-tasks. Our solution relies on designing a partitioner that breaks the space into smaller regions, each having a simpler distribution, and training a different generator for each partition. This is done in an unsupervised manner without requiring any labels. We formulate two desired criteria for the space partitioner that aid the training of our mixture of generators: 1) to produce connected partitions and 2) provide a proxy of distance between partitions and data samples, along with a direction for reducing that distance. These criteria are developed to avoid producing samples from places with non-existent data density, and also facilitate training by providing additional direction to the generators. We develop theoretical constraints for a space partitioner to satisfy the above criteria. Guided by our theoretical analysis, we design an effective neural architecture for the space partitioner that empirically assures these conditions. Experimental results on various standard benchmarks show that the proposed unsupervised model outperforms several recent methods.}
}




@article{cramer2022evaluation,
  title={Evaluation of individual and ensemble probabilistic forecasts of COVID-19 mortality in the United States},
  author={Cramer et al.},
  journal={Proceedings of the National Academy of Sciences},
  volume={119},
  number={15},
  pages={e2113561119},
  year={2022},
  publisher={National Acad Sciences},
  url={https://www.pnas.org/doi/abs/10.1073/pnas.2113561119},
  url_arxiv={https://www.medrxiv.org/content/10.1101/2021.02.03.21250974v3},
  abstract={Short-term probabilistic forecasts of the trajectory of the COVID-19 pandemic in the United States have served as a visible and important communication channel between the scientific modeling community and both the general public and decision-makers. Forecasting models provide specific, quantitative, and evaluable predictions that inform short-term decisions such as healthcare staffing needs, school closures, and allocation of medical supplies. Starting in April 2020, the US COVID-19 Forecast Hub (https://covid19forecasthub.org/) collected, disseminated, and synthesized tens of millions of specific predictions from more than 90 different academic, industry, and independent research groups. A multimodel ensemble forecast that combined predictions from dozens of groups every week provided the most consistently accurate probabilistic forecasts of incident deaths due to COVID-19 at the state and national level from April 2020 through October 2021. The performance of 27 individual models that submitted complete forecasts of COVID-19 deaths consistently throughout this year showed high variability in forecast skill across time, geospatial units, and forecast horizons. Two-thirds of the models evaluated showed better accuracy than a nave baseline model. Forecast accuracy degraded as models made predictions further into the future, with probabilistic error at a 20-wk horizon three to five times larger than when predicting at a 1-wk horizon. This project underscores the role that collaboration and active coordination between governmental public-health agencies, academic modeling teams, and industry partners can play in developing modern modeling capabilities to support local, state, and federal response to outbreaks.}
}




@inproceedings{Kalantari2021graph,
  title={Graph Gamma Process Linear Dynamical Systems},
  author={Rahi Kalantari and Mingyuan Zhou},
  booktitle={AISTATS 2021: International Conference on Artificial Intelligence and Statistics},
year={2021},
url={http://proceedings.mlr.press/v130/kalantari21a.html},
pdf={http://proceedings.mlr.press/v130/kalantari21a/kalantari21a.pdf},
abstract={We introduce graph gamma process (GGP) linear dynamical systems to model real-valued multivariate time series. GGP generates S latent states that are shared by K different communities, each of which is characterized by its own pattern of activation probabilities imposed on a S*S directed sparse graph, and allow both S and K to grow without bound. For temporal pattern discovery, the latent representation under the model is used to decompose the time series into a parsimonious set of multivariate sub-sequences generated by formed communities. In each sub-sequence, different data dimensions often share similar temporal patterns but may exhibit distinct magnitudes, and hence allowing the superposition of all sub-sequences to exhibit diverse behaviors at different data dimensions. On both synthetic and real-world time series, the proposed nonparametric Bayesian dynamic models, which are initialized at random, consistently exhibit good predictive performance in comparison to a variety of baseline models, revealing interpretable latent state transition patterns and decomposing the time series into distinctly behaved sub-sequences.}
}

@inproceedings{lotfi2021hyperbolic,
  title={Hyperbolic Graph Embedding with Enhanced Semi-Implicit Variational Inference},
  author={Ali Lotfi Rezaabad and Rahi Kalantari and Sriram Vishwanath and Mingyuan Zhou and Jonathan Tamir},
  booktitle={AISTATS 2021: International Conference on Artificial Intelligence and Statistics},
year={2021},
url={http://proceedings.mlr.press/v130/lotfi-rezaabad21a.html},
pdf={http://proceedings.mlr.press/v130/lotfi-rezaabad21a/lotfi-rezaabad21a.pdf},
url_code={https://github.com/utcsilab/ESI_HGE},
abstract={Hyperbolic graph embedding with enhanced semi-implicit variational inference.
AbstractEfficient modeling of relational data arising in physical, social, and information sciences is challenging due to complicated dependencies within the data. In this work we build off of semi-implicit graph variational auto-encoders to capture higher order statistics in a low-dimensional graph latent representation. We incorporate hyperbolic geometry in the latent space through a poincare embedding to efficiently represent graphs exhibiting hierarchical structure. To address the naive posterior latent distribution assumptions in classical variational inference, we use semi-implicit hierarchical variational Bayes to implicitly capture posteriors of given graph data, which may exhibit heavy tails, multiple modes, skewness, and highly correlated latent structures. We show that the existing semi-implicit variational inference objective provably reduces information in the observed graph. Based on this observation, we estimate and add an additional mutual information term to the semi-implicit variational inference learning objective to capture rich correlations arising between the input and latent spaces. We show that the inclusion of this regularization term in conjunction with the poincare embedding boosts the quality of learned high-level representations and enables more flexible and faithful graphical modeling. We experimentally demonstrate that our approach outperforms existing graph variational auto-encoders both in Euclidean and in hyperbolic spaces for edge link prediction and node classification.}
}



@inproceedings{fan2021contextual,
title={Contextual Dropout: An Efficient Sample-Dependent Dropout Module},
author={Xinjie Fan and Shujian Zhang and Korawat Tanwisuth and Xiaoning Qian and Mingyuan Zhou},
booktitle={ICLR 2021: International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=ct8_a9h1M},
pdf={https://openreview.net/pdf?id=ct8_a9h1M},
url_arxiv={https://arxiv.org/abs/2103.04181},
url_code={https://github.com/szhang42/Contextual_dropout_release},
abstract={Dropout has been demonstrated as a simple and effective module to not only regularize the training process of deep neural networks, but also provide the uncertainty estimation for prediction. However, the quality of uncertainty estimation is highly dependent on the dropout probabilities. Most current models use the same dropout distributions across all data samples due to its simplicity.  Despite the potential gains in the flexibility of modeling uncertainty, sample-dependent dropout, on the other hand, is less explored as it often encounters scalability issues or involves non-trivial model changes.  In this paper, we propose contextual dropout with an efficient structural design as a simple and scalable sample-dependent dropout module, which can be applied to a wide range of models at the expense of only slightly increased memory and computational cost. We learn the dropout probabilities with a variational objective, compatible with both Bernoulli dropout and Gaussian dropout. We apply the contextual dropout module to various models with applications to image classification and visual question answering and demonstrate the scalability of the method with large-scale datasets, such as ImageNet and VQA 2.0. Our experimental results show that the proposed method outperforms baseline methods in terms of both accuracy and quality of uncertainty estimation.}
}


@article{li2020self,
  title={Self-supervised Pre-training with Hard Examples Improves Visual Representations},
  author={Li, Chunyuan and Li, Xiujun and Zhang, Lei and Peng, Baolin and Zhou, Mingyuan and Gao, Jianfeng},
  journal={arXiv preprint arXiv:2012.13493},
  url={https://arxiv.org/abs/2012.13493},
  pdf={https://arxiv.org/pdf/2012.13493.pdf},
  url_arxiv={https://arxiv.org/abs/2012.13493},
  year={2020},
  abstract={Self-supervised pre-training (SSP) employs random image transformations to generate training data for visual representation learning. In this paper, we first present a modeling framework that unifies existing SSP methods as learning to predict pseudo-labels. Then, we propose new data augmentation methods of generating training examples whose pseudo-labels are harder to predict than those generated via random image transformations. Specifically, we use adversarial training and CutMix to create hard examples (HEXA) to be used as augmented views for MoCo-v2 and DeepCluster-v2, leading to two variants HEXA_{MoCo} and HEXA_{DCluster}, respectively. In our experiments, we pre-train models on ImageNet and evaluate them on multiple public benchmarks. Our evaluation shows that the two new algorithm variants outperform their original counterparts, and achieve new state-of-the-art on a wide range of tasks where limited task supervision is available for fine-tuning. These results verify that hard examples are instrumental in improving the generalization of the pre-trained models.}
}





@inproceedings{yue2020implicit,
  title={Implicit Distributional Reinforcement Learning},
  author={Yue, Yuguang and Wang, Zhendong and Zhou, Mingyuan},
booktitle = {NeurIPS 2020: Advances in Neural Information Processing Systems},
month={Dec.},
year = {2020},
url={https://arxiv.org/abs/2007.06159},
pdf={https://arxiv.org/pdf/2007.06159.pdf},
url_arxiv={https://arxiv.org/abs/2007.06159},
url_code={https://github.com/zhougroup/IDAC},
Note = {(the first two authors contributed equally)},
  abstract={To improve the sample efficiency of policy-gradient based reinforcement learning algorithms, we propose implicit distributional actor-critic (IDAC) that consists of a distributional critic, built on two deep generator networks (DGNs), and a semi-implicit actor (SIA), powered by a flexible policy distribution. We adopt a distributional perspective on the discounted cumulative return and model it with a state-action-dependent implicit distribution, which is approximated by the DGNs that take state-action pairs and random noises as their input. Moreover, we use the SIA to provide a semi-implicit policy distribution, which mixes the policy parameters with a reparameterizable distribution that is not constrained by an analytic density function. In this way, the policy's marginal distribution is implicit, providing the potential to model complex properties such as covariance structure and skewness, but its parameter and entropy can still be estimated. We incorporate these features with an off-policy algorithm framework to solve problems with continuous action space and compare IDAC with state-of-the-art algorithms on representative OpenAI Gym environments. We observe that IDAC outperforms these baselines in most tasks. Python code is provided.}
}


@inproceedings{Fan2020bayesian,
  title={Bayesian Attention Modules},
  author={Xinjie Fan and Shujian Zhang and Bo Chen and Mingyuan Zhou},
booktitle = {NeurIPS 2020: Advances in Neural Information Processing Systems},
month={Dec.},
year = {2020},
Note = {(the first two authors contributed equally)},
url={https://arxiv.org/abs/2010.10604},
url_arxiv={https://arxiv.org/abs/2010.10604},
pdf={https://arxiv.org/pdf/2010.10604.pdf},
url_code={https://github.com/zhougroup/BAM},
  abstract={Attention modules, as simple and effective tools, have not only enabled deep neural networks to achieve state-of-the-art results in many domains, but also enhanced their interpretability. Most current models use deterministic attention modules due to their simplicity and ease of optimization. Stochastic counterparts, on the other hand, are less popular despite their potential benefits. The main reason is that stochastic attention often introduces optimization issues or requires significant model changes. In this paper, we propose a scalable stochastic version of attention that is easy to implement and optimize. We construct simplex-constrained attention distributions by normalizing reparameterizable distributions, making the training process differentiable. We learn their parameters in a Bayesian framework where a data-dependent prior is introduced for regularization. We apply the proposed stochastic attention modules to various attention-based models, with applications to graph node classification, visual question answering, image captioning, machine translation, and language understanding. Our experiments show the proposed method brings consistent improvements over the corresponding baselines.}
}


@inproceedings{Chen2020bidirectional,
  title={Bidirectional Convolutional {P}oisson Gamma Dynamical Systems},
  author={Wenchao Chen and Chaojie Wang and Bo Chen and Yicheng Liu and Hao Zhang and Mingyuan Zhou},
booktitle = {NeurIPS 2020: Advances in Neural Information Processing Systems},
month={Dec.},
year = {2020},
url={https://proceedings.neurips.cc/paper/2020/hash/26178fc759d2b89c45dd31962f81dc61-Abstract.html
},
pdf={https://proceedings.neurips.cc/paper/2020/file/26178fc759d2b89c45dd31962f81dc61-Paper.pdf},
url_code={https://github.com/BoChenGroup/BCPGDS},
Note = {(the first two authors contributed equally)},
  abstract={Incorporating the natural document-sentence-word structure into hierarchical Bayesian modeling, we propose convolutional Poisson gamma dynamical systems (PGDS) that introduce not only word-level probabilistic convolutions, but also sentence-level stochastic temporal transitions. With word-level convolutions capturing phrase-level topics and sentence-level transitions capturing how the topic usages evolve over consecutive sentences, we aggregate the topic proportions of all sentences of a document as its feature representation. To consider not only forward but also backward sentence-level information transmissions, we further develop a bidirectional convolutional PGDS to incorporate the full contextual information to represent each sentence. For efficient inference, we construct a convolutional-recurrent inference network, which provides both sentence-level and document-level representations, and introduce a hybrid Bayesian inference scheme combining stochastic-gradient MCMC and amortized variational inference. Experimental results on a variety of document corpora demonstrate that the proposed models can extract expressive multi-level latent representations, including interpretable phrase-level topics and sentence-level temporal transitions as well as discriminative document-level features, achieving state-of-the-art document categorization performance while being memory and computation efficient.}
}


@inproceedings{Wang2020deep,
  title={Deep Relational Topic Modeling via Graph {P}oisson Gamma Belief Network},
  author={Chaojie Wang and Hao Zhang and Bo Chen and Dongsheng Wang and Zhengjue Wang and Mingyuan Zhou},
booktitle = {NeurIPS 2020: Advances in Neural Information Processing Systems},
month={Dec.},
year = {2020},
url={https://proceedings.neurips.cc/paper/2020/hash/05ee45de8d877c3949760a94fa691533-Abstract.html
},
pdf={https://proceedings.neurips.cc/paper/2020/file/05ee45de8d877c3949760a94fa691533-Paper.pdf
},
url_code={https://github.com/chaojiewang94/WGAAE},
Note = {(the first two authors contributed equally)},
  abstract={To analyze a collection of interconnected documents, relational topic models (RTMs) have been developed to describe both the link structure and document content, exploring their underlying relationships via a single-layer latent representation with limited expressive capability. To better utilize the document network, we first propose graph Poisson factor analysis (GPFA) that constructs a probabilistic model for interconnected documents and also provides closed-form Gibbs sampling update equations, moving beyond sophisticated approximate assumptions of existing RTMs. Extending GPFA, we develop a novel hierarchical RTM named graph Poisson gamma belief network (GPGBN), and further introduce two different Weibull distribution based variational graph auto-encoders for efficient model inference and effective network information aggregation. Experimental results demonstrate that our models extract high-quality hierarchical latent document representations, leading to improved performance over baselines on various graph analytic tasks.}
}

@inproceedings{Wang2020friendly,
  title={Friendly Topic Assistant for {T}ransformer Based Abstractive Summarization},
  author={Zhengjue Wang and Zhibin Duan and Hao Zhang and Chaojie Wang and Long Tian and Bo Chen and Mingyuan Zhou},
booktitle = {EMNLP 2020: Empirical Methods in Natural Language Processing},
month={Nov.},
year = {2020},
url={https://aclanthology.org/2020.emnlp-main.35/
},
pdf={https://aclanthology.org/2020.emnlp-main.35.pdf
},
  abstract={Abstractive document summarization is a comprehensive task including document understanding and summary generation, in which area Transformer-based models have achieved the state-of-the-art performance. Compared with Transformers, topic models are better at learning explicit document semantics, and hence could be integrated into Transformers to further boost their performance. To this end, we rearrange and explore the semantics learned by a topic model, and then propose a topic assistant (TA) including three modules. TA is compatible with various Transformer-based models and user-friendly since i) TA is a plug-and-play model that does not break any structure of the original Transformer network, making users easily fine-tune Transformer+TA based on a well pre-trained model; ii) TA only introduces a small number of extra parameters. Experimental results on three datasets demonstrate that TA is able to improve the performance of several Transformer-based models.}
}



@article{guo2020variational,
      title={Variational Temporal Deep Generative Model for Radar {HRRP} Target Recognition}, 
      author={Dandan Guo and Bo Chen and Wenchao Chen and Chaojie Wang and Hongwei Liu and Mingyuan Zhou},
      year={2020},
      journal={IEEE Transactions on Signal Processing},
  volume={68},
  pages={5795--5809},
      url={https://arxiv.org/abs/2009.13011},
    pdf={https://arxiv.org/pdf/2009.13011.pdf},
    url_arxiv={https://arxiv.org/abs/2009.13011},
    abstract={We develop a recurrent gamma belief network (rGBN) for radar automatic target recognition (RATR) based on high-resolution range profile (HRRP), which characterizes the temporal dependence across the range cells of HRRP. The proposed rGBN adopts a hierarchy of gamma distributions to build its temporal deep generative model. For scalable training and fast out-of-sample prediction, we propose the hybrid of a stochastic-gradient Markov chain Monte Carlo (MCMC) and a recurrent variational inference model to perform posterior inference. To utilize the label information to extract more discriminative latent representations, we further propose supervised rGBN to jointly model the HRRP samples and their corresponding labels. Experimental results on synthetic and measured HRRP data show that the proposed models are efficient in computation, have good classification accuracy and generalization ability, and provide highly interpretable multi-stochastic-layer latent structure.}
}





@article{Kalantari2020graph,
  title={Graph Gamma Process Generalized Linear Dynamical Systems},
  author={Kalantari, Rahi and Zhou, Mingyuan},
  journal={arXiv preprint arXiv:2007.12852},
  month={July},
  year={2020},
  url={https://arxiv.org/abs/2007.12852},
  pdf={https://arxiv.org/pdf/2007.12852.pdf},
  url_arxiv={https://arxiv.org/abs/2007.12852},
special_url={https://dds-covid19.github.io/},
special_text={COVID-19 Forecast Website},
special_url1={https://github.com/reichlab/covid19-forecast-hub/tree/master/data-processed/DDS-NBDS},
special_text1={COVID-19 Forecast Results},
special_url2={https://viz.covid19forecasthub.org/},
special_text2={COVID-19 Forecast Comparison},
  abstract={We introduce graph gamma process (GGP) linear dynamical systems to model  real-valued multivariate time series. For temporal pattern discovery, the latent representation under the model is used to decompose the time series into a parsimonious set of multivariate sub-sequences. In each sub-sequence, different data dimensions often share similar temporal patterns but may exhibit distinct magnitudes, and hence allowing the superposition of all sub-sequences to exhibit diverse behaviors at different data dimensions. We further generalize the proposed model by replacing the Gaussian observation layer with the negative binomial distribution to model multivariate count time series. Generated from the proposed GGP is an infinite dimensional directed sparse random graph, which is constructed by taking the logical OR operation of countably infinite binary adjacency matrices that share the same set of countably infinite nodes. Each of these adjacency matrices is associated with a weight to indicate its activation strength, and places a finite number of edges between a finite subset of nodes belonging to the same node community.  We use the generated random graph, whose number of nonzero-degree nodes is finite, to define both the sparsity pattern and dimension of the latent state transition matrix of a (generalized) linear dynamical system. The activation strength of each node community relative to the overall activation strength is used to extract a multivariate sub-sequence, revealing the data pattern captured by the corresponding community. On both synthetic and real-world time series, the proposed nonparametric Bayesian dynamic models, which are initialized at random, 
consistently exhibit  good  predictive performance in comparison to a variety of baseline models, revealing interpretable latent state transition patterns and decomposing the time series into distinctly behaved sub-sequences.}
}

	


@article{yin2022probabilistic,
  title={Probabilistic Best Subset Selection by Gradient-Based Optimization},
  author={Yin, Mingzhang and Ho, Nhat and Yan, Bowei and Qian, Xiaoning and Zhou, Mingyuan},
  journal={arXiv preprint arXiv:2006.06448},
  mon={June},
  year={2022},
  url={https://arxiv.org/abs/2006.06448},
  pdf={https://arxiv.org/pdf/2006.06448.pdf},
  url_arxiv={https://arxiv.org/abs/2006.06448},
  abstract={In high-dimensional statistics, variable selection recovers the latent sparse patterns from all possible covariate combinations. This paper proposes a novel optimization method to solve the exact L0-regularized regression problem, which is also known as the best subset selection. We reformulate the optimization problem from a discrete space to a continuous one via probabilistic reparameterization. The new objective function is differentiable but its gradient often cannot be computed in a closed form. Then we propose a family of unbiased gradient estimators to optimize the best subset selection objectives by the stochastic gradient descent. Within this family, we identify the estimator with uniformly minimum variance. Theoretically, we study the general conditions under which the method is guaranteed to converge to the ground truth in expectation. The proposed method can find the true regression model from thousands of covariates in seconds. In a wide variety of synthetic and semi-synthetic data, the proposed method outperforms existing variable selection tools based on the relaxed penalties, coordinate descent, and mixed integer optimization in both sparse pattern recovery and out-of-sample prediction.}
}



@article{zhang2020deep,
  title={Deep Autoencoding Topic Model with Scalable Hybrid {B}ayesian Inference},
  author={Zhang, Hao and Chen, Bo and Cong, Yulai and Guo, Dandan and Liu, Hongwei and Zhou, Mingyuan},
journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume={43},
  number={12},
  pages={4306--4322},
  year={2020},
  url={https://ieeexplore.ieee.org/document/9121755},
  pdf={Papers/DATM_PAMI2020.pdf},
  url_arxiv={http://arxiv.org/abs/2006.08804},
  abstract={To build a flexible and interpretable model for document analysis, we develop deep autoencoding topic model (DATM) that uses a hierarchy of gamma distributions to construct its multi-stochastic-layer generative network. In order to provide scalable posterior inference for the parameters of the generative network, we develop topic-layer-adaptive stochastic gradient Riemannian MCMC that jointly learns simplex-constrained global parameters across all layers and topics, with topic and layer specific learning rates. Given a posterior sample of the global parameters, in order to efficiently infer the local latent representations of a document under DATM across all stochastic layers, we propose a Weibull upward-downward variational encoder that deterministically propagates information upward via a deep neural network, followed by a Weibull distribution based stochastic downward generative model. To jointly model documents and their associated labels, we further propose supervised DATM that enhances the discriminative power of its latent representations. The efficacy and scalability of our models are demonstrated on both unsupervised and supervised learning tasks on big corpora.}
}





@article{li2020semi,
  title={Semi-Supervised Learning using Adversarial Training with Good and Bad Samples},
  author={Li, Wenyuan and Wang, Zichen and Yue, Yuguang and Li, Jiayun and Speier, William and Zhou, Mingyuan and Arnold, Corey},
  journal={Machine Vision and Applications},
  volume={31},
  number={6},
  pages={1--11},
  year={2020},
  url={https://link.springer.com/article/10.1007/s00138-020-01096-z},
  publisher={Springer Berlin Heidelberg},
  url_arxiv={https://arxiv.org/abs/1910.08540}
}



@inproceedings{Chen2020Switch,
  title     = {Switching {P}oisson Gamma Dynamical Systems},
  author    = {Chen, Wenchao and Chen, Bo and Liu, Yicheng and Zhao, Qianru and Zhou, Mingyuan},
  booktitle = {IJCAI 2020: International Joint Conference on
               Artificial Intelligence},
  publisher = {International Joint Conferences on Artificial Intelligence Organization}, 
           
  editor    = {Christian Bessiere},
  pages     = {2029--2036},
  year      = {2020},
    month     = {7},
  note      = {Main track},
  doi       = {10.24963/ijcai.2020/281},
  url       = {https://doi.org/10.24963/ijcai.2020/281},
  pdf      = {https://www.ijcai.org/Proceedings/2020/0281.pdf},
  abstract = {We propose Switching Poisson gamma dynamical systems (SPGDS) to model sequentially observed multivariate count data. Different from previous models, SPGDS assigns its latent variables into mixture of gamma distributed parameters to model complex sequences and describe the nonlinear dynamics, meanwhile, capture various temporal dependencies. For efficient inference, we develop a scalable hybrid stochastic gradient-MCMC and switching recurrent autoencoding variational inference, which is scalable to large scale sequences and fast in out-of-sample prediction. Experiments on both unsupervised and supervised tasks demonstrate that the proposed model not only has excellent fitting and prediction performance on complex dynamic sequences, but also separates different dynamical patterns within them.}
}







@inproceedings{wang2020thompson,
  title={Thompson Sampling via Local Uncertainty},
  author={Wang, Zhendong and Zhou, Mingyuan},
  booktitle={ICML 2020: International Conference on Machine Learning},
  year={2020},
  url={https://arxiv.org/abs/1910.13673},
  pdf={https://arxiv.org/pdf/1910.13673.pdf},
  url_arxiv={https://arxiv.org/abs/1910.13673},
  url_code={https://github.com/Zhendong-Wang/Thompson-Sampling-via-Local-Uncertainty},
  abstract={Thompson sampling is an efficient algorithm for sequential decision making, which exploits the posterior uncertainty to address the exploration-exploitation dilemma. There has been significant recent interest in integrating Bayesian neural networks into Thompson sampling. Most of these methods rely on global variable uncertainty for exploration. In this paper, we propose a new probabilistic modeling framework for Thompson sampling, where local latent variable uncertainty is used to sample the mean reward. Variational inference is used to approximate the posterior of the local variable, and semi-implicit structure is further introduced to enhance its expressiveness. Our experimental results on eight contextual bandit benchmark datasets show that Thompson sampling guided by local uncertainty achieves state-of-the-art performance while having low computational complexity.}
}



@inproceedings{guo2020recurrent,
  title={Recurrent Hierarchical Topic-Guided RNN for Language Generation},
  author={Guo, Dandan and Chen, Bo and Lu, Ruiying and Zhou, Mingyuan},
  booktitle={ICML 2020: International Conference on Machine Learning},
  year={2020},
  url={http://proceedings.mlr.press/v119/guo20a.html},
  pdf={https://arxiv.org/pdf/1912.10337.pdf},
  url_arxiv={https://arxiv.org/abs/1912.10337},
  url_code={https://github.com/Dan123dan/rGBN-RNN},
  abstract={To simultaneously capture syntax and global semantics from a text corpus, we propose a new larger-context recurrent neural network (RNN) based language model, which extracts recurrent hierarchical semantic structure via a dynamic deep topic model to guide natural language generation. Moving beyond a conventional RNN-based language model that ignores long-range word dependencies and sentence order, the proposed model captures not only intra-sentence word dependencies, but also temporal transitions between sentences and inter-sentence topic dependencies. For inference, we develop a hybrid of stochastic-gradient Markov chain Monte Carlo and recurrent autoencoding variational Bayes. Experimental results on a variety of real-world text corpora demonstrate that the proposed model not only outperforms larger-context RNN-based language models, but also learns interpretable recurrent multilayer topics and generates diverse sentences and paragraphs that are syntactically correct and semantically coherent.}
}





@inproceedings{hasanzadeh2020bayesian,
  title={Bayesian Graph Neural Networks with Adaptive Connection Sampling},
  author={Hasanzadeh, Arman and Hajiramezanali, Ehsan and Boluki, Shahin and Zhou, Mingyuan and Duffield, Nick and Narayanan, Krishna and Qian, Xiaoning},
  booktitle={ICML 2020: International Conference on Machine Learning},
  year={2020},
  url={https://arxiv.org/abs/2006.04064},
  url_arxiv={https://arxiv.org/abs/2006.04064},
  url_pdf={https://arxiv.org/pdf/2006.04064.pdf},
  url_code={https://github.com/armanihm/GDC},
  abstract={We propose a unified framework for adaptive connection sampling in graph neural networks (GNNs) that generalizes existing stochastic regularization methods for training GNNs. The proposed framework not only alleviates over-smoothing and over-fitting tendencies of deep GNNs, but also enables learning with uncertainty in graph analytic tasks with GNNs. Instead of using fixed sampling rates or hand-tuning them as model hyperparameters in existing stochastic regularization methods, our adaptive connection sampling can be trained jointly with GNN model parameters in both global and local fashions. GNN training with adaptive connection sampling is shown to be mathematically equivalent to an efficient approximation of training Bayesian GNNs. Experimental results with ablation studies on benchmark datasets validate that adaptively learning the sampling rate given graph training data is the key to boost the performance of GNNs in semi-supervised node classification, less prone to over-smoothing and over-fitting with more robust prediction.}
}



@inproceedings{dadaneh2020pairwise,
  title={Pairwise Supervised Hashing with Bernoulli Variational Auto-Encoder and Self-Control Gradient Estimator},
  author={Dadaneh, Siamak Zamani and Boluki, Shahin and Yin, Mingzhang and Zhou, Mingyuan and Qian, Xiaoning},
  booktitle={UAI 2020: Uncertainty in Artificial Intelligence},
  year={2020},
  pdf={http://www.auai.org/uai2020/proceedings/232_main_paper.pdf},
  url={http://www.auai.org/uai2020/proceedings/232_main_paper.pdf},
  url_arxiv={https://arxiv.org/abs/2005.10477},
  abstract={Semantic hashing has become a crucial component of fast similarity search in many large-scale information retrieval systems, in particular, for text data. Variational autoencoders (VAEs) with binary latent variables as hashing codes provide state-of-the-art performance in terms of precision for document retrieval. We propose a pairwise loss function with discrete latent VAE to reward within-class similarity and between-class dissimilarity for supervised hashing. Instead of solving the optimization relying on existing biased gradient estimators, an unbiased low-variance gradient estimator is adopted to optimize the hashing function by evaluating the non-differentiable loss function over two correlated sets of binary hashing codes to control the variance of gradient estimates. This new semantic hashing framework achieves superior performance compared to the state-of-the-arts, as demonstrated by our comprehensive experiments.}
}

 



@InProceedings{Yue2020Discrete,
  title = 	 {Discrete Action On-Policy Learning with Action-Value Critic},
  author = 	 {Yue, Yuguang and Tang, Yunhao and Yin, Mingzhang and Zhou, Mingyuan},
  booktitle = 	 {AISTATS 2020: International Conference on Artificial Intelligence and Statistics},
  pages = 	 {1977--1987},
  year = 	 {2020},
  editor = 	 {Chiappa, Silvia and Calandra, Roberto},
  volume = 	 {108},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Online},
  month = 	 {26--28 Aug},
  publisher = 	 {PMLR},
  pdf = 	 {Papers/CARSM_2020.pdf},
  url = 	 {http://proceedings.mlr.press/v108/yue20a.html},
  url_arxiv={https://arxiv.org/abs/2002.03534},
  url_code={https://github.com/yuguangyue/CARSM},
  abstract = 	 {Reinforcement learning (RL) in discrete action space is ubiquitous in real-world applications, but its complexity grows exponentially with the action-space dimension, making it challenging to apply existing on-policy gradient based deep RL algorithms efficiently. To effectively operate in multidimensional discrete action spaces, we construct a critic to estimate action-value functions, apply it on correlated actions, and combine these critic estimated action values to control the variance of gradient estimation. We follow rigorous statistical analysis to design how to generate and combine these correlated actions, and how to sparsify the gradients by shutting down the contributions from certain dimensions. These efforts result in a new discrete action on-policy RL algorithm that empirically outperforms related on-policy algorithms relying on variance control techniques. We demonstrate these properties on OpenAI Gym benchmark tasks, and illustrate how discretizing the action space could benefit the exploration phase and hence facilitate convergence to a better local optimal solution thanks to the flexibility of discrete policy. }
}


@InProceedings{Zhao2020Variational,
  title = 	 {Variational Autoencoders for Sparse and Overdispersed Discrete Data},
  author = 	 {Zhao, He and Rai, Piyush and Du, Lan and Buntine, Wray and Phung, Dinh and Zhou, Mingyuan},
  booktitle = 	 {AISTATS 2020: International Conference on Artificial Intelligence and Statistics},
  pages = 	 {1684--1694},
  year = 	 {2020},
  editor = 	 {Chiappa, Silvia and Calandra, Roberto},
  volume = 	 {108},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Online},
  month = 	 {26--28 Aug},
  publisher = 	 {PMLR},
  pdf = 	 {Papers/VAE_discrete_AISTATS2020.pdf},
  url = 	 {http://proceedings.mlr.press/v108/zhao20c.html},
  url_arxiv={https://arxiv.org/abs/1905.00616},
  url_code={href="https://github.com/ethanhezhao/NBVAE},
  abstract = 	 {Many applications, such as text modelling, high-throughput sequencing, and recommender systems, require analysing sparse, high-dimensional, and overdispersed discrete (count or binary) data. Recent deep probabilistic models based on variational autoencoders (VAE) have shown promising results on discrete data but may have inferior modelling performance due to the insufficient capability in modelling overdispersion and model misspecification. To address these issues, we develop a VAE-based framework using the negative binomial distribution as the data distribution. We also provide an analysis of its properties vis--vis other models. We conduct extensive experiments on three problems from discrete data analysis: text analysis/topic modelling, collaborative filtering, and multi-label learning. Our models outperform state-of-the-art approaches on these problems, while also capturing the phenomenon of overdispersion more effectively.}
}




@InProceedings{Boluki2020Learn,
  title = 	 {Learnable {B}ernoulli Dropout for {B}ayesian Deep Learning},
  author = 	 {Boluki, Shahin and Ardywibowo, Randy and Dadaneh, Siamak Zamani and Zhou, Mingyuan and Qian, Xiaoning},
  booktitle = 	 {AISTATS 2020: International Conference on Artificial Intelligence and Statistics},
  pages = 	 {3905--3916},
  year = 	 {2020},
  editor = 	 {Chiappa, Silvia and Calandra, Roberto},
  volume = 	 {108},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Online},
  month = 	 {26--28 Aug},
  publisher = 	 {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v108/boluki20a/boluki20a.pdf},
  url = 	 {http://proceedings.mlr.press/v108/boluki20a.html},
  url_arxiv = {https://arxiv.org/abs/2002.05155},
  abstract = 	 {In this work, we propose learnable Bernoulli dropout (LBD), a new model-agnostic dropout scheme that considers the dropout rates as parameters jointly optimized with other model parameters. By probabilistic modeling of Bernoulli dropout, our method enables more robust prediction and uncertainty quantification in deep models. Especially, when combined with variational auto-encoders (VAEs), LBD enables flexible semi-implicit posterior representations, leading to new semi-implicit VAE (SIVAE) models. We solve the optimization for training with respect to the dropout parameters using Augment-REINFORCE-Merge (ARM), an unbiased and low-variance gradient estimator. Our experiments on a range of tasks show the superior performance of our approach compared with other commonly used dropout schemes. Overall, LBD leads to improved accuracy and uncertainty estimates in image classification and semantic segmentation. Moreover, using SIVAE, we can achieve state-of-the-art performance on collaborative filtering for implicit feedback on several public datasets.}
}



@InProceedings{Wang2020Learn,
  title = 	 {Learning Dynamic Hierarchical Topic Graph with Graph Convolutional Network for Document Classification},
  author = 	 {Wang, Zhengjue and Wang, Chaojie and Zhang, Hao and Duan, Zhibin and Zhou, Mingyuan and Chen, Bo},
  booktitle = 	 {AISTATS 2020: International Conference on Artificial Intelligence and Statistics},
  pages = 	 {3959--3969},
  year = 	 {2020},
  editor = 	 {Chiappa, Silvia and Calandra, Roberto},
  volume = 	 {108},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Online},
  month = 	 {26--28 Aug},
  publisher = 	 {PMLR},
  pdf = 	 {Papers/HTG_AISTATS2020.pdf},
  url = 	 {http://proceedings.mlr.press/v108/wang20l.html},
Note = {(the first two authors contributed equally)},
  abstract = 	 {Constructing a graph with graph convolutional network (GCN)  to explore the relational structure of the data has attracted lots of interests in various tasks. However, for document classification, existing graph based methods often focus on the straightforward word-word and word-document relations, ignoring the hierarchical semantics. Besides, the graph construction is often independent from the task-specific GCN learning. To address these constrains, we integrate a probabilistic deep topic model into graph construction, and propose a novel trainable hierarchical topic graph (HTG), including word-level, hierarchical topic-level and document-level nodes, exhibiting semantic variation from fine-grained to coarse. Regarding the document classification as a document-node label generation task, HTG can be dynamically evolved with GCN by performing variational inference, which leads to an end-to-end document classification method, named dynamic HTG (DHTG). Besides achieving state-of-the-art classification results, our model learns an interpretable document graph with meaningful node embeddings and semantic edges.}
}

	
@inproceedings{
Fan2020Adaptive,
title={Adaptive Correlated {M}onte {C}arlo for Contextual Categorical Sequence Generation},
author={Xinjie Fan and Yizhe Zhang and Zhendong Wang and Mingyuan Zhou},
booktitle={ICLR 2020: International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=r1lOgyrKDS},
pdf={Papers/ACMC_ICLR2020.pdf},
url_arxiv={https://arxiv.org/abs/1912.13151},
url_code={https://github.com/xinjiefan/ACMC_ICLR},
abstract={Sequence generation models are commonly refined with reinforcement learning over user-defined metrics. However, high gradient variance hinders the practical use of this method. To stabilize this method, we adapt to contextual generation of categorical sequences a policy gradient estimator, which evaluates a set of correlated Monte Carlo (MC) rollouts for variance control. Due to the correlation, the number of unique rollouts is random and adaptive to model uncertainty; those rollouts naturally become baselines for each other, and hence are combined to effectively reduce gradient variance. We also demonstrate the use of correlated MC rollouts for binary-tree softmax models, which reduce the high generation cost in large vocabulary scenarios by decomposing each categorical action into a sequence of binary actions. We evaluate our methods on both neural program synthesis and image captioning. The proposed methods yield lower gradient variance and consistent improvement over related baselines.}
}

@inproceedings{
Zhang2020Variational,
title={Variational Hetero-Encoder Randomized {GAN}s for Joint Image-Text Modeling},
author={Hao Zhang and Bo Chen and Long Tian and Zhengjue Wang and Mingyuan Zhou},
booktitle={ICLR 2020: International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=H1x5wRVtvS},
pdf={Papers/VHE_GAN_v20_ICLR.pdf},
url_arxiv={https://arxiv.org/abs/1905.08622},
url_code={https://github.com/BoChenGroup/VHE-GAN},
abstract={For bidirectional joint image-text modeling, we develop variational hetero-encoder (VHE) randomized generative adversarial network (GAN), a versatile deep generative model that integrates a probabilistic text decoder, probabilistic image encoder, and GAN into a coherent end-to-end multi-modality learning framework. VHE randomized GAN (VHE-GAN) encodes an image to decode its associated text, and feeds the variational posterior as the source of randomness into the GAN image generator. We plug three off-the-shelf modules, including a deep topic model, a ladder-structured image encoder, and StackGAN++, into VHE-GAN, which already achieves competitive performance. This further motivates the development of VHE-raster-scan-GAN that generates photo-realistic images in not only a multi-scale low-to-high-resolution manner, but also a hierarchical-semantic coarse-to-fine fashion. By capturing and relating hierarchical semantic and visual concepts with end-to-end training, VHE-raster-scan-GAN achieves state-of-the-art performance in a wide variety of image-text multi-modality learning and generation tasks.}
}

@inproceedings{
Yin2020Meta-Learning,
title={Meta-Learning without Memorization},
author={Mingzhang Yin and George Tucker and Mingyuan Zhou and Sergey Levine and Chelsea Finn},
booktitle={ICLR 2020: International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=BklEFpEYwS},
pdf={https://openreview.net/pdf?id=BklEFpEYwS},
url_arxiv={https://arxiv.org/abs/1912.03820},
url_slide={https://mingzhang-yin.github.io/assets/pdfs/iclr2020_slides.pdf},
url_poster={https://openreview.net/attachment?id=BklEFpEYwS&name=poster},
url_code={https://github.com/google-research/google-research/tree/master/meta_learning_without_memorization},
abstract={The ability to learn new concepts with small amounts of data is a critical aspect of intelligence that has proven challenging for deep learning methods. Meta-learning has emerged as a promising technique for leveraging data from previous tasks to enable efficient learning of new tasks. However, most meta-learning algorithms implicitly require that the meta-training tasks be mutually-exclusive, such that no single model can solve all of the tasks at once. For example, when creating tasks for few-shot image classification, prior work uses a per-task random assignment of image classes to N-way classification labels. If this is not done, the meta-learner can ignore the task training data and learn a single model that performs all of the meta-training tasks zero-shot, but does not adapt effectively to new image classes.  This requirement means that the user must take great care in designing the tasks, for example by shuffling labels or removing task identifying information from the inputs. In some domains, this makes meta-learning entirely inapplicable. In this paper, we address this challenge by designing a meta-regularization objective using information theory that places precedence on data-driven adaptation. This causes the meta-learner to decide what must be learned from the task training data and what should be inferred from the task testing input. By doing so, our algorithm can successfully use data from non-mutually-exclusive tasks to efficiently adapt to novel tasks. We demonstrate its applicability to both contextual and gradient-based meta-learning algorithms, and apply it in practical settings where applying standard meta-learning has been difficult. Our approach substantially outperforms standard meta-learning algorithms in these settings.}
}

	

	
@inproceedings{
Wen2020Mutual,
title={Mutual Information Gradient Estimation for Representation Learning},
author={Liangjian Wen and Yiji Zhou and Lirong He and Mingyuan Zhou and Zenglin Xu},
booktitle={ICLR 2020: International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=ByxaUgrFvH},
pdf={https://openreview.net/pdf?id=ByxaUgrFvH},
abstract={Mutual Information (MI) plays an important role in representation learning. However, MI is unfortunately intractable in continuous and high-dimensional settings. Recent advances establish tractable and scalable MI estimators to discover useful representation. However, most of the existing methods are not capable of providing an accurate estimation of MI with low-variance when the MI is large. We argue that directly estimating the gradients of MI is more appealing for representation learning than estimating MI in itself. To this end, we propose the Mutual Information Gradient Estimator (MIGE) for representation learning based on the score estimation of implicit distributions. MIGE exhibits a tight and smooth gradient estimation of MI in the high-dimensional and large-MI settings. We expand the applications of MIGE in both unsupervised learning of deep representations based on InfoMax and the Information Bottleneck method. Experimental results have indicated significant performance improvement in learning useful representation.}
}


@article{zhang2019weibull,
  title={Weibull Racing Time-to-event Modeling and Analysis of Online Borrowers' Loan Payoff and Default},
  author={Zhang, Quan and Gao, Qiang and Lin, Mingfeng and Zhou, Mingyuan},
  journal={arXiv preprint arXiv:1911.01827},
  url={https://arxiv.org/abs/1911.01827},
  pdf={https://arxiv.org/pdf/1911.01827.pdf},
  year={2019},
  abstract={We propose Weibull delegate racing (WDR) to explicitly model surviving under competing events and to interpret how the covariates accelerate or decelerate the event time. It explains non-monotonic covariate effects by racing a potentially infinite number of sub-events, and consequently relaxes the ubiquitous proportional-hazards assumption which may be too restrictive. For inference, we develop a Gibbs-sampler-based MCMC algorithm along with maximum a posteriori estimations for big data applications. We analyze time to loan payoff and default on this http URL, demonstrating not only a distinguished performance of WDR, but also the value of standard and soft information.}
}


@inproceedings{schein2019poisso,
title = {Poisson-Randomized Gamma Dynamical Systems},
author = {Schein, Aaron and Linderman, Scott and Zhou, Mingyuan and Blei, David and Wallach, Hanna},
booktitle = {NeurIPS 2019: Advances in Neural Information Processing Systems},
editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
pages = {782--793},
year = {2019},
pdf={Papers/PRGDS_NeurIPS2019.pdf},
url_arxiv={https://arxiv.org/abs/1910.12991},
url_code={https://github.com/aschein/PRGDS},
url = {http://papers.nips.cc/paper/8366-poisson-randomized-gamma-dynamical-systems},
abstract={This paper presents the Poisson-randomized gamma dynamical system (PRGDS), a model for sequentially observed count tensors that encodes a strong inductive bias toward sparsity and burstiness. The PRGDS is based on a new motif in Bayesian latent variable modeling, an alternating chain of discrete Poisson and continuous gamma latent states that is analytically convenient and computationally tractable. This motif yields closed-form complete conditionals for all variables by way of the Bessel distribution and a novel discrete distribution that we call the shifted confluent hypergeometric distribution. We draw connections to closely related models and compare the PRGDS to these models in studies of real-world count data sets of text, international events, and neural spike trains. We find that a sparse variant of the PRGDS, which allows the continuous gamma latent states to take values of exactly zero, often obtains better predictive performance than other models and is uniquely capable of inferring latent structures that are highly localized in time.}
}





@inproceedings{hasanzadeh2019semi,
title = {Semi-Implicit Graph Variational Auto-Encoders},
author = {Hasanzadeh, Arman and Hajiramezanali, Ehsan and Narayanan, Krishna and Duffield, Nick and Zhou, Mingyuan and Qian, Xiaoning},
booktitle = {NeurIPS 2019: Advances in Neural Information Processing Systems 32},
editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
pages = {10712--10723},
year = {2019},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/9255-semi-implicit-graph-variational-auto-encoders},
pdf={Papers/SIG-VAE_NeurIPS2019.pdf},
url_arxiv={https://arxiv.org/abs/1908.07078},
url_code={https://github.com/sigvae/SIGraphVAE},
abstract={Semi-implicit graph variational auto-encoder (SIG-VAE) is proposed to expand the flexibility of variational graph auto-encoders (VGAE) to model graph data. SIG-VAE employs a hierarchical variational framework to enable neighboring node sharing for better generative modeling of graph dependency structure, together with a Bernoulli-Poisson link decoder. Not only does this hierarchical construction provide a more flexible generative graph model to better capture real-world graph properties, but also does SIG-VAE naturally lead to semi-implicit hierarchical variational inference that allows faithful modeling of implicit posteriors of given graph data, which may exhibit heavy tails, multiple modes, skewness, and rich dependency structures. SIG-VAE integrates a carefully designed generative model, well suited to model real-world sparse graphs, and a sophisticated variational inference network, which propagates the graph structural information and distribution uncertainty to capture complex posteriors. SIG-VAE clearly outperforms a simple combination of VGAE with variational inference, including semi-implicit variational inference~(SIVI) or normalizing flow (NF), which does not propagate uncertainty in its inference network, and provides more interpretable latent representations than VGAE does. Extensive experiments with a variety of graph data show that SIG-VAE significantly outperforms state-of-the-art methods on several different graph analytic tasks.}
}



@inproceedings{Hajiramezanali2019variational,
title = {Variational Graph Recurrent Neural Networks},
author = {Hajiramezanali, Ehsan and Hasanzadeh, Arman and Narayanan, Krishna and Duffield, Nick and Zhou, Mingyuan and Qian, Xiaoning},
booktitle = {NeurIPS 2019: Advances in Neural Information Processing Systems 32},
editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
pages = {10701--10711},
year = {2019},
url = {http://papers.nips.cc/paper/9254-variational-graph-recurrent-neural-networks},
pdf={Papers/VGRNN-NeurIPS2019.pdf},
url_arxiv={https://arxiv.org/abs/1908.09710},
url_code={https://github.com/VGraphRNN/VGRNN},
abstract={Representation learning over graph structured data has been mostly studied in static graph settings while efforts for modeling dynamic graphs are still scant. In this paper, we develop a novel hierarchical variational model that introduces additional latent random variables to jointly model the hidden states of a graph recurrent neural network (GRNN) to capture both topology and node attribute changes in dynamic graphs. We argue that the use of high-level latent random variables in this variational GRNN (VGRNN) can better capture potential variability observed in dynamic graphs as well as the uncertainty of node latent representation. With semi-implicit variational inference developed for this new VGRNN architecture (SI-VGRNN), we show that flexible non-Gaussian latent representations can further help dynamic graph analytic tasks. Our experiments with multiple real-world dynamic graph datasets demonstrate that SI-VGRNN and VGRNN consistently outperform the existing baseline and state-of-the-art methods by a significant margin in dynamic link prediction.}
}



@InProceedings{Yin2019ARSM,
  title = 	 {{ARSM}: Augment-{REINFORCE}-Swap-Merge Estimator for Gradient Backpropagation Through Categorical Variables},
  author = 	 {Yin, Mingzhang and Yue, Yuguang and Zhou, Mingyuan},
  booktitle = 	 {ICML 2019: International Conference on Machine Learning},
  pages = 	 {7095--7104},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Long Beach, California, USA},
  month = 	 {09--15 Jun},
  publisher = 	 {PMLR},
  pdf = 	 {Papers/ARSM_ICML2019.pdf},
  url = 	 {http://proceedings.mlr.press/v97/yin19c.html},
  url_arxiv={https://arxiv.org/abs/1905.01413},
  url_errata={Papers/errata.pdf},
  url_code={https://github.com/ARM-gradient/ARSM},
  Note = {(the first two authors contributed equally)},
  abstract = 	 {To address the challenge of backpropagating the gradient through categorical variables, we propose the augment-REINFORCE-swap-merge (ARSM) gradient estimator that is unbiased and has low variance. ARSM first uses variable augmentation, REINFORCE, and Rao-Blackwellization to re-express the gradient as an expectation under the Dirichlet distribution, then uses variable swapping to construct differently expressed but equivalent expectations, and finally shares common random numbers between these expectations to achieve significant variance reduction. Experimental results show ARSM closely resembles the performance of the true gradient for optimization in univariate settings; outperforms existing estimators by a large margin when applied to categorical variational auto-encoders; and provides a "try-and-see self-critic" variance reduction method for discrete-action policy gradient, which removes the need of estimating baselines by generating a random number of pseudo actions and estimating their action-value functions.}
}


@inproceedings{wang2019convolutional,
  title = 	 {Convolutional {P}oisson Gamma Belief Network},
  author = 	 {Wang, Chaojie and Chen, Bo and Xiao, Sucheng and Zhou, Mingyuan},
  booktitle = 	 {ICML 2019: International Conference on Machine Learning},
  pages = 	 {6515--6525},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Long Beach, California, USA},
  month = 	 {09--15 Jun},
  publisher = 	 {PMLR},
  pdf = 	 {Papers/CPGBN_v12_arXiv.pdf},
  url = 	 {http://proceedings.mlr.press/v97/wang19b.html},
  url_arxiv={https://arxiv.org/abs/1905.05394},
  url_code={https://github.com/BoChenGroup/CPGBN},
  abstract = 	 {For text analysis, one often resorts to a lossy representation that either completely ignores word order or embeds each word as a low-dimensional dense feature vector. In this paper, we propose convolutional Poisson factor analysis (CPFA) that directly operates on a lossless representation that processes the words in each document as a sequence of high-dimensional one-hot vectors. To boost its performance, we further propose the convolutional Poisson gamma belief network (CPGBN) that couples CPFA with the gamma belief network via a novel probabilistic pooling layer. CPFA forms words into phrases and captures very specific phrase-level topics, and CPGBN further builds a hierarchy of increasingly more general phrase-level topics. For efficient inference, we develop both a Gibbs sampler and a Weibull distribution based convolutional variational auto-encoder. Experimental results demonstrate that CPGBN can extract high-quality text latent representations that capture the word order information, and hence can be leveraged as a building block to enrich a wide variety of existing latent variable models that ignore word order.}
}


@InProceedings{schein2019locally,
  title = 	 {Locally Private {B}ayesian Inference for Count Models},
  author = 	 {Schein, Aaron and Wu, Zhiwei Steven and Schofield, Alexandra and Zhou, Mingyuan and Wallach, Hanna},
  booktitle = 	 {ICML 2019: International Conference on Machine Learning},
  pages = 	 {5638--5648},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Long Beach, California, USA},
  month = 	 {09--15 Jun},
  publisher = 	 {PMLR},
  pdf = 	 {Papers/Schein_locally_private_count_ICML2019.pdf},
  url = 	 {http://proceedings.mlr.press/v97/schein19a.html},
  url_arxiv = {https://arxiv.org/pdf/1803.08471.pdf},
  url_code={https://github.com/xandaschofield/locally_private_bpf_icml19},
  url_video={https://slideslive.com/38917932/privacy},
  url_poster={http://www.columbia.edu/~as5530/ScheinWuSchofieldZhouWallach2019_poster.pdf},
  abstract = 	 {We present a general and modular method for privacy-preserving Bayesian inference for Poisson factorization, a broad class of models that includes some of the most widely used models in the social sciences. Our method satisfies limited-precision local privacy, a generalization of local differential privacy that we introduce to formulate appropriate privacy guarantees for sparse count data. We present an MCMC algorithm that approximates the posterior distribution over the latent variables conditioned on data that has been locally privatized by the geometric mechanism. Our method is based on two insights: 1) a novel reinterpretation of the geometric mechanism in terms of the Skellam distribution and 2) a general theorem that relates the Skellam and Bessel distributions. We demonstrate our methods utility using two case studies that involve real-world email data. We show that our method consistently outperforms the commonly used naive approach, wherein inference proceeds as usual, treating the locally privatized data as if it were not privatized.}
}

@inproceedings{yin2018arm,
title={{ARM}: Augment-{REINFORCE}-Merge Gradient for Stochastic Binary Networks},
author={Mingzhang Yin and Mingyuan Zhou},
booktitle={ICLR 2019: International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=S1lg0jAcYm},
pdf={https://openreview.net/pdf?id=S1lg0jAcYm},
url_code={https://github.com/mingzhang-yin/ARM-gradient},
abstract={To backpropagate the gradients through stochastic binary layers, we propose the augment-REINFORCE-merge (ARM) estimator that is unbiased, exhibits low variance, and has low computational complexity. Exploiting variable augmentation, REINFORCE, and reparameterization, the ARM estimator achieves adaptive variance reduction for Monte Carlo integration by merging two expectations via common random numbers. The variance-reduction mechanism of the ARM estimator can also be attributed to either antithetic sampling in an augmented space, or the use of an optimal anti-symmetric "self-control" baseline function together with the REINFORCE estimator in that augmented space. Experimental results show the ARM estimator provides state-of-the-art performance in auto-encoding variational inference and maximum likelihood estimation, for discrete latent variable models with one or multiple stochastic binary layers. Python code for reproducible research is publicly available.}
}

@inproceedings{panda2019deep,
  title = 	 {Deep Topic Models for Multi-label Learning},
  author = 	 {Panda, Rajat and Pensia, Ankit and Mehta, Nikhil and Zhou, Mingyuan and Rai, Piyush},
  booktitle = 	 {AISTATS 2019: International Conference on Artificial Intelligence and Statistics},
  pages = 	 {2849--2857},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Sugiyama, Masashi},
  volume = 	 {89},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {},
  month = 	 {16--18 Apr},
  publisher = 	 {PMLR},
  url={http://proceedings.mlr.press/v89/panda19a.html},
  pdf={http://proceedings.mlr.press/v89/panda19a/panda19a.pdf},
  abstract = 	 {We present a probabilistic framework for multi-label learning based on a deep generative model for the binary label vector associated with each observation. Our generative model learns deep multi-layer latent embeddings of the binary label vector, which are conditioned on the input features of the observation. The model also has an interesting interpretation in terms of a deep topic model, with each label vector representing a bag-of-words document, with the input features being its meta-data. In addition to capturing the structural properties of the label space (e.g., a near-low-rank label matrix), the model also offers a clean, geometric interpretation. In particular, the nonlinear classification boundaries learned by the model can be seen as the union of multiple convex polytopes. Our model admits a simple and scalable inference via efficient Gibbs sampling or EM algorithm. We compare our model with state-of-the-art baselines for multi-label learning on  benchmark data sets, and also report some interesting qualitative results.}
}




@inproceedings{zhou2018parsimonious,
title = {Parsimonious {B}ayesian Deep Networks},
author = {Zhou, Mingyuan},
booktitle = {NeurIPS 2018: Advances in Neural Information Processing Systems},
editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
pages = {3190--3200},
year = {2018},
url={http://papers.nips.cc/paper/7581-parsimonious-bayesian-deep-networks.html},
pdf={Papers/Zhou_PBDN_NIPS2018.pdf},
url_arxiv={http://arxiv.org/abs/1805.08719},
url_poster={Papers/PBDN_NIPS2018_poster.pdf},
url_code={https://github.com/mingyuanzhou/PBDN},
url_demo={images/Two_spirials_PBDN_1to10layers.gif},
abstract={Combining Bayesian nonparametrics and a forward model selection strategy, we construct parsimonious Bayesian deep networks (PBDNs) that infer capacity-regularized network architectures from the data and require neither cross-validation nor fine-tuning when training the model. One of the two essential components of a PBDN is the development of a special infinite-wide single-hidden-layer neural network, whose number of active hidden units can be inferred from the data. The other one is the construction of a greedy layer-wise learning algorithm that uses a forward model selection criterion to determine when to stop adding another hidden layer. We develop both Gibbs sampling and stochastic gradient descent based maximum a posteriori inference for PBDNs, providing state-of-the-art classification accuracy and interpretable data subtypes near the decision boundaries, while maintaining low computational complexity for out-of-sample prediction.}
}




@inproceedings{zhang2018nonparametric,
title = {Nonparametric {B}ayesian {L}omax delegate racing for survival analysis with competing risks},
author = {Zhang, Quan and Zhou, Mingyuan},
booktitle = {NeurIPS 2018: Advances in Neural Information Processing Systems},
editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
pages = {5002--5013},
year = {2018},
url = {http://papers.nips.cc/paper/7748-nonparametric-bayesian-lomax-delegate-racing-for-survival-analysis-with-competing-risks},
pdf={Papers/ZhangZhou_LomaxDelegateRacing_NIPS2018.pdf},
url_arxiv={http://arxiv.org/abs/1810.08564},
url_code={https://github.com/zhangquan-ut/Lomax-delegate-racing-for-survival-analysis-with-competing-risks},
abstract={We propose Lomax delegate racing (LDR) to explicitly model the mechanism of survival under competing risks and to interpret how the covariates accelerate or decelerate the time to event. LDR explains non-monotonic covariate effects by racing a potentially infinite number of sub-risks, and consequently relaxes the ubiquitous proportional-hazards assumption which may be too restrictive. Moreover, LDR is naturally able to model not only censoring, but also missing event times or event types. For inference, we develop a Gibbs sampler under data augmentation for moderately sized data, along with a stochastic gradient descent maximum a posteriori inference algorithm for big data applications. Illustrative experiments are provided on both synthetic and real datasets, and comparison with various benchmark algorithms for survival analysis with competing risks demonstrates distinguished performance of LDR.}
}



@inproceedings{zhao2018dirichlet,
title = {Dirichlet Belief Networks for Topic Structure Learning},
author = {Zhao, He and Du, Lan and Buntine, Wray and Zhou, Mingyuan},
booktitle = {NeurIPS 2018: Advances in Neural Information Processing Systems},
editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
pages = {7955--7966},
year = {2018}, 
url = {https://papers.nips.cc/paper/8020-dirichlet-belief-networks-for-topic-structure-learning.html},
pdf={Papers/Zhao_DirBN_NIPS2018.pdf},
url_arxiv={https://arxiv.org/abs/1811.00717},
url_code={https://github.com/ethanhezhao/DirBN},
abstract={Recently, considerable research effort has been devoted to developing deep architectures for topic models to learn topic structures. Although several deep models have been proposed to learn better topic proportions of documents, how to leverage the benefits of deep structures for learning word distributions of topics has not yet been rigorously studied. Here we propose a new multi-layer generative process on word distributions of topics, where each layer consists of a set of topics and each topic is drawn from a mixture of the topics of the layer above. As the topics in all layers can be directly interpreted by words, the proposed model is able to discover interpretable topic hierarchies. As a self-contained module, our model can be flexibly adapted to different kinds of topic models to improve their modelling accuracy and interpretability. Extensive experiments on text corpora demonstrate the advantages of the proposed model.}
}


@inproceedings{guo2018deep,
title = {Deep Poisson Gamma Dynamical Systems},
author = {Guo, Dandan and Chen, Bo and Zhang, Hao and Zhou, Mingyuan},
booktitle = {NeurIPS 2018: Advances in Neural Information Processing Systems},
editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
pages = {8442--8452},
year = {2018}, 
url = {https://papers.nips.cc/paper/8064-deep-poisson-gamma-dynamical-systems.html},
pdf={Papers/Guo_DPGDS_NIPS2018.pdf},
url_arxiv={https://arxiv.org/abs/1810.11209},
abstract={We develop deep Poisson-gamma dynamical systems (DPGDS) to model sequentially observed multivariate count data, improving previously proposed models by not only mining deep hierarchical latent structure from the data, but also capturing both first-order and long-range temporal dependencies. Using sophisticated but simple-to-implement data augmentation techniques, we derived closed-form Gibbs sampling update equations by first backward and upward propagating auxiliary latent counts, and then forward and downward sampling latent variables. Moreover, we develop stochastic gradient MCMC inference that is scalable to very long multivariate count time series. Experiments on both synthetic and a variety of real-world data demonstrate that the proposed model not only has excellent predictive performance, but also provides highly interpretable multilayer latent structure to represent hierarchical and temporal information propagation.}
}

@inproceedings{hajiramezanali2018bayesian,
title = {Bayesian Multi-domain Learning for Cancer Subtype Discovery from Next-generation Sequencing Count Data},
author = {Hajiramezanali, Ehsan and Zamani Dadaneh, Siamak and Karbalayghareh, Alireza and Zhou, Mingyuan and Qian, Xiaoning},
booktitle = {NeurIPS 2018: Advances in Neural Information Processing Systems},
editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
pages = {9115--9124},
year = {2018}, 
url = {https://papers.nips.cc/paper/8125-bayesian-multi-domain-learning-for-cancer-subtype-discovery-from-next-generation-sequencing-count-data.html},
pdf={Papers/Ehsan_BMDL_NIPS2018.pdf},
abstract={Precision medicine aims for personalized prognosis and therapeutics by utilizing recent genome-scale high-throughput profiling techniques, including next-generation sequencing (NGS). However, translating NGS data faces several challenges. First, NGS count data are often overdispersed, requiring appropriate modeling. Second, compared to the number of involved molecules and system complexity, the number of available samples for studying complex disease, such as cancer, is often limited, especially considering disease heterogeneity. The key question is whether we may integrate available data from all different sources or domains to achieve reproducible disease prognosis based on NGS count data. In this paper, we develop a Bayesian Multi-Domain Learning (BMDL) model that derives domain-dependent latent representations of overdispersed count data based on hierarchical negative binomial factorization for accurate cancer subtyping even if the number of samples for a specific cancer type is small. Experimental results from both our simulated and NGS datasets from The Cancer Genome Atlas (TCGA) demonstrate the promising potential of BMDL for effective multi-domain learning without negative transfer'' effects often seen in existing multi-task learning and transfer learning methods.}
}




@inproceedings{han2018masking,
title = {Masking: A New Perspective of Noisy Supervision},
author = {Han, Bo and Yao, Jiangchao and Niu, Gang and Zhou, Mingyuan and Tsang, Ivor and Zhang, Ya and Sugiyama, Masashi},
booktitle = {NeurIPS 2018: Advances in Neural Information Processing Systems},
editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
pages = {5836--5846},
year = {2018}, 
pdf = {Papers/Han_Masking_NIPS2018.pdf},
url={https://papers.nips.cc/paper/7825-masking-a-new-perspective-of-noisy-supervision.html},
url_code={https://github.com/bhanML/Masking},
abstract={It is important to learn various types of classifiers given training data with noisy labels. Noisy labels, in the most popular noise model hitherto, are corrupted from ground-truth labels by an unknown noise transition matrix. Thus, by estimating this matrix, classifiers can escape from overfitting those noisy labels. However, such estimation is practically difficult, due to either the indirect nature of two-step approaches, or not big enough data to afford end-to-end approaches. In this paper, we propose a human-assisted approach called ''Masking'' that conveys human cognition of invalid class transitions and naturally speculates the structure of the noise transition matrix. To this end, we derive a structure-aware probabilistic model incorporating a structure prior, and solve the challenges from structure extraction and structure alignment. Thanks to Masking, we only estimate unmasked noise transition probabilities and the burden of estimation is tremendously reduced. We conduct extensive experiments on CIFAR-10 and CIFAR-100 with three noise structures as well as the industrial-level Clothing1M with agnostic noise structure, and the results show that Masking can improve the robustness of classifiers significantly.}
}




@InProceedings{yin2018semi, 

title = 	 {Semi-Implicit Variational Inference}, 

author = 	 {Yin, Mingzhang and Zhou, Mingyuan}, 

booktitle = 	 {ICML 2018: International Conference on Machine Learning}, 

pages = 	 {5660--5669}, 

year = 	 {2018}, 

editor = 	 {Dy, Jennifer and Krause, Andreas}, 

volume = 	 {80}, 

series = 	 {Proceedings of Machine Learning Research}, 

address = 	 {Stockholmsmssan, Stockholm Sweden}, 

month = 	 {10--15 Jul}, 
publisher = 	 {PMLR}, 
pdf = 	 {Papers/SIVI.pdf}, 
url = 	 {http://proceedings.mlr.press/v80/yin18b.html}, 
url_arxiv={http://arxiv.org/abs/1805.11183}, 
url_slide={Papers/SIVI_ICML2018_slides.pdf}, 
url_poster={Papers/SIVI_ICML2018_poster.pdf}, 
url_slide_additional={Papers/VB_Bordeaux_July3_2018.pdf}, 
url_code={https://GitHub.com/mingzhang-yin/SIVI}, 
abstract = 	 {Semi-implicit variational inference (SIVI) is introduced to expand the commonly used analytic variational distribution family, by mixing the variational parameter with a flexible distribution. This mixing distribution can assume any density function, explicit or not, as long as independent random samples can be generated via reparameterization. Not only does SIVI expand the variational family to incorporate highly flexible variational distributions, including implicit ones that have no analytic density functions, but also sandwiches the evidence lower bound (ELBO) between a lower bound and an upper bound, and further derives an asymptotically exact surrogate ELBO that is amenable to optimization via stochastic gradient ascent. With a substantially expanded variational family and a novel optimization algorithm, SIVI is shown to closely match the accuracy of MCMC in inferring the posterior in a variety of Bayesian inference tasks.}
}



@InProceedings{zhao2018inter, 
title = 	 {Inter and Intra Topic Structure Learning with Word Embeddings}, 
author = 	 {Zhao, He and Du, Lan and Buntine, Wray and Zhou, Mingyuan}, 
booktitle = 	 {ICML 2018: International Conference on Machine Learning}, 
pages = 	 {5892--5901}, 
year = 	 {2018}, 
editor = 	 {Dy, Jennifer and Krause, Andreas}, 
volume = 	 {80}, 
series = 	 {Proceedings of Machine Learning Research}, 
address = 	 {Stockholmsmssan, Stockholm Sweden}, 
month = 	 {10--15 Jul}, 
publisher = 	 {PMLR}, 
pdf = 	 {Papers/WEDTM_ICML2018.pdf}, 
url = 	 {http://proceedings.mlr.press/v80/zhao18a.html}, 
url_code=	{https://GitHub.com/ethanhezhao/WEDTM}, 
abstract = 	 {One important task of topic modeling for text analysis is interpretability. By discovering structured topics one is able to yield improved interpretability as well as modeling accuracy. In this paper, we propose a novel topic model with a deep structure that explores both inter-topic and intra-topic structures informed by word embeddings. Specifically, our model discovers inter topic structures in the form of topic hierarchies and discovers intra topic structures in the form of sub-topics, each of which is informed by word embeddings and captures a fine-grained thematic aspect of a normal topic. Extensive experiments demonstrate that our model achieves the state-of-the-art performance in terms of perplexity, document classification, and topic quality. Moreover, with topic hierarchies and sub-topics, the topics discovered in our model are more interpretable, providing an illuminating means to understand text data.}
}

	
@inproceedings{acharya2018dual, 
title={A Dual {M}arkov Chain Topic Model for Dynamic Environments}, 
author={Acharya, Ayan and Ghosh, Joydeep and Zhou, Mingyuan}, 
booktitle={KDD 2018: Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining}, 
pages={1099--1108}, 
year={2018}, 
pdf={Papers/Ayan_KDD2018.pdf}, 
url={https://www.kdd.org/kdd2018/accepted-papers/view/a-dual-markov-chain-topic-model-for-dynamic-environments}, 
url_video={https://www.youtube.com/watch?v=GBio_I-rAGc}, 
organization={ACM}, 
abstract={The abundance of digital text has led to extensive research on topic models that reason about documents using latent representations. Since for many online or streaming textual sources such as news outlets, the number, and nature of topics change over time, there have been several efforts that attempt to address such situations using dynamic versions of topic models. Unfortunately, existing approaches encounter more complex inferencing when their model parameters are varied over time, resulting in high computation complexity and performance degradation. This paper introduces the DM-DTM, a dual Markov chain dynamic topic model, for characterizing a corpus that evolves over time. This model uses a gamma Markov chain and a Dirichlet Markov chain to allow the topic popularities and word-topic assignments, respectively, to vary smoothly over time. Novel applications of the Negative-Binomial augmentation trick result in simple, efficient, closed-form updates of all the required conditional posteriors, resulting in far lower computational requirements as well as less sensitivity to initial conditions, as compared to existing approaches. Moreover, via a gamma process prior, the number of desired topics is inferred directly from the data rather than being pre-specified and can vary as the data changes. Empirical comparisons using multiple real-world corpora demonstrate a clear superiority of DM-DTM over strong baselines for both static and dynamic topic models.}
}


@article{dadaneh2018bayesian, 
  author = {Dadaneh, Siamak Zamani and Zhou, Mingyuan and Qian, Xiaoning}, 
  title = "{Bayesian Negative Binomial Regression for Differential Expression with Confounding Factors}", 
  journal = {Bioinformatics}, 
  volume = {34}, 
  number = {19}, 
  pages = {3349-3356}, 
  year = {2018}, 
  month = {04}, 
  abstract = "{Rapid adoption of high-throughput sequencing technologies has enabled better understanding of genome-wide molecular profile changes associated with phenotypic differences in biomedical studies. Often, these changes are due to multiple interacting factors. Existing methods are mostly considering differential expression across two conditions studying one main factor without considering other confounding factors. In addition, they are often coupled with essential sophisticated ad-hoc pre-processing steps such as normalization, restricting their adaptability to general experimental setups. Complex multi-factor experimental design to accurately decipher genotype-phenotype relationships signifies the need for developing effective statistical tools for genome-scale sequencing data profiled under multi-factor conditions.We have developed a novel Bayesian negative binomial regression (BNB-R) method for the analysis of RNA sequencing (RNA-seq) count data. In particular, the natural model parameterization removes the needs for the normalization step, while the method is capable of tackling complex experimental design involving multi-variate dependence structures. Efficient Bayesian inference of model parameters is obtained by exploiting conditional conjugacy via novel data augmentation techniques. Comprehensive studies on both synthetic and real-world RNA-seq data demonstrate the superior performance of BNB-R in terms of the areas under both the receiver operating characteristic and precision-recall curves.BNB-R is implemented in R language and is available at https://github.com/siamakz/BNBR.Supplementary data are available at Bioinformatics online.}", 
  issn = {1367-4803}, 
  doi = {10.1093/bioinformatics/bty330}, 
  url={https://doi.org/10.1093/bioinformatics/bty330},
pdf={https://academic.oup.com/bioinformatics/article-pdf/34/19/3349/25839666/bty330.pdf}, 
url_code={https://GitHub.com/siamakz/BNBR}
}


@article{hajiramezanali2018differential, 
title={Differential Expression Analysis of Dynamical Sequencing Count Data with a Gamma {M}arkov Chain}, 
author={Hajiramezanali, Ehsan and Dadaneh, Siamak Zamani and de Figueiredo, Paul and Sze, Sing-Hoi and Zhou, Mingyuan and Qian, Xiaoning}, 
journal={arXiv preprint arXiv:1803.02527}, 
url={https://arxiv.org/abs/1803.02527}, 
pdf={https://arxiv.org/pdf/1803.02527.pdf}, 
year={2018}
}


@article{zamani2018covariate, 
author = {Zamani Dadaneh, Siamak and Zhou, Mingyuan and Qian, Xiaoning}, 
  title = "{Covariate-Dependent Negative Binomial Factor Analysis of {RNA} Sequencing Data}", 
  journal = {Bioinformatics}, 
  volume = {34}, 
  number = {13}, 
  pages = {i61-i69}, 
  year = {2018}, 
  month = {06}, 
  abstract = "{High-throughput sequencing technologies, in particular RNA sequencing (RNA-seq), have become the basic practice for genomic studies in biomedical research. In addition to studying genes individually, for example, through differential expression analysis, investigating co-ordinated expression variations of genes may help reveal the underlying cellular mechanisms to derive better understanding and more effective prognosis and intervention strategies. Although there exists a variety of co-expression network based methods to analyze microarray data for this purpose, instead of blindly extending these methods for microarray data that may introduce unnecessary bias, it is crucial to develop methods well adapted to RNA-seq data to identify the functional modules of genes with similar expression patterns.We have developed a fully Bayesian covariate-dependent negative binomial factor analysis (dNBFA) methoddNBFAfor RNA-seq count data, to capture coordinated gene expression changes, while considering effects from covariates reflecting different influencing factors. Unlike existing co-expression network based methods, our proposed model does not require multiple ad-hoc choices on data processing, transformation, as well as co-expression measures and can be directly applied to RNA-seq data. Furthermore, being capable of incorporating covariate information, the proposed method can tackle setups with complex confounding factors in different experiment designs. Finally, the natural model parameterization removes the need for a normalization preprocessing step, as commonly adopted to compensate for the effect of sequencing-depth variations. Efficient Bayesian inference of model parameters is derived by exploiting conditional conjugacy via novel data augmentation techniques. Experimental results on several real-world RNA-seq datasets on complex diseases suggest dNBFA as a powerful tool for discovering the gene modules with significant differential expression and meaningful biological insight.dNBFA is implemented in R language and is available at https://github.com/siamakz/dNBFA.}", 
  issn = {1367-4803}, 
  doi = {10.1093/bioinformatics/bty237}, 
  url = {https://doi.org/10.1093/bioinformatics/bty237}, 
  pdf = {https://academic.oup.com/bioinformatics/article-pdf/34/13/i61/25098240/bty237.pdf}, 
  url_code={https://GitHub.com/siamakz/dNBFA}
}


@article{zhang2018permuted, 
author  = {Quan Zhang and Mingyuan Zhou}, 
title   = {Permuted and Augmented Stick-Breaking {B}ayesian Multinomial Regression}, 
journal = {Journal of Machine Learning Research}, 
year    = {2018}, 
volume  = {18}, 
number  = {204}, 
pages   = {1-33}, 
url     = {http://jmlr.org/papers/v18/17-409.html}, 
pdf = {Papers/paSB_MultReg_v16.pdf}, 
url_slide={Papers/paSB_MultReg_slides_v2.pdf}, 
url_arxiv={https://arxiv.org/abs/1612.09413}, 
abstract={To model categorical response variables given their covariates, we propose a permuted and augmented stick-breaking (paSB) construction that one-to-one maps the observed categories to randomly permuted latent sticks. This new construction transforms multinomial regression into regression analysis of stick-specific binary random variables that are mutually independent given their covariate-dependent stick success probabilities, which are parameterized by the regression coefficients of their corresponding categories. The paSB construction allows transforming an arbitrary cross-entropy-loss binary classifier into a Bayesian multinomial one. Specifically, we parameterize the negative logarithms of the stick failure probabilities with a family of covariate-dependent softplus functions to construct nonparametric Bayesian multinomial softplus regression, and transform Bayesian support vector machine (SVM) into Bayesian multinomial SVM. These Bayesian multinomial regression models are not only capable of providing probability estimates, quantifying uncertainty, increasing robustness, and producing nonlinear classification decision boundaries, but also amenable to posterior simulation. Example results demonstrate their attractive properties and performance.}
}


@inproceedings{
zhang2018whai,
title={{WHAI}: {W}eibull Hybrid Autoencoding Inference for Deep Topic Modeling},
author={Hao Zhang and Bo Chen and Dandan Guo and Mingyuan Zhou},
booktitle={ICLR 2018: International Conference on Learning Representations},
year={2018},
url={https://openreview.net/forum?id=S1cZsf-RW},
pdf={Papers/WHAI_ICLR2018.pdf},
url_arxiv={https://arxiv.org/abs/1803.01328},
url_code={https://GitHub.com/BoChenGroup/WHAI},
abstract={To train an inference network jointly with a deep generative topic model, making it both scalable to big corpora and fast in out-of-sample prediction, we develop Weibull hybrid autoencoding inference (WHAI) for deep latent Dirichlet allocation, which infers posterior samples via a hybrid of stochastic-gradient MCMC and autoencoding variational Bayes. The generative network of WHAI has a hierarchy of gamma distributions, while the inference network of WHAI is a Weibull upward-downward variational autoencoder, which integrates a deterministic-upward deep neural network, and a stochastic-downward deep generative model based on a hierarchy of Weibull distributions. The Weibull distribution can be used to well approximate a gamma distribution with an analytic Kullback-Leibler divergence, and has a simple reparameterization via the uniform noise, which help efficiently compute the gradients of the evidence lower bound with respect to the parameters of the inference network. The effectiveness and efficiency of WHAI are illustrated with experiments on big corpora.}
}


@inproceedings{kalantari2018nonparametric, 
title = 	 {Nonparametric {B}ayesian Sparse Graph Linear Dynamical Systems}, 
author = 	 {Rahi Kalantari and Joydeep Ghosh and Mingyuan Zhou}, 
booktitle = 	 {AISTATS 2018: International Conference on Artificial Intelligence and Statistics}, 
pages = 	 {1952--1960}, 
year = 	 {2018}, 
editor = 	 {Amos Storkey and Fernando Perez-Cruz}, 
volume = 	 {84}, 
series = 	 {Proceedings of Machine Learning Research}, 
address = 	 {Playa Blanca, Lanzarote, Canary Islands}, 
month = 	 {09--11 Apr}, 
publisher = 	 {PMLR}, 
pdf = 	 {Papers/BerPo_LDS_v4.pdf}, 
url = 	 {http://proceedings.mlr.press/v84/kalantari18a.html}, 
url_arxiv=	{https://arxiv.org/abs/1802.07434}, 
abstract = 	 {A nonparametric Bayesian sparse graph linear dynamical system (SGLDS) is proposed to model sequentially observed multivariate data. SGLDS uses the Bernoulli-Poisson link together with a gamma process to generate an infinite dimensional sparse random graph to model state transitions. Depending on the sparsity pattern of the corresponding row and column of the graph affinity matrix, a latent state of SGLDS can be categorized as either a non-dynamic state or a dynamic one. A normal-gamma construction is used to shrink the energy captured by the non-dynamic states, while the dynamic states can be further categorized into live, absorbing, or noise-injection states, which capture different types of dynamical components of the underlying time series. The state-of-the-art performance of SGLDS is demonstrated with experiments on both synthetic and real data.}
}

	

	

@article{xie2018,
author = "Xie, Fangzheng and Zhou, Mingyuan and Xu, Yanxun",
doi = "10.1214/17-AOAS1123",
fjournal = "Annals of Applied Statistics",
journal = "Ann. Appl. Stat.",
month = "09",
number = "3",
pages = "1605--1627",
publisher = "The Institute of Mathematical Statistics",
title = "{BayCount}: A {B}ayesian Decomposition Method for Inferring Tumor Heterogeneity using RNA-Seq Counts",
url = "https://doi.org/10.1214/17-AOAS1123",
volume = "12",
year = "2018",
pdf={Papers/BayCount_AOAS.pdf},
url_arxiv={https://arxiv.org/abs/1702.07981},
abstract={Tumors are heterogeneous. A tumor sample usually consists of a set of subclones with distinct transcriptional profiles and potentially different degrees of aggressiveness and responses to drugs. Understanding tumor heterogeneity is therefore critical for precise cancer prognosis and treatment. In this paper we introduce BayCounta Bayesian decomposition method to infer tumor heterogeneity with highly over-dispersed RNA sequencing count data. Using negative binomial factor analysis, BayCount takes into account both the between-sample and gene-specific random effects on raw counts of sequencing reads mapped to each gene. For the posterior inference, we develop an efficient compound Poisson-based blocked Gibbs sampler. Simulation studies show that BayCount is able to accurately estimate the subclonal inference, including the number of subclones, the proportions of these subclones in each tumor sample, and the gene expression profiles in each subclone. For real world data examples, we apply BayCount to The Cancer Genome Atlas lung cancer and kidney cancer RNA sequencing count data and obtain biologically interpretable results. Our method represents the first effort in characterizing tumor heterogeneity using RNA sequencing count data that simultaneously removes the need of normalizing the counts, achieves statistical robustness, and obtains biologically/clinically meaningful insights. The R package BayCount implementing our model and algorithm is available for download.}
}




@inproceedings{wang2018multimodal,
	author = {Chaojie Wang and Bo Chen and Mingyuan Zhou},
	title = {Multimodal {P}oisson Gamma Belief Network},
	booktitle = {AAAI Conference on Artificial Intelligence},
	year = {2018},
	url = {https://aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16207},
	pdf={Papers/mpgbn_aaai18.pdf},
	url_poster={Papers/AAAI2018_poster.pdf},
	url_code={https://GitHub.com/BoChenGroup/Multimodal_PGBN},
	abstract = {To learn a deep generative model of multimodal data, we propose a  multimodal Poisson gamma belief network (mPGBN) that tightly couple the data of different modalities at multiple hidden layers. The mPGBN unsupervisedly extracts a nonnegative latent representation using an upward-downward Gibbs sampler. It imposes sparse connections between different layers, making it simple to visualize the generative process and the relationships between the latent features of different modalities. Our experimental results on bi-modal data consisting of images and tags show that the mPGBN can easily impute a missing modality and hence is useful for both image annotation and retrieval. We further demonstrate that the mPGBN achieves state-of-the-art results on unsupervisedly extracting latent features from multimodal data.}
}



@article{zhou2018discussion, 
title={Discussion on" Sparse Graphs using Exchangeable Random Measures" by Francois Caron and Emily B. Fox}, 
author={Zhou, Mingyuan}, 
journal={arXiv preprint arXiv:1802.07721}, 
url={https://rss.onlinelibrary.wiley.com/doi/abs/10.1111/rssb.12233}, 
pdf={Papers/Zhou_Discussion_CaronFox_JRSSB.pdf}, 
url_arxiv={https://arxiv.org/pdf/1802.07721}, 
journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
volume = {79},
number = {5},
pages = {1295-1366}, 
year={2018}, 
abstract={This is a discussion on "Sparse graphs using exchangeable random measures" by Francois Caron and Emily B. Fox, published in Journal of the Royal Statistical Society, Series B, 2017.}
}


@article{zhou2018nonparametric, 
title={Nonparametric Bayesian Negative Binomial Factor Analysis}, 
author={Zhou, Mingyuan}, 
journal={Bayesian Analysis}, 
volume={13}, 
number={4}, 
pages={1065--1093}, 
year={2018}, 
publisher={International Society for Bayesian Analysis}, 
url={https://projecteuclid.org/euclid.ba/1510801993}, 
pdf={Papers/NBFA_v10.pdf}, 
url_arxiv={http://arxiv.org/abs/1604.07464}, 
url_code={https://GitHub.com/mingyuanzhou/NBFA}, 
abstract={A common approach to analyze a covariate-sample count matrix, an element of which represents how many times a covariate appears in a sample, is to factorize it under the Poisson likelihood. We show its limitation in capturing the tendency for a covariate present in a sample to both repeat itself and excite related ones. To address this limitation, we construct negative binomial factor analysis (NBFA) to factorize the matrix under the negative binomial likelihood, and relate it to a Dirichlet-multinomial distribution based mixed-membership model. To support countably infinite factors, we propose the hierarchical gamma-negative binomial process. By exploiting newly proved connections between discrete distributions, we construct two blocked and a collapsed Gibbs sampler that all adaptively truncate their number of factors, and demonstrate that the blocked Gibbs sampler developed under a compound Poisson representation converges fast and has low computational complexity. Example results show that NBFA has a distinct mechanism in adjusting its number of inferred factors according to the sample lengths, and provides clear advantages in parsimonious representation, predictive power, and computational complexity over previously proposed discrete latent variable models, which either completely ignore burstiness, or model only the burstiness of the covariates but not that of the factors.}
}


@inproceedings{cong2017deep, 
title = 	 {Deep Latent {D}irichlet Allocation with Topic-Layer-Adaptive Stochastic Gradient {R}iemannian {MCMC}}, 
author = 	 {Yulai Cong and Bo Chen and Hongwei Liu and Mingyuan Zhou}, 
booktitle = 	 {ICML 2017: International Conference on Machine Learning}, 
pages = 	 {864--873}, 
year = 	 {2017}, 
editor = 	 {Doina Precup and Yee Whye Teh}, 
volume = 	 {70}, 
series = 	 {Proceedings of Machine Learning Research}, 
address = 	 {International Convention Centre, Sydney, Australia}, 
month = 	 {06--11 Aug}, 
publisher = 	 {PMLR}, 
pdf = 	 {Papers/DLDA_TLASGR_v12.pdf}, 
url = 	 {http://proceedings.mlr.press/v70/cong17a.html}, 
url_arxiv={https://arxiv.org/abs/1706.01724}, 
url_slide={Papers/DLDA_ICML2017_slides.pdf}, 
url_poster={Papers/DLDA_ICML2017_poster.pdf}, 
url_code={https://GitHub.com/mingyuanzhou/DeepLDA_TLASGR_MCMC}, 
abstract = 	 {It is challenging to develop stochastic gradient based scalable inference for deep discrete latent variable models (LVMs), due to the difficulties in not only computing the gradients, but also adapting the step sizes to different latent factors and hidden layers. For the Poisson gamma belief network (PGBN), a recently proposed deep discrete LVM, we derive an alternative representation that is referred to as deep latent Dirichlet allocation (DLDA). Exploiting data augmentation and marginalization techniques, we derive a block-diagonal Fisher information matrix and its inverse for the simplex-constrained global model parameters of DLDA. Exploiting that Fisher information matrix with stochastic gradient MCMC, we present topic-layer-adaptive stochastic gradient Riemannian (TLASGR) MCMC that jointly learns simplex-constrained global parameters across all layers and topics, with topic and layer specific learning rates. State-of-the-art results are demonstrated on big data sets.}
}


@article{dadaneh2018bnp, 
title={BNP-Seq: Bayesian Nonparametric Differential Expression Analysis of Sequencing Count Data}, 
author={Dadaneh, Siamak Zamani and Qian, Xiaoning and Zhou, Mingyuan}, 
journal={Journal of the American Statistical Association}, 
volume={113}, 
number={521}, 
pages={81--94}, 
year={2018}, 
publisher={Taylor \& Francis}, 
url={http://dx.doi.org/10.1080/01621459.2017.1328358}, 
pdf={Papers/BNPseq_v12_arXiv.pdf}, 
url_arxiv={http://arxiv.org/abs/1608.03991}, 
url_code={https://GitHub.com/siamakz/BNPseq}, 
url_abstract={We perform differential expression analysis of high-throughput sequencing count data under a Bayesian nonparametric framework, removing sophisticated ad-hoc pre-processing steps commonly required in existing algorithms. We propose to use the gamma (beta) negative binomial process, which takes into account different sequencing depths using sample-specific negative binomial probability (dispersion) parameters, to detect differentially expressed genes by comparing the posterior distributions of gene-specific negative binomial dispersion (probability) parameters. These model parameters are inferred by borrowing statistical strength across both the genes and samples. Extensive experiments on both simulated and real-world RNA sequencing count data show that the proposed differential expression analysis algorithms clearly outperform previously proposed ones in terms of the areas under both the receiver operating characteristic and precision-recall curves.}
}


@article{cong2017fast, 
title={Fast Simulation of Hyperplane-Truncated Multivariate Normal Distributions}, 
author={Cong, Yulai and Chen, Bo and Zhou, Mingyuan}, 
journal={Bayesian Analysis}, 
volume={12}, 
number={4}, 
pages={1017--1037}, 
year={2017}, 
publisher={International Society for Bayesian Analysis}, 
url={https://projecteuclid.org/euclid.ba/1488337478}, 
pdf= {Papers/MVN_Hyperplanes_v9.pdf}, 
url_arxiv={http://arxiv.org/abs/1607.04751}, 
url_code={https://github.com/BoChenGroup/Fast-simulation-of-hyperplane-truncated-multivariate-normal-distribution}, 
abstract={  We introduce a fast and easy-to-implement simulation algorithm for a multivariate normal distribution truncated on the intersection of a set of hyperplanes, and further generalize it to efficiently simulate random variables from a multivariate normal distribution whose covariance (precision) matrix can be decomposed as a positive-definite matrix minus (plus) a low-rank symmetric matrix. Example results illustrate the correctness and efficiency of the proposed simulation algorithms.}	
}



@inproceedings{schein2016poisson,
title = {Poisson-Gamma Dynamical Systems},
author = {Schein, Aaron and Wallach, Hanna and Zhou, Mingyuan},
booktitle = {NeurIPS 2016: Advances in Neural Information Processing Systems},
editor = {D. D. Lee and M. Sugiyama and U. V. Luxburg and I. Guyon and R. Garnett},
pages = {5005--5013},
year = {2016}, 
url={https://papers.nips.cc/paper/6083-poisson-gamma-dynamical-systems},
pdf = {Papers/ScheinZhouWallach2016_paper.pdf},
url_code = {https://GitHub.com/aschein/pgds">Python Code in GitHub},
url_slide={Papers/ScheinZhouWallach2016_slides.pdf},
url_poster={Papers/ScheinZhouWallach2016_poster.pdf},
url_video={https://channel9.msdn.com/Events/Neural-Information-Processing-Systems-Conference/Neural-Information-Processing-Systems-Conference-NIPS-2016/Poisson-Gamma-dynamical-systems},
note = {(Full oral presentation)},
abstract = {This paper presents a dynamical system based on the Poisson-Gamma construction for sequentially observed multivariate count data. Inherent to the model is a novel Bayesian nonparametric prior that ties and shrinks parameters in a powerful way. We develop theory about the model's infinite limit and its steady-state. The model's inductive bias is demonstrated on a variety of real-world datasets where it is shown to learn interpretable structure and have superior predictive performance.}
}


@article{zhou2016softplus, 
title={Softplus Regressions and Convex Polytopes}, 
author={Zhou, Mingyuan}, 
journal={arXiv:1608.06383}, 
year={2016}, 
url={https://arxiv.org/abs/1608.06383}, 
pdf={Papers/softplus_regression_v14.pdf}, 
url_arxiv={https://arxiv.org/abs/1608.06383}, 
abstract={To construct flexible nonlinear predictive distributions, the paper introduces a family of softplus function based regression models that convolve, stack, or combine both operations by convolving countably infinite stacked gamma distributions, whose scales depend on the covariates. Generalizing logistic regression that uses a single hyperplane to partition the covariate space into two halves, softplus regressions employ multiple hyperplanes to construct a confined space, related to a single convex polytope defined by the intersection of multiple half-spaces or a union of multiple convex polytopes, to separate one class from the other. The gamma process is introduced to support the convolution of countably infinite (stacked) covariate-dependent gamma distributions. For Bayesian inference, Gibbs sampling derived via novel data augmentation and marginalization techniques is used to deconvolve and/or demix the highly complex nonlinear predictive distribution. Example results demonstrate that softplus regressions provide flexible nonlinear decision boundaries, achieving classification accuracies comparable to that of kernel support vector machine while requiring significant less computation for out-of-sample prediction.}
}




@article{zhou2017frequency, 
title={Frequency of Frequencies Distributions and Size-Dependent Exchangeable Random Partitions}, 
author={Zhou, Mingyuan and Favaro, Stefano and Walker, Stephen G}, 
journal={Journal of the American Statistical Association}, 
pages={1--13}, 
year={2017}, 
publisher={Taylor \& Francis}, 
url={http://www.tandfonline.com/doi/full/10.1080/01621459.2016.1222290}, 
pdf={Papers/dEPPF_v18.pdf}, 
url_arxiv={http://arxiv.org/abs/1608.00264}, 
url_code={https://GitHub.com/mingyuanzhou/FoF_EPPF}, 
abstract={Motivated by the fundamental problem of modeling the frequency of frequencies (FoF) distribution, this article introduces the concept of a cluster structure to define a probability function that governs the joint distribution of a random count and its exchangeable random partitions. A cluster structure, naturally arising from a completely random measure mixed Poisson process, allows the probability distribution of the random partitions of a subset of a population to be dependent on the population size, a distinct and motivated feature that makes it more flexible than a partition structure. This allows it to model an entire FoF distribution whose structural properties change as the population size varies. An FoF vector can be simulated by drawing an infinite number of Poisson random variables, or by a stick-breaking construction with a finite random number of steps. A generalized negative binomial process model is proposed to generate a cluster structure, where in the prior the number of clusters is finite and Poisson distributed, and the cluster sizes follow a truncated negative binomial distribution. We propose a simple Gibbs sampling algorithm to extrapolate the FoF vector of a population given the FoF vector of a sample taken without replacement from the population. We illustrate our results and demonstrate the advantages of the proposed models through the analysis of real text, genomic, and survey data. Supplementary materials for this article are available online.}
}






@inproceedings{schein2016bayesian, 
title = 	 {Bayesian {P}oisson {T}ucker Decomposition for Learning the Structure of International Relations}, 
author = 	 {Aaron Schein and Mingyuan Zhou and David Blei and Hanna Wallach}, 
booktitle = 	 {ICML 2016: International Conference on Machine Learning}, 
pages = 	 {2810--2819}, 
year = 	 {2016}, 
editor = 	 {Maria Florina Balcan and Kilian Q. Weinberger}, 
volume = 	 {48}, 
series = 	 {Proceedings of Machine Learning Research}, 
address = 	 {New York, New York, USA}, 
month = 	 {20--22 Jun}, 
publisher = 	 {PMLR}, 
pdf = 	 {Papers/ScheinZhouBleiWallach2016_paper.pdf}, 
url = 	 {http://proceedings.mlr.press/v48/schein16.html}, 
url_arxiv={https://arxiv.org/abs/1606.01855}, 
url_slide={http://www.columbia.edu/~as5530/ScheinZhouBleiWallach2016_slides.pdf},
url_video={http://techtalks.tv/talks/bayesian-poisson-tucker-decomposition-for-learning-the-structure-of-international-relations/62411/},
url_poster={http://www.columbia.edu/~as5530/ScheinZhouBleiWallach2016_poster.pdf},
url_code={https://github.com/aschein/bptd},
abstract = 	 {We introduce Bayesian Poisson Tucker decomposition (BPTD) for modeling countrycountry interaction event data. These data consist of interaction events of the form country i took action a toward country j at time t. BPTD discovers overlapping countrycommunity memberships, including the number of latent communities. In addition, it discovers directed communitycommunity interaction networks that are specific to topics of action types and temporal regimes. We show that BPTD yields an efficient MCMC inference algorithm and achieves better predictive performance than related models. We also demonstrate that it discovers interpretable latent structure that agrees with our knowledge of international relations.}
}


@article{zhou2016augmentable, 
title={Augmentable Gamma Belief Networks}, 
author={Zhou, Mingyuan and Cong, Yulai and Chen, Bo}, 
journal={Journal of Machine Learning Research}, 
volume={17}, 
number={163}, 
pages={1--44}, 
year={2016}, 
url={http://jmlr.org/papers/v17/15-633.html}, 
pdf={Papers/DeepPoGamma_Journal_v5.pdf}, 
url_arxiv={http://arxiv.org/abs/1512.03081}, 
url_slide={Papers/GBN_ISBA_201606.pdf}, 
url_code ={https://GitHub.com/mingyuanzhou/GBN}, 
 abstract = {To infer multilayer deep representations of high-dimensional discrete and nonnegative real vectors, we propose an augmentable gamma belief network (GBN) that factorizes each of its hidden layers into the product of a sparse connection weight matrix and the nonnegative real hidden units of the next layer. The GBN's hidden layers are jointly trained with an upward-downward Gibbs sampler that solves each layer with the same subroutine. The gamma-negative binomial process combined with a layer-wise training strategy allows inferring the width of each layer given a fixed budget on the width of the first layer. Example results illustrate interesting relationships between the width of the first layer and the inferred network structure, and demonstrate that the GBN can add more layers to improve its performance in both unsupervisedly extracting features and predicting heldout data. For exploratory data analysis, we extract trees and subnetworks from the learned deep network to visualize how the very specific factors discovered at the first hidden layer and the increasingly more general factors discovered at deeper hidden layers are related to each other, and we generate synthetic data by propagating random variables through the deep network from the top hidden layer back to the bottom data layer.}
}



@inproceedings{zhou2015poisson, 
title={The {P}oisson gamma belief network}, 
author = {Zhou, Mingyuan and Cong, Yulai and Chen, Bo}, 
 booktitle = {NeurIPS 2015: Advances in Neural Information Processing Systems}, 
editor = {C. Cortes and N. D. Lawrence and D. D. Lee and M. Sugiyama and R. Garnett}, 
pages = {3043--3051}, 
year = {2015}, 
   url={https://papers.nips.cc/paper/5645-the-poisson-gamma-belief-network}, 
pdf={Papers/DeepPoGamma_v5.pdf}, 
url_arxiv={http://arxiv.org/abs/1511.02199}, 
url_poster={Papers/PGBN_NIPS2015_Poster.pdf}, 
url_code ={https://GitHub.com/mingyuanzhou/GBN}, 
abstract={To infer a multilayer representation of high-dimensional count vectors, we propose the Poisson gamma belief network (PGBN) that factorizes each of its layers into the product of a connection weight matrix and the nonnegative real hidden units of the next layer. The PGBN's hidden layers are jointly trained with an upward-downward Gibbs sampler, each iteration of which upward samples Dirichlet distributed connection weight vectors starting from the first layer (bottom data layer), and then downward samples gamma distributed hidden units starting from the top hidden layer. The gamma-negative binomial process combined with a layer-wise training strategy allows the PGBN to infer the width of each layer given a fixed budget on the width of the first layer. The PGBN with a single hidden layer reduces to Poisson factor analysis. Example results on text analysis illustrate interesting relationships between the width of the first layer and the inferred network structure, and demonstrate that the PGBN, whose hidden units are imposed with correlated gamma priors, can add more layers to increase its performance gains over Poisson factor analysis, given the same limit on the width of the first layer.}
}

	

@article{zhou2016priors, 
title={Priors for Random Count Matrices Derived from a Family of Negative Binomial Processes}, 
author={Zhou, Mingyuan and Padilla, Oscar Hernan Madrid and Scott, James G}, 
journal={Journal of the American Statistical Association}, 
volume={111}, 
number={515}, 
pages={1144--1156}, 
year={2016}, 
publisher={Taylor \& Francis}, 
url={https://www.tandfonline.com/doi/abs/10.1080/01621459.2015.1075407}, 
pdf={Papers/NBP_VectorMatrix_Journal_revise3_ArXiv.pdf}, 
url_arxiv={http://arxiv.org/abs/1404.3331}, 
url_slide={Papers/NBP_Count_Matrix_201506.pdf}, 
url_code={https://GitHub.com/mingyuanzhou/NBP_random_count_matrices},
}



@InProceedings{10.1007/978-3-319-23528-8_18,
author={Acharya, Ayan
and Teffer, Dean
and Henderson, Jette
and Tyler, Marcus
and Zhou, Mingyuan
and Ghosh, Joydeep},
editor={Appice, Annalisa
and Rodrigues, Pedro Pereira
and Santos Costa, V{\'i}tor
and Soares, Carlos
and Gama, Jo{\~a}o
and Jorge, Al{\'i}pio},
title={Gamma Process Poisson Factorization for Joint Modeling of Network and Documents},
booktitle={ECML PKDD 2015: Machine Learning and Knowledge Discovery in Databases},
year={2015},
publisher={Springer International Publishing},
address={Cham},
pages={283--299},
abstract={Developing models to discover, analyze, and predict clusters within networked entities is an area of active and diverse research. However, many of the existing approaches do not take into consideration pertinent auxiliary information. This paper introduces Joint Gamma Process Poisson Factorization (J-GPPF) to jointly model network and side-information. J-GPPF naturally fits sparse networks, accommodates separately-clustered side information in a principled way, and effectively addresses the computational challenges of analyzing large networks. Evaluated with hold-out link prediction performance on sparse networks (both synthetic and real-world) with side information, J-GPPF is shown to clearly outperform algorithms that only model the network adjacency matrix.},
isbn={978-3-319-23528-8},
url={https://link.springer.com/chapter/10.1007/978-3-319-23528-8_18},
pdf={Papers/acth15.pdf},
}




@INPROCEEDINGS{7362890, 
author={Mingyuan Zhou}, 
booktitle={European Signal Processing Conference (EUSIPCO)}, 
 title={Nonparametric Bayesian Matrix Factorization for Assortative Networks}, 
 year={2015}, 
volume={}, 
number={}, 
pages={2776-2780}, 
abstract={We describe in detail the gamma process edge partition model that is well suited to analyze assortative relational networks. The model links the binary edges of an undirected and unweighted relational network with a latent factor model via the Bernoulli-Poisson link, and uses the gamma process to support a potentially infinite number of latent communities. The communities are allowed to overlap with each other, with a community's overlapping parts assumed to be more densely connected than its non-overlapping ones. The model is evaluated with synthetic data to illustrate its ability to model as-sortative networks and its restriction on modeling dissortative ones.}, 
keywords={Bayes methods;matrix decomposition;social networking (online);stochastic processes;Bernoulli-Poisson link;latent factor model;unweighted relational network;binary edges;assortative relational networks;gamma process edge partition model;nonparametric Bayesian matrix factorization;Predator prey systems;Bayes methods;Computational modeling;Analytical models;Data models;Mathematical model;Europe;Gamma process;factor analysis;Bernoulli-Poisson link;overlapping community detection;link prediction}, 
doi={10.1109/EUSIPCO.2015.7362890}, 
ISSN={2076-1465}, 
month={Aug},
url={https://ieeexplore.ieee.org/document/7362890/similar#similar},
pdf={Papers/EPM_EUROSIPCO2015.pdf},
url_slide = {EPM_EUSIPCO_201509.pdf},
note={(Invited special session paper)},
}

@inproceedings{zhou2015infinite, 
title = 	 {Infinite Edge Partition Models for Overlapping Community Detection and Link Prediction}, 
author = 	 {Mingyuan Zhou}, 
booktitle = 	 {AISTATS 2015: International Conference on Artificial Intelligence and Statistics}, 
pages = 	 {1135--1143}, 
year = 	 {2015}, 
editor = 	 {Guy Lebanon and S. V. N. Vishwanathan}, 
volume = 	 {38}, 
series = 	 {Proceedings of Machine Learning Research}, 
address = 	 {San Diego, California, USA}, 
month = 	 {09--12 May}, 
publisher = 	 {PMLR}, 
pdf = 	 {Papers/EPM_AISTATS2015_v8_1.pdf}, 
url = 	 {http://proceedings.mlr.press/v38/zhou15a.html}, 
url_code={https://GitHub.com/mingyuanzhou/EPM}, 
url_poster={Papers/EPM_AISTATS2015_Poster.pdf}, 
abstract = 	 {A hierarchical gamma process infinite edge partition model is proposed to factorize the binary adjacency matrix of an unweighted undirected relational network under a  Bernoulli-Poisson link. The model describes  both homophily and stochastic equivalence, and is scalable to  big sparse networks by focusing its computation on  pairs of linked nodes. It can not only discover overlapping communities and inter-community interactions, but also predict missing edges. A simplified version omitting inter-community interactions is also provided and we reveal its  interesting connections to existing models. The number of communities is automatically inferred in a nonparametric Bayesian manner, and efficient  inference via Gibbs sampling is derived using novel data augmentation techniques. Experimental results on four real networks demonstrate the models scalability and state-of-the-art performance.},
}


@inproceedings{acharya2015nonparametric, 
title = 	 {Nonparametric {B}ayesian Factor Analysis for Dynamic Count Matrices}, 
author = 	 {Ayan Acharya and Joydeep Ghosh and Mingyuan Zhou}, 
booktitle = 	 {AISTATS 2015: International Conference on Artificial Intelligence and Statistics}, 
pages = 	 {1--9}, 
year = 	 {2015}, 
editor = 	 {Guy Lebanon and S. V. N. Vishwanathan}, 
volume = 	 {38}, 
series = 	 {Proceedings of Machine Learning Research}, 
address = 	 {San Diego, California, USA}, 
month = 	 {09--12 May}, 
publisher = 	 {PMLR}, 
pdf = 	 {Papers/GP_DPFA_AISTATS2015_v6.pdf}, 
url = 	 {http://proceedings.mlr.press/v38/acharya15.html}, 
url_arxiv={http://arxiv.org/abs/1512.08996}, 
abstract = 	 {A gamma process dynamic Poisson factor analysis model is proposed to factorize a dynamic count matrix, whose columns are sequentially observed count vectors. The model builds a novel  Markov chain that sends the latent gamma random variables at time (t-1) as the shape parameters of those at time t, which are linked to observed or latent counts under the Poisson likelihood.  The significant challenge of inferring the gamma shape parameters is fully addressed, using unique data augmentation and marginalization techniques for the negative binomial distribution. The same nonparametric Bayesian model also applies to the factorization of a dynamic binary matrix, via a Bernoulli-Poisson link that connects a binary observation to a latent count, with closed-form conditional posteriors for the latent counts and efficient computation for sparse observations. We apply the model to text and music analysis, with state-of-the-art results.},
}




@inproceedings{zhou2014beta, 
title={Beta-Negative Binomial Process and Exchangeable Random Partitions for Mixed-Membership Modeling},
author = {Zhou, Mingyuan},
booktitle = {NeurIPS 2014: Advances in Neural Information Processing Systems},
editor = {Z. Ghahramani and M. Welling and C. Cortes and N. D. Lawrence and K. Q. Weinberger},
pages = {3455--3463},
year = {2014}, 
url = {http://papers.nips.cc/paper/5298-beta-negative-binomial-process-and-exchangeable-random-partitions-for-mixed-membership-modeling.html},
pdf={Papers/BNBP_Collapsed_v7_arXiv.pdf},
url_poster={Papers/BNBP_NIPS2014_Poster.pdf},
url_code = {https://GitHub.com/mingyuanzhou/BNBP_collapsed},
url_arxiv={http://arxiv.org/abs/1410.7812},
Abstract={The beta-negative binomial process (BNBP), an integer-valued stochastic process, is employed to partition a count vector into a latent random count matrix. As the marginal probability distribution of the BNBP that governs the exchangeable random partitions of grouped data has not yet been developed, current inference for the BNBP has to truncate the number of atoms of the beta process. This paper introduces an exchangeable partition probability function to explicitly describe how the BNBP clusters the data points of each group into a random number of exchangeable partitions, which are shared across all the groups. A fully collapsed Gibbs sampler is developed for the BNBP, leading to a novel nonparametric Bayesian topic model that is distinct from existing ones, with simple implementation, fast convergence, good mixing, and state-of-the-art predictive performance.},
}

	
		
@article{polatkan2015bayesian, 
title={A {B}ayesian Nonparametric Approach to Image Super-Resolution}, 
author={Polatkan, Gungor and Zhou, Mingyuan and Carin, Lawrence and Blei, David and Daubechies, Ingrid}, 
journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
volume={37}, 
number={2}, 
pages={346--358}, 
year={2015}, 
publisher={IEEE}, 
url={http://ieeexplore.ieee.org/document/6809161/}, 
pdf={http://arxiv.org/abs/1209.5019}, 
url_arxiv={http://arxiv.org/abs/1209.5019}, 
abstract={Super-resolution methods form high-resolution images from low-resolution images. In this paper, we develop a new Bayesian nonparametric model for super-resolution. Our method uses a beta-Bernoulli process to learn a set of recurring visual patterns, called dictionary elements, from the data. Because it is nonparametric, the number of elements found is also determined from the data. We test the results on both benchmark and natural images, comparing with several other models from the research literature. We perform large-scale human evaluation experiments to assess the visual quality of the results. In a first implementation, we use Gibbs sampling to approximate the posterior. However, this algorithm is not feasible for large-scale data. To circumvent this, we then develop an online variational Bayes (VB) algorithm. This algorithm finds high quality dictionaries in a fraction of the time needed by the Gibbs sampler.},
}
		

@article{zhou2015negative, 

title={Negative Binomial Process Count and Mixture Modeling}, 

author={Zhou, Mingyuan and Carin, Lawrence}, 

journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
volume={37}, 
number={2}, 

pages={307--320}, 

year={2015}, 

publisher={IEEE}, 

url = {https://ieeexplore.ieee.org/document/6636308}, 

pdf= {Papers/Mingyuan_PAMI_9.pdf}, 

url_arxiv = {http://arxiv.org/abs/1209.3442}, 

url_code={Softwares/NBP_PFA_v1.zip}, 
abstract={The seemingly disjoint problems of count and mixture modeling are united under the negative binomial (NB) process. A gamma process is employed to model the rate measure of a Poisson process, whose normalization provides a random probability measure for mixture modeling and whose marginalization leads to an NB process for count modeling. A draw from the NB process consists of a Poisson distributed finite number of distinct atoms, each of which is associated with a logarithmic distributed number of data samples. We reveal relationships between various count- and mixture-modeling distributions and construct a Poisson-logarithmic bivariate distribution that connects the NB and Chinese restaurant table distributions. Fundamental properties of the models are developed, and we derive efficient Bayesian inference. It is shown that with augmentation and normalization, the NB process and gamma-NB process can be reduced to the Dirichlet process and hierarchical Dirichlet process, respectively. These relationships highlight theoretical, structural, and computational advantages of the NB process. A variety of NB processes, including the beta-geometric, beta-NB, marked-beta-NB, marked-gamma-NB and zero-inflated-NB processes, with distinct sharing mechanisms, are also constructed. These models are applied to topic modeling, with connections made to existing algorithms under Poisson factor analysis. Example results show the importance of inferring both the NB dispersion and probability parameters.},
}

@phdthesis{zhou2013nonparametric,
  title={Nonparametric {B}ayesian Dictionary Learning and Count and Mixture Modeling},
  author={Zhou, Mingyuan},
  year={2013},
  school={Duke University},
  url={https://hdl.handle.net/10161/7204},
pdf={https://dukespace.lib.duke.edu/dspace/bitstream/handle/10161/7204/Zhou_duke_0066D_11883.pdf?sequence=1&isAllowed=y},
abstract={Analyzing the ever-increasing data of unprecedented scale, dimensionality, diversity, and complexity poses considerable challenges to conventional approaches of statistical modeling. Bayesian nonparametrics constitute a promising research direction, in that such techniques can fit the data with a model that can grow with complexity to match the data. In this dissertation we consider nonparametric Bayesian modeling with completely random measures, a family of pure-jump stochastic processes with nonnegative increments. In particular, we study dictionary learning for sparse image representation using the beta process and the dependent hierarchical beta process, and we present the negative binomial process, a novel nonparametric Bayesian prior that unites the seemingly disjoint problems of count and mixture modeling. We show a wide variety of successful applications of our nonparametric Bayesian latent variable models to real problems in science and engineering, including count modeling, text analysis, image processing, compressive sensing, and computer vision.}
}


@inproceedings{zhou2012augment,
title = {Augment-and-Conquer Negative Binomial Processes},
author = {Zhou, Mingyuan and Carin, Lawrence},
booktitle = {NeurIPS 2012: Advances in Neural Information Processing Systems},
editor = {F. Pereira and C. J. C. Burges and L. Bottou and K. Q. Weinberger},
pages = {2546--2554},
year = {2012}, 
url = {https://papers.nips.cc/paper/4677-augment-and-conquer-negative-binomial-processes},
pdf = {Papers/Mingyuan_NBP_NIPS2012.pdf},
Url_slide = {Papers/NBP_NIPS2012_Slides_Mingyuan_12052012.pdf},
Url_Poster = {Papers/NBP_NIPS2012_Poster_MingyuanZhou_12052012.pdf},
Url_Code = {Softwares/NBP_PFA_v1.zip},
Note= {(Spotlight oral presentation)},
Abstract={By developing data augmentation methods unique to the negative binomial (NB) distribution, we unite seemingly disjoint count and mixture models under the NB process framework. We develop fundamental properties of the models and derive efficient Gibbs sampling inference. We show that the gamma-NB process can be reduced to the hierarchical Dirichlet process with normalization, highlighting its unique theoretical, structural and computational advantages. A variety of NB processes with distinct sharing mechanisms are constructed and applied to topic modeling, with connections to existing algorithms, showing the importance of inferring both the NB dispersion and probability parameters.},
}



@inproceedings{zhou2012lognormal, 

author =    {Mingyuan Zhou and Lingbo Li and David Dunson and Lawrence Carin}, 
title =     {Lognormal and Gamma Mixed Negative Binomial Regression}, 

booktitle = {ICML 2012: International Conference on Machine Learning}, 

series =    {ICML '12}, 
year =      {2012}, 

editor =    {John Langford and Joelle Pineau}, 
location =  {Edinburgh, Scotland, GB}, 
isbn =      {978-1-4503-1285-1}, 

month =     {July}, 
publisher = {Omnipress}, 

address =   {New York, NY, USA}, 

pages=      {1343--1350}, 

Url_Paper= {Papers/Mingyuan_ICML_2012.pdf}, 
Url_Appendix={Papers/Mingyuan_LGNB_Appendix.pdf}, 

Url_Code = {Softwares/LGNB_Regression_v0.zip}, 
Url_Slide = {Papers/LGNB_MingyuanZhou_06272012.pdf}, 

Url_Video = {http://techtalks.tv/talks/lognormal-and-gamma-mixed-negative-binomial-regression/57332/}, 
url= {https://icml.cc/Conferences/2012/papers/665.pdf}, 

Url_Poster= {Papers/LGNB_ICML2012_Poster_MingyuanZhou_06272012.pdf}, 

Abstract = {In regression analysis of counts, a lack of simple and efficient algorithms for posterior computation has made Bayesian approaches appear unattractive and thus underdeveloped. We propose a lognormal and gamma mixed negative binomial (NB) regression model for counts, and present efficient closed-form Bayesian inference; unlike conventional Poisson models, the proposed approach has two free parameters to include two different kinds of dispersion, and allows the incorporation of prior information, such as sparsity in the regression coefficients. By placing a gamma distribution prior on the NB dispersion parameter r, and connecting a lognormal distribution prior with the logit of the NB probability parameter p, efficient Gibbs sampling and variational Bayes inference are both developed. The closed-form updates are obtained by exploiting conditional conjugacy via both a compound Poisson representation and a Polya-Gamma distribution based data augmentation approach. The proposed Bayesian inference can be implemented routinely, while being easily generalizable to more complex settings involving multivariate dependence structures. The algorithms are illustrated using real examples.},
}



@InProceedings{zhou2011beta, 
title = 	 {Beta-Negative Binomial Process and Poisson Factor Analysis}, 
author = 	 {Mingyuan Zhou and Lauren Hannah and David Dunson and Lawrence Carin}, 
booktitle = 	 {AISTATS 2011: International Conference on Artificial Intelligence and Statistics}, 
pages = 	 {1462--1471}, 
year = 	 {2012}, 
editor = 	 {Neil D. Lawrence and Mark Girolami}, 
volume = 	 {22}, 
series = 	 {Proceedings of Machine Learning Research}, 
address = 	 {La Palma, Canary Islands}, 
month = 	 {21--23 Apr}, 
publisher = 	 {PMLR}, 
Url_Code = {Softwares/NBP_PFA_v1.zip}, 
pdf = 	 {Papers/AISTATS2012_NegBinoBeta_PFA_v19.pdf}, 
Url_Poster = {Papers/BNBP_AISTATS2012_Poster_MingyuanZhou_04122012_Tall.pdf}, 
url = 	 {http://proceedings.mlr.press/v22/zhou12c.html}, 
abstract = 	 {A beta-negative binomial (BNB) process is proposed, leading to a beta-gamma-Poisson process, which may be viewed as a multi-scoop generalization of the beta-Bernoulli process. The BNB process is augmented into a beta-gamma-gamma-Poisson hierarchical structure, and applied as a nonparametric Bayesian prior for an infinite Poisson factor analysis model. A finite approximation for the beta process Levy random measure is constructed for convenient implementation. Efficient MCMC computations are performed with data augmentation and marginalization techniques. Encouraging results are shown on document count matrix factorization.}
}


@inproceedings{li2011integration, 
author =    {Lingbo Li and Mingyuan Zhou and Guillermo Sapiro and Lawrence Carin}, 
title =     {On the Integration of Topic Modeling and Dictionary Learning }, 
booktitle = {ICML 2011: International Conference on Machine Learning}, 
series =    {ICML '11}, 
year =      {2011}, 
editor =    {Lise Getoor and Tobias Scheffer}, 
location =  {Bellevue, Washington, USA}, 
isbn =      {978-1-4503-0619-5}, 
month =     {June}, 
publisher = {ACM}, 
pages=      {625--632}, 
url= {http://www.icml-2011.org/papers/375_icmlpaper.pdf}, 
pdf= {http://www.icml-2011.org/papers/375_icmlpaper.pdf}, 
Abstract={A new nonparametric Bayesian model is developed to integrate dictionary learning and topic model into a unified framework. The model is employed to analyze partially annotated images, with the dictionary learning performed directly on image patches. Efficient inference is performed with a Gibbs slice sampler, and encouraging results are reported on widely used datasets.},
}

@article{xing2012dictionary,
	author = {Xing, Zhengming and Zhou, Mingyuan and Castrodad, Alexey and Sapiro, Guillermo and Carin, Lawrence},
	title = {Dictionary Learning for Noisy and Incomplete Hyperspectral Images},
	journal = {SIAM Journal on Imaging Sciences},
	volume = {5},
	number = {1},
	pages = {33-56},
	year = {2012},
	url= {http://epubs.siam.org/doi/abs/10.1137/110837486},
	pdf= {Papers/BPFA_HSI_6.pdf},
	Abstract = {We consider analysis of noisy and incomplete hyperspectral imagery, with the objective of removing the noise and inferring the missing data. The noise statistics may be wavelength dependent, and the fraction of data missing (at random) may be substantial, including potentially entire bands, offering the potential to significantly reduce the quantity of data that need be measured. To achieve this objective, the imagery is divided into contiguous three-dimensional (3D) spatio-spectral blocks of spatial dimension much less than the image dimension. It is assumed that each such 3D block may be represented as a linear combination of dictionary elements of the same dimension, plus noise, and the dictionary elements are learned in situ based on the observed data (no a priori training). The number of dictionary elements needed for representation of any particular block is typically small relative to the block dimensions, and all the image blocks are processed jointly (collaboratively") to infer the underlying dictionary. We address dictionary learning from a Bayesian perspective, considering two distinct means of imposing sparse dictionary usage. These models allow inference of the number of dictionary elements needed as well as the underlying wavelength-dependent noise statistics. It is demonstrated that drawing the dictionary elements from a Gaussian process prior, imposing structure on the wavelength dependence of the dictionary elements, yields significant advantages, relative to the more conventional approach of using an independent and identically distributed Gaussian prior for the dictionary elements; this advantage is particularly evident in the presence of noise. The framework is demonstrated by processing hyperspectral imagery with a significant number of voxels missing uniformly at random, with imagery at specific wavelengths missing entirely, and in the presence of substantial additive noise.}
}

@article{zhou2012nonparametric,
	Abstract = {Nonparametric Bayesian methods are considered for recovery of imagery based upon compressive, incomplete, and/or noisy measurements. A truncated beta-Bernoulli process is employed to infer an appropriate dictionary for the data under test and also for image recovery. In the context of compressive sensing, significant improvements in image recovery are manifested using learned dictionaries, relative to using standard orthonormal image expansions. The compressive-measurement projections are also optimized for the learned dictionary. Additionally, we consider simpler (incomplete) measurements, defined by measuring a subset of image pixels, uniformly selected at random. Spatial interrelationships within imagery are exploited through use of the Dirichlet and probit stick-breaking processes. Several example results are presented, with comparisons to other methods in the literature.},
	Author = {Zhou, Mingyuan and Chen, Haojun and Paisley, John and Ren, Lu and Li, Lingbo and Xing, Zhengming and Dunson, David and Sapiro, Guillermo and Carin, Lawrence},
	Journal = {IEEE Transactions on Image Processing},
	Number = {1},
	Pages = {130--144},
	Publisher = {IEEE},
	Title = {Nonparametric {B}ayesian Dictionary Learning for Analysis of Noisy and Incomplete Images},
	Url_Code = {Results/BPFAImage/},
	url = {http://ieeexplore.ieee.org/document/5898409/},
	pdf = {Papers/BPFAImage_Journal_11-2.pdf},
	Volume = {21},
	Year = {2012}
	}

@inproceedings{zhou2011dependent,
	Abstract = {A dependent hierarchical beta process (dHBP) is developed as a prior for data that may be represented in terms of a sparse set of latent features, with covariate-dependent feature usage. The dHBP is applicable to general covariates and data models, imposing that signals with similar covariates are likely to be manifested in terms of similar features. Coupling the dHBP with the Bernoulli process, and upon marginalizing out the dHBP, the model may be interpreted as a covariate-dependent hierarchical Indian buffet process. As applications, we consider interpolation and denoising of an image, with covariates defined by the location of image patches within an image. Two types of noise models are considered: (i) typical white Gaussian noise; and (ii) spiky noise of arbitrary amplitude, distributed uniformly at random. In these examples, the features correspond to the atoms of a dictionary, learned based upon the data under test (without a priori training data). State-of-the-art performance is demonstrated, with efficient inference using hybrid Gibbs, Metropolis-Hastings and slice sampling.},
	Address = {Fort Lauderdale, FL, USA},
	Author = {Mingyuan Zhou and Hongxia Yang and Guillermo Sapiro and David Dunson and Lawrence Carin},
	Booktitle = {AISTATS 2011: International Conference on Artificial Intelligence and Statistics},
	Editor = {Geoffrey Gordon and David Dunson and Miroslav Dud{\'\i}k},
	Month = {11--13 Apr},
	Note = {(Oral presentation)},
	Pages = {883--891},
	Publisher = {PMLR},
	Series = {Proceedings of Machine Learning Research},
	Title = {Dependent Hierarchical Beta Process for Image Interpolation and Denoising},
	url = {http://proceedings.mlr.press/v15/zhou11a.html},
	pdf = {http://proceedings.mlr.press/v15/zhou11a/zhou11a.pdf},
	Url_Slide = {Papers/dHBP_AISTATS2011_MingyuanZhou_04122011.pdf},
	Url_Video = {http://videolectures.net/aistats2011_zhou_dependent/},
	Volume = {15},
	Year = {2011}
	}
	

@INPROCEEDINGS{zhou2010nonparametric,
  author={Zhou, Mingyuan and Wang, Chunping and Chen, Minhua and Paisley, John and Dunson, David and Carin, Lawrence},
  booktitle={2010 IEEE Sensor Array and Multichannel Signal Processing Workshop}, 
  title={Nonparametric Bayesian Matrix Completion}, 
  year={2010},
  pages={213-216},
  url={https://ieeexplore.ieee.org/abstract/document/5606741},
  abstract={The beta-binomial processes are considered for inferring missing values in matrices. The model moves beyond the low-rank assumption, modeling the matrix columns as residing in a nonlinear subspace. Large-scale problems are considered via efficient Gibbs sampling, yielding predictions as well as a measure of confidence in each prediction. Algorithm performance is considered for several datasets, with encouraging performance relative to existing approaches.}
  }

@inproceedings{zhou2009non,
	Abstract = {Non-parametric Bayesian techniques are considered for learning dictionaries for sparse image representations, with applications in denoising, inpainting and compressive sensing (CS). The beta process is employed as a prior for learning the dictionary, and this non-parametric method naturally infers an appropriate dictionary size. The Dirichlet process and a probit stick-breaking process are also considered to exploit structure within an image. The proposed method can learn a sparse dictionary in situ; training images may be exploited if available, but they are not required. Further, the noise variance need not be known, and can be non-stationary. Another virtue of the proposed method is that sequential inference can be readily employed, thereby allowing scaling to large images. Several example results are presented, using both Gibbs and variational Bayesian inference, with comparisons to other state-of-the-art approaches.},
	Author = {Zhou, Mingyuan and Haojun Chen and Lu Ren and Sapiro, Guillermo and Carin, Lawrence and John W. Paisley},
	Booktitle = {NeurIPS 2009: Advances in Neural Information Processing Systems},
	Editor = {Y. Bengio and D. Schuurmans and J. D. Lafferty and C. K. I. Williams and A. Culotta},
	Note = {(Full oral presentation)},
	Pages = {2295--2303},
	 
	Title = {Non-Parametric {B}ayesian Dictionary Learning for Sparse Image Representations},
	Url_Code = {Softwares/BPFA_Denoising_Inpainting_codes_Inference_10292009.zip},
	url = {http://papers.nips.cc/paper/3851-non-parametric-bayesian-dictionary-learning-for-sparse-image-representations},
	pdf = {http://papers.nips.cc/paper/3851-non-parametric-bayesian-dictionary-learning-for-sparse-image-representations.pdf},
	Url_Slide = {Papers/MingyuanZhou_NIPS_12082009.pdf},
	Url_Video = {http://videolectures.net/mingyuan_zhou/},
	Year = {2009}
	}


