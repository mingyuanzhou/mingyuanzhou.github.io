

@article{zhou2016softplus,
  title={Softplus regressions and convex polytopes},
  author={Zhou, Mingyuan},
  journal={arXiv:1608.06383},
  year={2016},
  url_link={https://arxiv.org/abs/1608.06383},
  url_pdf={Papers/softplus_regression_v14.pdf},
  url_arxiv={https://arxiv.org/abs/1608.06383},
  abstract={To construct flexible nonlinear predictive distributions, the paper introduces a family of softplus function based regression models that convolve, stack, or combine both operations by convolving countably infinite stacked gamma distributions, whose scales depend on the covariates. Generalizing logistic regression that uses a single hyperplane to partition the covariate space into two halves, softplus regressions employ multiple hyperplanes to construct a confined space, related to a single convex polytope defined by the intersection of multiple half-spaces or a union of multiple convex polytopes, to separate one class from the other. The gamma process is introduced to support the convolution of countably infinite (stacked) covariate-dependent gamma distributions. For Bayesian inference, Gibbs sampling derived via novel data augmentation and marginalization techniques is used to deconvolve and/or demix the highly complex nonlinear predictive distribution. Example results demonstrate that softplus regressions provide flexible nonlinear decision boundaries, achieving classification accuracies comparable to that of kernel support vector machine while requiring significant less computation for out-of-sample prediction.}
}




@article{zhou2017frequency,
  title={Frequency of Frequencies Distributions and Size-Dependent Exchangeable Random Partitions},
  author={Zhou, Mingyuan and Favaro, Stefano and Walker, Stephen G},
  journal={Journal of the American Statistical Association},
  pages={1--13},
  year={2017},
  publisher={Taylor \& Francis},
  url_link={http://www.tandfonline.com/doi/full/10.1080/01621459.2016.1222290},
  url_pdf={Papers/dEPPF_v18.pdf},
  url_arxiv={http://arxiv.org/abs/1608.00264},
  url_code={https://GitHub.com/mingyuanzhou/FoF_EPPF},
  abstract={Motivated by the fundamental problem of modeling the frequency of frequencies (FoF) distribution, this article introduces the concept of a cluster structure to define a probability function that governs the joint distribution of a random count and its exchangeable random partitions. A cluster structure, naturally arising from a completely random measure mixed Poisson process, allows the probability distribution of the random partitions of a subset of a population to be dependent on the population size, a distinct and motivated feature that makes it more flexible than a partition structure. This allows it to model an entire FoF distribution whose structural properties change as the population size varies. An FoF vector can be simulated by drawing an infinite number of Poisson random variables, or by a stick-breaking construction with a finite random number of steps. A generalized negative binomial process model is proposed to generate a cluster structure, where in the prior the number of clusters is finite and Poisson distributed, and the cluster sizes follow a truncated negative binomial distribution. We propose a simple Gibbs sampling algorithm to extrapolate the FoF vector of a population given the FoF vector of a sample taken without replacement from the population. We illustrate our results and demonstrate the advantages of the proposed models through the analysis of real text, genomic, and survey data. Supplementary materials for this article are available online.}
}






@inproceedings{schein2016bayesian,
  title = 	 {Bayesian Poisson Tucker Decomposition for Learning the Structure of International Relations},
  author = 	 {Aaron Schein and Mingyuan Zhou and David Blei and Hanna Wallach},
  booktitle = 	 {International Conference on Machine Learning},
  pages = 	 {2810--2819},
  year = 	 {2016},
  editor = 	 {Maria Florina Balcan and Kilian Q. Weinberger},
  volume = 	 {48},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {New York, New York, USA},
  month = 	 {20--22 Jun},
  publisher = 	 {PMLR},
  url_pdf = 	 {Papers/ScheinZhouBleiWallach2016_paper.pdf},
  url_link = 	 {http://proceedings.mlr.press/v48/schein16.html},
  url_arxiv={https://arxiv.org/abs/1606.01855},
  abstract = 	 {We introduce Bayesian Poisson Tucker decomposition (BPTD) for modeling country–country interaction event data. These data consist of interaction events of the form “country i took action a toward country j at time t.” BPTD discovers overlapping country–community memberships, including the number of latent communities. In addition, it discovers directed community–community interaction networks that are specific to “topics” of action types and temporal “regimes.” We show that BPTD yields an efficient MCMC inference algorithm and achieves better predictive performance than related models. We also demonstrate that it discovers interpretable latent structure that agrees with our knowledge of international relations.}
}


@article{zhou2016augmentable,
  title={Augmentable gamma belief networks},
  author={Zhou, Mingyuan and Cong, Yulai and Chen, Bo},
  journal={Journal of Machine Learning Research},
  volume={17},
  number={163},
  pages={1--44},
  year={2016},
  url_link={http://jmlr.org/papers/v17/15-633.html},
  url_pdf={Papers/DeepPoGamma_Journal_v5.pdf},
  url_arxiv={http://arxiv.org/abs/1512.03081},
  url_slide={Papers/GBN_ISBA_201606.pdf},
  url_code = "https://GitHub.com/mingyuanzhou/GBN}, 
  abstract = {To infer multilayer deep representations of high-dimensional discrete and nonnegative real vectors, we propose an augmentable gamma belief network (GBN) that factorizes each of its hidden layers into the product of a sparse connection weight matrix and the nonnegative real hidden units of the next layer. The GBN's hidden layers are jointly trained with an upward-downward Gibbs sampler that solves each layer with the same subroutine. The gamma-negative binomial process combined with a layer-wise training strategy allows inferring the width of each layer given a fixed budget on the width of the first layer. Example results illustrate interesting relationships between the width of the first layer and the inferred network structure, and demonstrate that the GBN can add more layers to improve its performance in both unsupervisedly extracting features and predicting heldout data. For exploratory data analysis, we extract trees and subnetworks from the learned deep network to visualize how the very specific factors discovered at the first hidden layer and the increasingly more general factors discovered at deeper hidden layers are related to each other, and we generate synthetic data by propagating random variables through the deep network from the top hidden layer back to the bottom data layer.}
}



@inproceedings{zhou2015poisson,
  title={The Poisson gamma belief network},
  author = {Zhou, Mingyuan and Cong, Yulai and Chen, Bo},
   booktitle = {Advances in Neural Information Processing Systems 28},
  editor = {C. Cortes and N. D. Lawrence and D. D. Lee and M. Sugiyama and R. Garnett},
  pages = {3043--3051},
  year = {2015},
  publisher = {Curran Associates, Inc.},
  url_link={https://papers.nips.cc/paper/5645-the-poisson-gamma-belief-network},
  url_pdf={Papers/DeepPoGamma_v5.pdf},
  url_arxiv={http://arxiv.org/abs/1511.02199},
  url_poster={Papers/PGBN_NIPS2015_Poster.pdf},
  url_code = "https://GitHub.com/mingyuanzhou/GBN},
  abstract={To infer a multilayer representation of high-dimensional count vectors, we propose the Poisson gamma belief network (PGBN) that factorizes each of its layers into the product of a connection weight matrix and the nonnegative real hidden units of the next layer. The PGBN's hidden layers are jointly trained with an upward-downward Gibbs sampler, each iteration of which upward samples Dirichlet distributed connection weight vectors starting from the first layer (bottom data layer), and then downward samples gamma distributed hidden units starting from the top hidden layer. The gamma-negative binomial process combined with a layer-wise training strategy allows the PGBN to infer the width of each layer given a fixed budget on the width of the first layer. The PGBN with a single hidden layer reduces to Poisson factor analysis. Example results on text analysis illustrate interesting relationships between the width of the first layer and the inferred network structure, and demonstrate that the PGBN, whose hidden units are imposed with correlated gamma priors, can add more layers to increase its performance gains over Poisson factor analysis, given the same limit on the width of the first layer.}
}

	

@article{zhou2016priors,
  title={Priors for random count matrices derived from a family of negative binomial processes},
  author={Zhou, Mingyuan and Padilla, Oscar Hernan Madrid and Scott, James G},
  journal={Journal of the American Statistical Association},
  volume={111},
  number={515},
  pages={1144--1156},
  year={2016},
  publisher={Taylor \& Francis},
  url_link={https://www.tandfonline.com/doi/abs/10.1080/01621459.2015.1075407},
  url_pdf={Papers/NBP_VectorMatrix_Journal_revise3_ArXiv.pdf},
  url_arxiv={http://arxiv.org/abs/1404.3331},
  url_slide={Papers/NBP_Count_Matrix_201506.pdf},
  url_code={https://GitHub.com/mingyuanzhou/NBP_random_count_matrices},
   abstract = { ABSTRACTWe define a family of probability distributions for random count matrices with a potentially unbounded number of rows and columns. The three distributions we consider are derived from the gamma-Poisson, gamma-negative binomial, and beta-negative binomial processes, which we refer to generically as a family of negative-binomial processes. Because the models lead to closed-form update equations within the context of a Gibbs sampler, they are natural candidates for nonparametric Bayesian priors over count matrices. A key aspect of our analysis is the recognition that although the random count matrices within the family are defined by a row-wise construction, their columns can be shown to be independent and identically distributed (iid). This fact is used to derive explicit formulas for drawing all the columns at once. Moreover, by analyzing these matricesâ combinatorial structure, we describe how to sequentially construct a column-iidÂ random count matrix one row at a time, and derive the predictive distribution of a new row count vector with previously unseen features. We describe the similarities and differences between the three priors, and argue that the greater flexibility of the gamma- and beta-negative binomial processesâespecially their ability to model over-dispersed, heavy-tailed count dataâmakes these well suited to a wide variety of real-world applications. As an example of our framework, we construct a naive-Bayes text classifier to categorize a count vector to one of several existing random count matrices of different categories. The classifier supports an unbounded number of features and, unlike most existing methods, it does not require a predefined finite vocabulary to be shared by all the categories, and needs neither feature selection nor parameter tuning. Both the gamma- and beta-negative binomial processes are shown to significantly outperform the gamma-Poisson process when applied to document categorization, with comparable performance to other state-of-the-art supervised text classification algorithms. Supplementary materials for this article are available online.}
}



@InProceedings{Acharya2015gamma,
author={Acharya, Ayan
and Teffer, Dean
and Henderson, Jette
and Tyler, Marcus
and Zhou, Mingyuan
and Ghosh, Joydeep},
editor={Appice, Annalisa
and Rodrigues, Pedro Pereira
and Santos Costa, V{\'i}tor
and Soares, Carlos
and Gama, Jo{\~a}o
and Jorge, Al{\'i}pio},
title={Gamma Process Poisson Factorization for Joint Modeling of Network and Documents},
booktitle={ECML PKDD 2015: Machine Learning and Knowledge Discovery in Databases},
year={2015},
publisher={Springer International Publishing},
address={Cham},
pages={283--299},
abstract={Developing models to discover, analyze, and predict clusters within networked entities is an area of active and diverse research. However, many of the existing approaches do not take into consideration pertinent auxiliary information. This paper introduces Joint Gamma Process Poisson Factorization (J-GPPF) to jointly model network and side-information. J-GPPF naturally fits sparse networks, accommodates separately-clustered side information in a principled way, and effectively addresses the computational challenges of analyzing large networks. Evaluated with hold-out link prediction performance on sparse networks (both synthetic and real-world) with side information, J-GPPF is shown to clearly outperform algorithms that only model the network adjacency matrix.},
isbn={978-3-319-23528-8},
url_link={https://link.springer.com/chapter/10.1007/978-3-319-23528-8_18},
url_pdf={Papers/acth15.pdf}
}




@INPROCEEDINGS{7362890,  author={Mingyuan Zhou},  booktitle={European Signal Processing Conference (EUSIPCO)},   title={Nonparametric Bayesian matrix factorization for assortative networks},   year={2015},  volume={},  number={},  pages={2776-2780},  abstract={We describe in detail the gamma process edge partition model that is well suited to analyze assortative relational networks. The model links the binary edges of an undirected and unweighted relational network with a latent factor model via the Bernoulli-Poisson link, and uses the gamma process to support a potentially infinite number of latent communities. The communities are allowed to overlap with each other, with a community's overlapping parts assumed to be more densely connected than its non-overlapping ones. The model is evaluated with synthetic data to illustrate its ability to model as-sortative networks and its restriction on modeling dissortative ones.},  keywords={Bayes methods;matrix decomposition;social networking (online);stochastic processes;Bernoulli-Poisson link;latent factor model;unweighted relational network;binary edges;assortative relational networks;gamma process edge partition model;nonparametric Bayesian matrix factorization;Predator prey systems;Bayes methods;Computational modeling;Analytical models;Data models;Mathematical model;Europe;Gamma process;factor analysis;Bernoulli-Poisson link;overlapping community detection;link prediction},  doi={10.1109/EUSIPCO.2015.7362890},  ISSN={2076-1465},  month={Aug},
url_link={https://ieeexplore.ieee.org/document/7362890/similar#similar},
url_pdf={Papers/EPM_EUROSIPCO2015.pdf},
url_slide = {EPM_EUSIPCO_201509.pdf},
note={(Invited special session paper)}
}

@inproceedings{zhou2015infinite,
  title = 	 {Infinite Edge Partition Models for Overlapping Community Detection and Link Prediction},
  author = 	 {Mingyuan Zhou},
  booktitle = 	 {International Conference on Artificial Intelligence and Statistics},
  pages = 	 {1135--1143},
  year = 	 {2015},
  editor = 	 {Guy Lebanon and S. V. N. Vishwanathan},
  volume = 	 {38},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {San Diego, California, USA},
  month = 	 {09--12 May},
  publisher = 	 {PMLR},
  url_pdf = 	 {Papers/EPM_AISTATS2015_v8_1.pdf},
  url_link = 	 {http://proceedings.mlr.press/v38/zhou15a.html},
  url_code={https://GitHub.com/mingyuanzhou/EPM},
  url_poster={Papers/EPM_AISTATS2015_Poster.pdf},
  abstract = 	 {A hierarchical gamma process infinite edge partition model is proposed to factorize the binary adjacency matrix of an unweighted undirected relational network under a  Bernoulli-Poisson link. The model describes  both homophily and stochastic equivalence, and is scalable to  big sparse networks by focusing its computation on  pairs of linked nodes. It can not only discover overlapping communities and inter-community interactions, but also predict missing edges. A simplified version omitting inter-community interactions is also provided and we reveal its  interesting connections to existing models. The number of communities is automatically inferred in a nonparametric Bayesian manner, and efficient  inference via Gibbs sampling is derived using novel data augmentation techniques. Experimental results on four real networks demonstrate the models’ scalability and state-of-the-art performance.}
}


@inproceedings{acharya2015nonparametric,
  title = 	 {Nonparametric {B}ayesian Factor Analysis for Dynamic Count Matrices},
  author = 	 {Ayan Acharya and Joydeep Ghosh and Mingyuan Zhou},
  booktitle = 	 {International Conference on Artificial Intelligence and Statistics},
  pages = 	 {1--9},
  year = 	 {2015},
  editor = 	 {Guy Lebanon and S. V. N. Vishwanathan},
  volume = 	 {38},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {San Diego, California, USA},
  month = 	 {09--12 May},
  publisher = 	 {PMLR},
  url_pdf = 	 {Papers/GP_DPFA_AISTATS2015_v6.pdf},
  url_link = 	 {http://proceedings.mlr.press/v38/acharya15.html},
  url_arxiv={http://arxiv.org/abs/1512.08996},
  abstract = 	 {A gamma process dynamic Poisson factor analysis model is proposed to factorize a dynamic count matrix, whose columns are sequentially observed count vectors. The model builds a novel  Markov chain that sends the latent gamma random variables at time (t-1) as the shape parameters of those at time t, which are linked to observed or latent counts under the Poisson likelihood.  The significant challenge of inferring the gamma shape parameters is fully addressed, using unique data augmentation and marginalization techniques for the negative binomial distribution. The same nonparametric Bayesian model also applies to the factorization of a dynamic binary matrix, via a Bernoulli-Poisson link that connects a binary observation to a latent count, with closed-form conditional posteriors for the latent counts and efficient computation for sparse observations. We apply the model to text and music analysis, with state-of-the-art results.},
}




@inproceedings{zhou2014beta,
  title={Beta-Negative Binomial Process and Exchangeable Random Partitions for Mixed-Membership Modeling},
author = {Zhou, Mingyuan},
booktitle = {Advances in Neural Information Processing Systems},
editor = {Z. Ghahramani and M. Welling and C. Cortes and N. D. Lawrence and K. Q. Weinberger},
pages = {3455--3463},
year = {2014},
publisher = {Curran Associates, Inc.},
url_link = {http://papers.nips.cc/paper/5298-beta-negative-binomial-process-and-exchangeable-random-partitions-for-mixed-membership-modeling.html},
url_pdf={Papers/BNBP_Collapsed_v7_arXiv.pdf},
url_poster={Papers/BNBP_NIPS2014_Poster.pdf},
url_code = {https://GitHub.com/mingyuanzhou/BNBP_collapsed},
url_arxiv={http://arxiv.org/abs/1410.7812},
Abstract={The beta-negative binomial process (BNBP), an integer-valued stochastic process, is employed to partition a count vector into a latent random count matrix. As the marginal probability distribution of the BNBP that governs the exchangeable random partitions of grouped data has not yet been developed, current inference for the BNBP has to truncate the number of atoms of the beta process. This paper introduces an exchangeable partition probability function to explicitly describe how the BNBP clusters the data points of each group into a random number of exchangeable partitions, which are shared across all the groups. A fully collapsed Gibbs sampler is developed for the BNBP, leading to a novel nonparametric Bayesian topic model that is distinct from existing ones, with simple implementation, fast convergence, good mixing, and state-of-the-art predictive performance.}
}

	
		
@article{polatkan2015bayesian,
  title={A {B}ayesian nonparametric approach to image super-resolution},
  author={Polatkan, Gungor and Zhou, Mingyuan and Carin, Lawrence and Blei, David and Daubechies, Ingrid},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume={37},
  number={2},
  pages={346--358},
  year={2015},
  publisher={IEEE},
  url_link={http://ieeexplore.ieee.org/document/6809161/},
  url_pdf={http://arxiv.org/abs/1209.5019},
  url_arxiv={http://arxiv.org/abs/1209.5019},
  abstract={Super-resolution methods form high-resolution images from low-resolution images. In this paper, we develop a new Bayesian nonparametric model for super-resolution. Our method uses a beta-Bernoulli process to learn a set of recurring visual patterns, called dictionary elements, from the data. Because it is nonparametric, the number of elements found is also determined from the data. We test the results on both benchmark and natural images, comparing with several other models from the research literature. We perform large-scale human evaluation experiments to assess the visual quality of the results. In a first implementation, we use Gibbs sampling to approximate the posterior. However, this algorithm is not feasible for large-scale data. To circumvent this, we then develop an online variational Bayes (VB) algorithm. This algorithm finds high quality dictionaries in a fraction of the time needed by the Gibbs sampler.}
}
		

@article{zhou2015negative,
  title={Negative binomial process count and mixture modeling},
  author={Zhou, Mingyuan and Carin, Lawrence},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume={37},
  number={2},
  pages={307--320},
  year={2015},
  publisher={IEEE},
  url_link = {https://ieeexplore.ieee.org/document/6636308},
  url_pdf= {Papers/Mingyuan_PAMI_9.pdf},
  url_arxiv = {http://arxiv.org/abs/1209.3442},
  url_code={Softwares/NBP_PFA_v1.zip},
  abstract={The seemingly disjoint problems of count and mixture modeling are united under the negative binomial (NB) process. A gamma process is employed to model the rate measure of a Poisson process, whose normalization provides a random probability measure for mixture modeling and whose marginalization leads to an NB process for count modeling. A draw from the NB process consists of a Poisson distributed finite number of distinct atoms, each of which is associated with a logarithmic distributed number of data samples. We reveal relationships between various count- and mixture-modeling distributions and construct a Poisson-logarithmic bivariate distribution that connects the NB and Chinese restaurant table distributions. Fundamental properties of the models are developed, and we derive efficient Bayesian inference. It is shown that with augmentation and normalization, the NB process and gamma-NB process can be reduced to the Dirichlet process and hierarchical Dirichlet process, respectively. These relationships highlight theoretical, structural, and computational advantages of the NB process. A variety of NB processes, including the beta-geometric, beta-NB, marked-beta-NB, marked-gamma-NB and zero-inflated-NB processes, with distinct sharing mechanisms, are also constructed. These models are applied to topic modeling, with connections made to existing algorithms under Poisson factor analysis. Example results show the importance of inferring both the NB dispersion and probability parameters.}
}




@inproceedings{zhou2012augment,
title = {Augment-and-Conquer Negative Binomial Processes},
author = {Zhou, Mingyuan and Carin, Lawrence},
booktitle = {Advances in Neural Information Processing Systems},
editor = {F. Pereira and C. J. C. Burges and L. Bottou and K. Q. Weinberger},
pages = {2546--2554},
year = {2012},
publisher = {Curran Associates, Inc.},
Url_Link = {https://papers.nips.cc/paper/4677-augment-and-conquer-negative-binomial-processes},
Url_PDF = {Papers/Mingyuan_NBP_NIPS2012.pdf},
Url_slide = {Papers/NBP_NIPS2012_Slides_Mingyuan_12052012.pdf},
Url_Poster = {Papers/NBP_NIPS2012_Poster_MingyuanZhou_12052012.pdf},
Url_Code = {Softwares/NBP_PFA_v1.zip},
Note= {(Spotlight oral presentation)},
Abstract={By developing data augmentation methods unique to the negative binomial (NB) distribution, we unite seemingly disjoint count and mixture models under the NB process framework. We develop fundamental properties of the models and derive efficient Gibbs sampling inference. We show that the gamma-NB process can be reduced to the hierarchical Dirichlet process with normalization, highlighting its unique theoretical, structural and computational advantages. A variety of NB processes with distinct sharing mechanisms are constructed and applied to topic modeling, with connections to existing algorithms, showing the importance of inferring both the NB dispersion and probability parameters.}
}



@inproceedings{zhou2012lognormal,
  author =    {Mingyuan Zhou and Lingbo Li and David Dunson and Lawrence Carin},
  title =     {Lognormal and Gamma Mixed Negative Binomial Regression},
  booktitle = {International Conference on Machine Learning},
  series =    {ICML '12},
  year =      {2012},
  editor =    {John Langford and Joelle Pineau},
  location =  {Edinburgh, Scotland, GB},
  isbn =      {978-1-4503-1285-1},
  month =     {July},
  publisher = {Omnipress},
  address =   {New York, NY, USA},
  pages=      {1343--1350},
  Url_Paper= {Papers/Mingyuan_ICML_2012.pdf},
  Url_Appendix={Papers/Mingyuan_LGNB_Appendix.pdf},
  Url_Code = {Softwares/LGNB_Regression_v0.zip},
  Url_Slide = {Papers/LGNB_MingyuanZhou_06272012.pdf},
  Url_Video = {http://techtalks.tv/talks/lognormal-and-gamma-mixed-negative-binomial-regression/57332/},
  Url_Link= {https://icml.cc/Conferences/2012/papers/665.pdf},
  Url_Poster= {Papers/LGNB_ICML2012_Poster_MingyuanZhou_06272012.pdf},
  Abstract = {In regression analysis of counts, a lack of simple and efficient algorithms for posterior computation has made Bayesian approaches appear unattractive and thus underdeveloped. We propose a lognormal and gamma mixed negative binomial (NB) regression model for counts, and present efficient closed-form Bayesian inference; unlike conventional Poisson models, the proposed approach has two free parameters to include two different kinds of dispersion, and allows the incorporation of prior information, such as sparsity in the regression coefficients. By placing a gamma distribution prior on the NB dispersion parameter r, and connecting a lognormal distribution prior with the logit of the NB probability parameter p, efficient Gibbs sampling and variational Bayes inference are both developed. The closed-form updates are obtained by exploiting conditional conjugacy via both a compound Poisson representation and a Polya-Gamma distribution based data augmentation approach. The proposed Bayesian inference can be implemented routinely, while being easily generalizable to more complex settings involving multivariate dependence structures. The algorithms are illustrated using real examples.}
}



@InProceedings{zhou2011beta,
  title = 	 {Beta-Negative Binomial Process and Poisson Factor Analysis},
  author = 	 {Mingyuan Zhou and Lauren Hannah and David Dunson and Lawrence Carin},
  booktitle = 	 {International Conference on Artificial Intelligence and Statistics},
  pages = 	 {1462--1471},
  year = 	 {2012},
  editor = 	 {Neil D. Lawrence and Mark Girolami},
  volume = 	 {22},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {La Palma, Canary Islands},
  month = 	 {21--23 Apr},
  publisher = 	 {PMLR},
  Url_Code = {Softwares/NBP_PFA_v1.zip},
  Url_PDF = 	 {Papers/AISTATS2012_NegBinoBeta_PFA_v19.pdf},
  Url_Poster = {Papers/BNBP_AISTATS2012_Poster_MingyuanZhou_04122012_Tall.pdf},
  Url_Link = 	 {http://proceedings.mlr.press/v22/zhou12c.html},
  abstract = 	 {A beta-negative binomial (BNB) process is proposed, leading to a beta-gamma-Poisson process, which may be viewed as a “multi-scoop” generalization of the beta-Bernoulli process. The BNB process is augmented into a beta-gamma-gamma-Poisson hierarchical structure, and applied as a nonparametric Bayesian prior for an infinite Poisson factor analysis model. A finite approximation for the beta process Levy random measure is constructed for convenient implementation. Efficient MCMC computations are performed with data augmentation and marginalization techniques. Encouraging results are shown on document count matrix factorization.}
}


@inproceedings{li2011integration,
  author =    {Lingbo Li and Mingyuan Zhou and Guillermo Sapiro and Lawrence Carin},
  title =     {On the Integration of Topic Modeling and Dictionary Learning },
  booktitle = {International Conference on Machine Learning},
  series =    {ICML '11},
  year =      {2011},
  editor =    {Lise Getoor and Tobias Scheffer},
  location =  {Bellevue, Washington, USA},
  isbn =      {978-1-4503-0619-5},
  month =     {June},
  publisher = {ACM},
  pages=      {625--632},
  Url_Link= {http://www.icml-2011.org/papers/375_icmlpaper.pdf},
  Url_PDF= {http://www.icml-2011.org/papers/375_icmlpaper.pdf},
  Abstract={A new nonparametric Bayesian model is developed to integrate dictionary learning and topic model into a unified framework. The model is employed to analyze partially annotated images, with the dictionary learning performed directly on image patches. Efficient inference is performed with a Gibbs slice sampler, and encouraging results are reported on widely used datasets.}
}

@article{xing2012dictionary,
	author = {Xing, Zhengming and Zhou, Mingyuan and Castrodad, Alexey and Sapiro, Guillermo and Carin, Lawrence},
	title = {Dictionary Learning for Noisy and Incomplete Hyperspectral Images},
	journal = {SIAM Journal on Imaging Sciences},
	volume = {5},
	number = {1},
	pages = {33-56},
	year = {2012},
	Url_Link= {http://epubs.siam.org/doi/abs/10.1137/110837486},
	Url_PDF= {Papers/BPFA_HSI_6.pdf},
	Abstract = {We consider analysis of noisy and incomplete hyperspectral imagery, with the objective of removing the noise and inferring the missing data. The noise statistics may be wavelength dependent, and the fraction of data missing (at random) may be substantial, including potentially entire bands, offering the potential to significantly reduce the quantity of data that need be measured. To achieve this objective, the imagery is divided into contiguous three-dimensional (3D) spatio-spectral blocks of spatial dimension much less than the image dimension. It is assumed that each such 3D block may be represented as a linear combination of dictionary elements of the same dimension, plus noise, and the dictionary elements are learned in situ based on the observed data (no a priori training). The number of dictionary elements needed for representation of any particular block is typically small relative to the block dimensions, and all the image blocks are processed jointly (“collaboratively") to infer the underlying dictionary. We address dictionary learning from a Bayesian perspective, considering two distinct means of imposing sparse dictionary usage. These models allow inference of the number of dictionary elements needed as well as the underlying wavelength-dependent noise statistics. It is demonstrated that drawing the dictionary elements from a Gaussian process prior, imposing structure on the wavelength dependence of the dictionary elements, yields significant advantages, relative to the more conventional approach of using an independent and identically distributed Gaussian prior for the dictionary elements; this advantage is particularly evident in the presence of noise. The framework is demonstrated by processing hyperspectral imagery with a significant number of voxels missing uniformly at random, with imagery at specific wavelengths missing entirely, and in the presence of substantial additive noise.}
}

@article{zhou2012nonparametric,
	Abstract = {Nonparametric Bayesian methods are considered for recovery of imagery based upon compressive, incomplete, and/or noisy measurements. A truncated beta-Bernoulli process is employed to infer an appropriate dictionary for the data under test and also for image recovery. In the context of compressive sensing, significant improvements in image recovery are manifested using learned dictionaries, relative to using standard orthonormal image expansions. The compressive-measurement projections are also optimized for the learned dictionary. Additionally, we consider simpler (incomplete) measurements, defined by measuring a subset of image pixels, uniformly selected at random. Spatial interrelationships within imagery are exploited through use of the Dirichlet and probit stick-breaking processes. Several example results are presented, with comparisons to other methods in the literature.},
	Author = {Zhou, Mingyuan and Chen, Haojun and Paisley, John and Ren, Lu and Li, Lingbo and Xing, Zhengming and Dunson, David and Sapiro, Guillermo and Carin, Lawrence},
	Journal = {IEEE Transactions on Image Processing},
	Number = {1},
	Pages = {130--144},
	Publisher = {IEEE},
	Title = {Nonparametric {B}ayesian dictionary learning for analysis of noisy and incomplete images},
	Url_Code = {Results/BPFAImage/},
	Url_Link = {http://ieeexplore.ieee.org/document/5898409/},
	Url_PDF = {Papers/BPFAImage_Journal_11-2.pdf},
	Volume = {21},
	Year = {2012}
	}

@inproceedings{zhou2011dependent,
	Abstract = {A dependent hierarchical beta process (dHBP) is developed as a prior for data that may be represented in terms of a sparse set of latent features, with covariate-dependent feature usage. The dHBP is applicable to general covariates and data models, imposing that signals with similar covariates are likely to be manifested in terms of similar features. Coupling the dHBP with the Bernoulli process, and upon marginalizing out the dHBP, the model may be interpreted as a covariate-dependent hierarchical Indian buffet process. As applications, we consider interpolation and denoising of an image, with covariates defined by the location of image patches within an image. Two types of noise models are considered: (i) typical white Gaussian noise; and (ii) spiky noise of arbitrary amplitude, distributed uniformly at random. In these examples, the features correspond to the atoms of a dictionary, learned based upon the data under test (without a priori training data). State-of-the-art performance is demonstrated, with efficient inference using hybrid Gibbs, Metropolis-Hastings and slice sampling.},
	Address = {Fort Lauderdale, FL, USA},
	Author = {Mingyuan Zhou and Hongxia Yang and Guillermo Sapiro and David Dunson and Lawrence Carin},
	Booktitle = {International Conference on Artificial Intelligence and Statistics},
	Editor = {Geoffrey Gordon and David Dunson and Miroslav Dud{\'\i}k},
	Month = {11--13 Apr},
	Note = {(Oral presentation)},
	Pages = {883--891},
	Publisher = {PMLR},
	Series = {Proceedings of Machine Learning Research},
	Title = {Dependent Hierarchical Beta Process for Image Interpolation and Denoising},
	Url_Link = {http://proceedings.mlr.press/v15/zhou11a.html},
	Url_PDF = {http://proceedings.mlr.press/v15/zhou11a/zhou11a.pdf},
	Url_Slide = {Papers/dHBP_AISTATS2011_MingyuanZhou_04122011.pdf},
	Url_Video = {http://videolectures.net/aistats2011_zhou_dependent/},
	Volume = {15},
	Year = {2011}
	}

@inproceedings{zhou2009non,
	Abstract = {Non-parametric Bayesian techniques are considered for learning dictionaries for sparse image representations, with applications in denoising, inpainting and compressive sensing (CS). The beta process is employed as a prior for learning the dictionary, and this non-parametric method naturally infers an appropriate dictionary size. The Dirichlet process and a probit stick-breaking process are also considered to exploit structure within an image. The proposed method can learn a sparse dictionary in situ; training images may be exploited if available, but they are not required. Further, the noise variance need not be known, and can be non-stationary. Another virtue of the proposed method is that sequential inference can be readily employed, thereby allowing scaling to large images. Several example results are presented, using both Gibbs and variational Bayesian inference, with comparisons to other state-of-the-art approaches.},
	Author = {Zhou, Mingyuan and Haojun Chen and Lu Ren and Sapiro, Guillermo and Carin, Lawrence and John W. Paisley},
	Booktitle = {Advances in Neural Information Processing Systems},
	Editor = {Y. Bengio and D. Schuurmans and J. D. Lafferty and C. K. I. Williams and A. Culotta},
	Note = {(Oral presentation)},
	Pages = {2295--2303},
	Publisher = {Curran Associates, Inc.},
	Title = {Non-Parametric {B}ayesian Dictionary Learning for Sparse Image Representations},
	Url_Code = {Softwares/BPFA_Denoising_Inpainting_codes_Inference_10292009.zip},
	Url_Link = {http://papers.nips.cc/paper/3851-non-parametric-bayesian-dictionary-learning-for-sparse-image-representations},
	Url_PDF = {http://papers.nips.cc/paper/3851-non-parametric-bayesian-dictionary-learning-for-sparse-image-representations.pdf},
	Url_Slide = {Papers/MingyuanZhou_NIPS_12082009.pdf},
	Url_Video = {http://videolectures.net/mingyuan_zhou/},
	Year = {2009}
	}