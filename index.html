<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 2.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
	<meta http-equiv="content-type" content="text/html; charset=utf-8" />
	<meta name="description" content="Assistant Professor of Statistics in the Department
	 of Information, Risk, and Operations Management, McCombs School of Business, 
	 University of Texas at Austin" />
	<meta name="keywords" content="Mingyuan Zhou, Bayesian nonparametrics, negative binomial,
	count and mixture modeling, Duke University, University of Texas at Austin" />
	<meta name="Mingyuan Zhou" content="Research and teaching" />
	<link rel="stylesheet" type="text/css" href="Mingyuan.css" title="Basic Profile" media="all" />
	<title>Mingyuan Zhou</title>
</head>

<body>


	<div id="sidebar">
		<a href="index.html"><img src="images/Mingyuan.jpg"  height="230" alt="Sample logotype" /></a>
		<h1><a style="text-decoration: none" href="index.html">Mingyuan Zhou</a></h1>
		<!-- <p class="slogan">everyone has a story to tell</p> -->
		
		<ul>
			<!--<li><a href="#">Page one</a><br />The front page...</li>-->
			<li><a href="Paper.html">Publications</a></li>
			<li><a href="Code.html">Software</a></li>
			<li><a href="teaching.html">Teaching</a></li>
			<li><a href="http://scholar.google.com/citations?user=LXwCIisAAAAJ&hl=en">Google Scholar</a></li>
			<li><a href="http://videolectures.net/mingyuan_zhou">Video Lectures</a></li>
			<li><a href="CV_MingyuanZhou.pdf">CV</a></li>
			<p>
			<font size="2">IROM Dept., University of Texas<br>
			2110 Speedway Stop B6500<br>
			Austin, TX 78712<br>
			Office: CBA 6.458<br>
			Phone: +1 512-232-6763 <br></font>
			<font size="1">
			mingyuan.zhou@mccombs.utexas.edu<br>
			mzhou@utexas.edu<br></font>
		</ul>
	</div>
	
	<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-35321140-3', 'mingyuanzhou.github.io');
  ga('send', 'pageview');

</script>
	
	<!--style="font-family: Calibri; color: blue; text-decoration: underline;"-->

	<div id="content">
		<!--<h2>What Starts Here Changes The World!</h2>-->
		<p>
		<p>
		<br>
		
		<p>I am an assistant professor of statistics at  <a style="color: #CC5500;" href="http://www.utexas.edu">
		The University of Texas at Austin</a>. 
		I am with the <a href="http://www.mccombs.utexas.edu/departments/iromâ€Ž">
		Department of Information, Risk, and Operations Management (IROM)</a> at 
		the <a href="http://www.mccombs.utexas.edu">McCombs School of Business</a>. 
		I am also a core faculty member in the <a href="http://ssc.utexas.edu">Department of Statistics and Data Sciences</a> at the College of Natural Sciences. 
		
		<p>I received my Ph.D.  
		from <a href="http://www.duke.edu">Duke University</a> in 2013, Master's  
		from the <a href="http://english.cas.cn">Chinese Academy of 
		Sciences</a> in 2008, and B.Sc. from
		 <a href="http://www.nju.edu.cn/html/eng">Nanjing University</a> in 2005. 
		 My Ph.D. advisor was <a href="http://people.ee.duke.edu/~lcarin">Lawrence Carin</a>.
		
		<p>My research lies at the intersection of Bayesian statistics and machine learning. 
		I am interested in developing statistical theory and methods, hierarchical models, and efficient Bayesian inference for big data. 
		I am currently focused on the development of nonparametric Bayesian hierarchical models for count data analysis, mixture modeling (clustering, mixed-membership modeling, topic modeling), 
		dictionary learning (feature learning, factor analysis), network modeling, and multilayer deep representation. 
		
		<!--<p>I am building the negative binomial process family to introduce new exchangeable random partitions and novel clustering algorithms.-->
		
		<p>
		
		
		
		<br/>
		
		<h3>Duke-Tsinghua Machine Learning Summer School</h3>
		<p> 
		
		I am giving an invited lecture in <a href="https://dukekunshan.edu.cn/en/events/machine-learning-2016">Duke-Tsinghua Machine Learning Summer</a>, which will be held from August 1 to 10 in Duke Kunshan University, Kunshan, China. Kunshan is a city of Jiangsu Province that is close to both Shanghai ans Suzhou.  Registration is now open: <a href="https://dukekunshan.edu.cn/en/events/machine-learning-2016">https://dukekunshan.edu.cn/en/events/machine-learning-2016</a>. 
		
		
		<p>
		
		
		<!--
		<h3>For 23rd EUSIPCO Conference:   </h3>
		<li> Slides: <a style=" color: blue;" href="Papers/EPM_EUSIPCO_201509.pdf">Nonparametric Bayesian matrix factorization for assortative networks</a>
		</li>
		
		<h3>For 10th BNP Conference:   </h3>
		<li> Slides: <a style=" color: blue;" href="Papers/NBP_Count_Matrix_201506.pdf">Priors for random count matrices with random or fixed row sums</a>
		</li>
		--!>
		
		<!--<p> Prof. <a style=" color: blue;" href="https://stat.duke.edu/~ll162/">Lizhen Lin</a> from the Department of Statistics and Data Sciences will present the lectures. (Due to an unexpected travel delay, I will not be able to arrive in Austin in time.) 
		--!>
		
		<h3>Research Highlights:  </h3>
		<ul>
		
		
		
		<p>
		<li> Our NIPS2015 paper "<a style=" color:  blue;" href="Papers/DeepPoGamma_v5.pdf">The Poisson gamma belief network</a>" presents a Poisson-gamma-gamma-gamma... generative model that can be used to unsupervisedly extract multilayer deep representation of high-dimensional count vectors, with the network structure automatically inferred from the data given a fixed budget on the width of the first layer. When applied to deep topic models, the Poisson gamma belief network (PGBN) extracts very specific topics at the first hidden layer and increasingly more general topics at deeper hidden layers. Jointly training all the layers is simple and the code is easy to implement, as a PGBN of T layers can be broken into T subproblems that are solved with the same subroutine, with the computation mainly spent on training the first hidden layer. The extracted deep network can also be used to simulate very interpretable synthetic documents, which reflect various general aspects of the corpus that the network is trained on. 
		</li>
		
		<p align="center">
		<img src="images/PGBN.png"  height="180"/>
		
		<p>
		<li> The paper "<a style=" color: blue;" href="http://www.tandfonline.com/doi/abs/10.1080/01621459.2015.1075407">Priors for random count matrices derived from a family of negative binomial processes</a>" is accepted for publication in Journal of the American Statistical Association.
		</li>
		<p>
		
		
		<p>  
		
		<li>Two papers are accepted for publication in AISTATS 2015:
		
			<ul>
				<li>Infinite edge partition models for overlapping community detection and link prediction.</li>

				<li>Nonparametric Bayesian factor analysis for dynamic count matrices.</li>
			</ul>
		</li>
		
		
		
		<p>
		<li>My NIPS2014 paper "<a style=" color:  blue;" href="Papers/BNBP_Collapsed_v7_arXiv.pdf">Beta-negative binomial process and exchangeable random partitions for mixed-membership modeling</a>" introduces a nonparametric Bayesian prior to describes how to partition a count vector into a latent column-exchangeable random count matrix, whose number of nonzero  columns is random and whose each row sums to a fixed integer. Note that in topic modeling, one is essentially trying to partition a count vector, each element of which is the total length of a document,   into a Document by Topic latent count matrix.   A fully collapsed Gibbs sampling algorithm naturally arises from the latent count matrix prior  governed by the beta-negative binomial process.   
		</li>
		
		
		<p>
		<li>The paper "<a style=" color: blue;" href="http://arxiv.org/abs/1410.3155">Sample size dependent species models</a>" that I co-authored with Dr. Stephen Walker introduces the concept of a cluster structure to define a joint distribution of the sample size and its exchangeable random partitions. The cluster structure allows the probability distribution of the random partitions of a subset of the sample to be dependent on the sample size, a feature not permitted by Kingman's partition structure. This paper is motivated by the fundamental problem of measuring species diversity using a random sample from the population. The model has been successfully applied to the analysis of sequencing count data. Below is an interesting picture from this paper:
		</li>
		
		
		<p>
		<p align="center">
		<img src="images/gNBPgCRP_1.png"  height="250"/>
		
		<p>
		<li> The paper "<a style=" color: blue;" href="http://arxiv.org/abs/1404.3331">Priors for random count matrices derived from a family of negative binomial processes</a>" that I co-authored with O.-H. Madrid-Padilla and James Scott defines a family of probability distributions to generate infinite random count matrices, whose columns are independent, and identically distributed (i.i.d.).
A random count matrix, with a Poisson distributed and potentially unbounded random  number of columns, can be generated by either drawing all its i.i.d.
 columns at once or adding one row at a time.  Both the gamma- and beta-negative binomial process random count matrices can model row-heterogeneity with row-wise parameters, whose conditional posteriors have closed-form expressions. The left (right) picture below shows a gamma (beta) negative binomial process random count matrix, whose rows are added one by one, with the new columns introduced by each row appended to the right of the matrix. 
		Our models lead to nonparametric naive Bayes classifiers that do NOT need to predetermine a finite number of features that are shared across all the categories.  
		</li>
		<p>
		<p align="center">
		<img src="images/GNBP.png"  height="220" align="middle" title="Gamma-negative binomial process random count matrix" > &nbsp &nbsp &nbsp &nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp  <img src="images/BNBP.png" title="Beta-negative binomial process random count matrix"   height="220" align="middle" >
		<p>
		
		
		<p>
		
		<li> The paper "<a style=" color: blue;" href="http://arxiv.org/abs/1209.3442">Negative binomial process count and mixture Modeling</a>" that I co-authored with Dr. Larry Carin will appear in IEEE Trans. Pattern Analysis and Machine Intelligence: Special Issue on Bayesian Nonparametrics. The picture below describes a useful bivariate count distribution discovered in the paper.
		</li>
		<p>
		<p align="center">
		<img src="images/NB_CRT_Pois_Log.png"  height="220"/>
		
		
		

		<!--
		I am interested in developing statistical theory and methods, hierarchical models, 
		efficient Bayesian inference, and large-scale learning algorithms.
		I am focused on the development of nonparametric Bayesian priors and 
		their hierarchical constructions. I have been applying them to feature learning and mixture modeling.
		
		 in particular, negative binomial process count and mixture models. -->
		

		<p class="credits">&copy; May 2016 Mingyuan Zhou<br />
		</ul>
	</div>
</body>
</html>
